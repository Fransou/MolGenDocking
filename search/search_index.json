{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MolGenDocking: Molecular Generation and Docking Benchmarks","text":"<p>Welcome to MolGenDocking, a comprehensive framework for molecular generation tasks with integrated protein-ligand docking evaluation. This project provides datasets, benchmarks, and a reward server for training and evaluating models that generate drug-like molecules optimized for specific biological targets.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>git clone https://github.com/Fransou/MolGenDocking.git\ncd MolGenDocking\npip install -e .\n</code></pre>"},{"location":"#running-the-reward-server","title":"Running the Reward Server","text":"<pre><code>export DOCKING_ORACLE=autodock_gpu\n... # Set other environment variables as needed\nexport DATA_PATH=... # Path to your data directory\nuvicorn --host 0.0.0.0 --port 8000 mol_gen_docking.server:app\n</code></pre>"},{"location":"#using-the-api","title":"Using the API","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/get_reward\",\n    json={\n        \"query\": \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\",\n        \"prompt\": \"Generate a drug-like molecule...\",\n        \"metadata\": [\n            {\n                \"properties\": [\"QED\", \"protein_1\"],\n                \"objectives\": [\"above\", \"minimize\"],\n                \"target\": [0.7, 0.0]\n            }\n        ]\n    }\n)\n</code></pre> <p>\u2699\ufe0f Reward Server API We use AutoDock-GPU for fast GPU-accelerated docking calculations. The Molecular Verifier server is built using FastAPI, and supports concurrent requests, ensuring efficient handling of multiple docking evaluations, and asynchroneous pipelines.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use MolGenDocking in your research, please cite:</p> <pre><code>...\n</code></pre>"},{"location":"#license","title":"License","text":"<p>Apache License 2.0. See LICENSE for details.</p>"},{"location":"#support","title":"Support","text":"<p>For issues, questions, or contributions, please visit our GitHub repository.</p>"},{"location":"api/data/dataset/","title":"Data Models","text":""},{"location":"api/data/dataset/#mol_gen_docking.data.pydantic_dataset.Conversation","title":"<code>Conversation</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete conversation structure with messages and metadata.</p> <p>This model represents a full conversation between a user and assistant, including all messages, system configuration, and evaluation metrics. The meta field should contain all information necessary to compute rewards, ground truth values, and other evaluation metrics.</p> <p>Attributes:</p> Name Type Description <code>meta</code> <code>Dict[str, Any]</code> <p>Metadata dictionary containing evaluation and reward computation data.</p> <code>messages</code> <code>List[Message]</code> <p>List of Message objects representing conversation turns.</p> <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt that defines assistant behavior.</p> <code>available_tools</code> <code>Optional[List[str]]</code> <p>Optional list of tool names available to the assistant (e.g., 'calculator', 'web_search', 'code_interpreter').</p> <code>truncate_at_max_tokens</code> <code>Optional[int]</code> <p>Optional maximum token limit for conversation.</p> <code>truncate_at_max_image_tokens</code> <code>Optional[int]</code> <p>Optional maximum image token limit.</p> <code>output_modalities</code> <code>Optional[List[str]]</code> <p>Optional list of output types the model can produce (e.g., 'text', 'image', 'audio').</p> <code>identifier</code> <code>str</code> <p>Unique identifier for this conversation.</p> <code>references</code> <code>List[Any]</code> <p>Optional list of reference materials, ground truth data, or expected outputs for evaluation.</p> <code>rating</code> <code>Optional[float]</code> <p>Optional quality rating or score (typically 0.0 to 1.0).</p> <code>source</code> <code>Optional[str]</code> <p>Optional source or origin (e.g., 'human-generated', 'synthetic').</p> <code>training_masks_strategy</code> <code>str</code> <p>Strategy for applying attention masks during training (e.g., 'full', 'partial', 'causal').</p> <code>custom_training_masks</code> <code>Optional[Dict[str, Any]]</code> <p>Optional custom mask configuration for advanced masking strategies.</p> Example <pre><code>conversation = Conversation(\n    meta={\"task\": \"qa\", \"difficulty\": \"hard\"},\n    messages=[message1, message2],\n    identifier=\"conv_001\",\n    training_masks_strategy=\"causal\",\n    source=\"human-generated\"\n)\n</code></pre> Source code in <code>mol_gen_docking/data/pydantic_dataset.py</code> <pre><code>class Conversation(BaseModel):\n    \"\"\"Complete conversation structure with messages and metadata.\n\n    This model represents a full conversation between a user and assistant,\n    including all messages, system configuration, and evaluation metrics.\n    The meta field should contain all information necessary to compute\n    rewards, ground truth values, and other evaluation metrics.\n\n    Attributes:\n        meta: Metadata dictionary containing evaluation and reward computation data.\n        messages: List of Message objects representing conversation turns.\n        system_prompt: Optional system prompt that defines assistant behavior.\n        available_tools: Optional list of tool names available to the assistant\n            (e.g., 'calculator', 'web_search', 'code_interpreter').\n        truncate_at_max_tokens: Optional maximum token limit for conversation.\n        truncate_at_max_image_tokens: Optional maximum image token limit.\n        output_modalities: Optional list of output types the model can produce\n            (e.g., 'text', 'image', 'audio').\n        identifier: Unique identifier for this conversation.\n        references: Optional list of reference materials, ground truth data,\n            or expected outputs for evaluation.\n        rating: Optional quality rating or score (typically 0.0 to 1.0).\n        source: Optional source or origin (e.g., 'human-generated', 'synthetic').\n        training_masks_strategy: Strategy for applying attention masks during\n            training (e.g., 'full', 'partial', 'causal').\n        custom_training_masks: Optional custom mask configuration for\n            advanced masking strategies.\n\n    Example:\n        ```python\n        conversation = Conversation(\n            meta={\"task\": \"qa\", \"difficulty\": \"hard\"},\n            messages=[message1, message2],\n            identifier=\"conv_001\",\n            training_masks_strategy=\"causal\",\n            source=\"human-generated\"\n        )\n        ```\n    \"\"\"\n\n    meta: Dict[str, Any] = Field(\n        ..., description=\"Metadata for reward computation and evaluation (required)\"\n    )\n    messages: List[Message] = Field(\n        ..., description=\"List of messages in conversation order\"\n    )\n    system_prompt: Optional[str] = Field(\n        None, description=\"System prompt defining assistant behavior and constraints\"\n    )\n    available_tools: Optional[List[str]] = Field(\n        None, description=\"List of tools available for the assistant to use\"\n    )\n    truncate_at_max_tokens: Optional[int] = Field(\n        None, description=\"Maximum token count before truncation\"\n    )\n    truncate_at_max_image_tokens: Optional[int] = Field(\n        None, description=\"Maximum image token count before truncation\"\n    )\n    output_modalities: Optional[List[str]] = Field(\n        None, description=\"Supported output modalities (text, image, audio, etc.)\"\n    )\n    identifier: str = Field(..., description=\"Unique conversation identifier\")\n    references: List[Any] = Field(\n        default_factory=list,\n        description=\"Reference data for evaluation and ground truth\",\n    )\n    rating: Optional[float] = Field(\n        None, description=\"Quality rating (typically 0.0-1.0 scale)\"\n    )\n    source: Optional[str] = Field(\n        None, description=\"Source or origin of the conversation\"\n    )\n    training_masks_strategy: str = Field(\n        ..., description=\"Attention mask strategy during training\"\n    )\n    custom_training_masks: Optional[Dict[str, Any]] = Field(\n        None, description=\"Custom mask configuration\"\n    )\n</code></pre>"},{"location":"api/data/dataset/#mol_gen_docking.data.pydantic_dataset.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a single message in a conversation.</p> <p>A message is a single turn in a multi-turn conversation. It contains the speaker's role, the message content, and optional metadata.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal['system', 'user', 'assistant']</code> <p>The role of the message sender (system, user, or assistant).</p> <code>content</code> <code>str</code> <p>The text content of the message.</p> <code>meta</code> <code>Dict[str, Any]</code> <p>Optional metadata dictionary (e.g., timestamps, token counts).</p> <code>identifier</code> <code>Optional[str]</code> <p>Optional unique identifier for this message.</p> <code>multimodal_document</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary containing multimodal content like images, audio, or file attachments.</p> Example <pre><code>message = Message(\n    role=\"user\",\n    content=\"What is the capital of France?\",\n    meta={\"tokens\": 10}\n)\n</code></pre> Source code in <code>mol_gen_docking/data/pydantic_dataset.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"Represents a single message in a conversation.\n\n    A message is a single turn in a multi-turn conversation. It contains\n    the speaker's role, the message content, and optional metadata.\n\n    Attributes:\n        role: The role of the message sender (system, user, or assistant).\n        content: The text content of the message.\n        meta: Optional metadata dictionary (e.g., timestamps, token counts).\n        identifier: Optional unique identifier for this message.\n        multimodal_document: Optional dictionary containing multimodal content\n            like images, audio, or file attachments.\n\n    Example:\n        ```python\n        message = Message(\n            role=\"user\",\n            content=\"What is the capital of France?\",\n            meta={\"tokens\": 10}\n        )\n        ```\n    \"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\"] = Field(\n        ..., description=\"The role of the sender: 'system', 'user', or 'assistant'\"\n    )\n    content: str = Field(..., description=\"The text content of the message\")\n    meta: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Optional metadata (timestamps, token counts, flags, etc.)\",\n    )\n    identifier: Optional[str] = Field(\n        None, description=\"Optional unique identifier for the message\"\n    )\n    multimodal_document: Optional[Dict[str, Any]] = Field(\n        None, description=\"Optional multimodal content (images, audio, files, etc.)\"\n    )\n</code></pre>"},{"location":"api/data/dataset/#mol_gen_docking.data.pydantic_dataset.Sample","title":"<code>Sample</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Root model containing all conversations for a single sample.</p> <p>A sample is the top-level container that groups one or more related conversations together with shared metadata and optional trajectories. This is typically the unit used for dataset serialization.</p> <p>Attributes:</p> Name Type Description <code>identifier</code> <code>str</code> <p>Unique identifier for this sample.</p> <code>conversations</code> <code>List[Conversation]</code> <p>List of Conversation objects in this sample.</p> <code>trajectories</code> <code>List[Any]</code> <p>Optional list of trajectories or execution paths for this sample (e.g., for reasoning or planning tasks).</p> <code>meta</code> <code>Dict[str, Any]</code> <p>Optional metadata about the sample (difficulty, domain, etc.).</p> <code>source</code> <code>Optional[str]</code> <p>Optional source or origin (e.g., 'benchmark', 'user-submitted').</p> Example <pre><code>sample = Sample(\n    identifier=\"sample_001\",\n    conversations=[conversation1, conversation2],\n    meta={\"domain\": \"chemistry\", \"difficulty\": \"expert\"},\n    source=\"benchmark\"\n)\n</code></pre> Source code in <code>mol_gen_docking/data/pydantic_dataset.py</code> <pre><code>class Sample(BaseModel):\n    \"\"\"Root model containing all conversations for a single sample.\n\n    A sample is the top-level container that groups one or more related\n    conversations together with shared metadata and optional trajectories.\n    This is typically the unit used for dataset serialization.\n\n    Attributes:\n        identifier: Unique identifier for this sample.\n        conversations: List of Conversation objects in this sample.\n        trajectories: Optional list of trajectories or execution paths\n            for this sample (e.g., for reasoning or planning tasks).\n        meta: Optional metadata about the sample (difficulty, domain, etc.).\n        source: Optional source or origin (e.g., 'benchmark', 'user-submitted').\n\n    Example:\n        ```python\n        sample = Sample(\n            identifier=\"sample_001\",\n            conversations=[conversation1, conversation2],\n            meta={\"domain\": \"chemistry\", \"difficulty\": \"expert\"},\n            source=\"benchmark\"\n        )\n        ```\n    \"\"\"\n\n    identifier: str = Field(..., description=\"Unique sample identifier\")\n    conversations: List[Conversation] = Field(\n        ..., description=\"List of conversations in this sample\"\n    )\n    trajectories: List[Any] = Field(\n        default_factory=list, description=\"Optional trajectories or execution paths\"\n    )\n    meta: Dict[str, Any] = Field(\n        default_factory=dict,\n        description=\"Sample metadata (difficulty, domain, annotations, etc.)\",\n    )\n    source: Optional[str] = Field(None, description=\"Source or origin of the sample\")\n</code></pre>"},{"location":"api/data/dataset/#mol_gen_docking.data.pydantic_dataset.read_jsonl","title":"<code>read_jsonl(input_file)</code>","text":"<p>Read Sample objects from a JSONL file.</p> <p>Each line in the file should be a valid JSON object representing a Sample. Lines are parsed sequentially into Sample instances.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>Path</code> <p>Path to the input JSONL file.</p> required <p>Returns:</p> Type Description <code>list[Sample]</code> <p>List of Sample objects parsed from the file.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the input file does not exist.</p> Example <pre><code>from pathlib import Path\nsamples = read_jsonl(Path(\"data/input.jsonl\"))\nfor sample in samples:\n    print(sample.identifier)\n</code></pre> Source code in <code>mol_gen_docking/data/pydantic_dataset.py</code> <pre><code>def read_jsonl(input_file: Path) -&gt; list[Sample]:\n    \"\"\"Read Sample objects from a JSONL file.\n\n    Each line in the file should be a valid JSON object representing\n    a Sample. Lines are parsed sequentially into Sample instances.\n\n    Args:\n        input_file: Path to the input JSONL file.\n\n    Returns:\n        List of Sample objects parsed from the file.\n\n    Raises:\n        AssertionError: If the input file does not exist.\n\n    Example:\n        ```python\n        from pathlib import Path\n        samples = read_jsonl(Path(\"data/input.jsonl\"))\n        for sample in samples:\n            print(sample.identifier)\n        ```\n    \"\"\"\n    assert input_file.exists(), input_file\n\n    samples: list[Sample] = []\n    with open(input_file) as fin:\n        for line in fin:\n            samples.append(Sample(**json.loads(line)))\n\n    return samples\n</code></pre>"},{"location":"api/data/dataset/#mol_gen_docking.data.pydantic_dataset.write_jsonl","title":"<code>write_jsonl(output_file, samples)</code>","text":"<p>Write Sample objects to a JSONL file.</p> <p>Each sample is serialized to JSON and written on a separate line. Parent directories are created automatically if they don't exist.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>Path</code> <p>Path to the output JSONL file.</p> required <code>samples</code> <code>list[Sample]</code> <p>List of Sample objects to write.</p> required Example <pre><code>from pathlib import Path\nsamples = [sample1, sample2, sample3]\nwrite_jsonl(Path(\"data/output.jsonl\"), samples)\n</code></pre> Source code in <code>mol_gen_docking/data/pydantic_dataset.py</code> <pre><code>def write_jsonl(output_file: Path, samples: list[Sample]) -&gt; None:\n    \"\"\"Write Sample objects to a JSONL file.\n\n    Each sample is serialized to JSON and written on a separate line.\n    Parent directories are created automatically if they don't exist.\n\n    Args:\n        output_file: Path to the output JSONL file.\n        samples: List of Sample objects to write.\n\n    Example:\n        ```python\n        from pathlib import Path\n        samples = [sample1, sample2, sample3]\n        write_jsonl(Path(\"data/output.jsonl\"), samples)\n        ```\n    \"\"\"\n    output_file.parent.mkdir(exist_ok=True, parents=True)\n\n    with open(output_file, \"w\") as fout:\n        for sample in samples:\n            fout.write(json.dumps(sample.model_dump(), ensure_ascii=False) + \"\\n\")\n</code></pre>"},{"location":"api/data/meeko_process/","title":"Meeko Process Module","text":"<p>A script to process receptors using Meeko in order to prepare them for molecular docking with AutoDock-GPU.</p>"},{"location":"api/data/meeko_process/#mol_gen_docking.data.meeko_process.ReceptorProcess","title":"<code>ReceptorProcess</code>","text":"<p>Receptor processing pipeline for preparing proteins for AutoDock-GPU molecular docking.</p> <p>This class provides a complete workflow for processing receptor PDB files through Meeko's <code>mk_prepare_receptor.py</code> tool and AutoGrid4. It handles common issues like unrecognized residues and heteroatoms, and supports parallel processing via Ray.</p> <p>The processing pipeline consists of three main steps:</p> <ol> <li> <p>Preprocessing (optional): Remove heteroatoms (ligands, waters, ions) from the    receptor using PDBFixer/OpenMM.</p> </li> <li> <p>Meeko processing: Convert PDB to PDBQT format and generate grid parameter    files (.gpf) with specified docking box dimensions.</p> </li> <li> <p>AutoGrid computation: Precompute affinity maps for faster docking.</p> </li> </ol> <p>The class handles edge cases gracefully:</p> <ul> <li>Skips already-processed receptors (checks for <code>_ag.pdbqt</code> and <code>_ag.maps.fld</code>)</li> <li>Retries with <code>--allow_bad_res</code> flag if unrecognized residues are outside the   docking box (non-critical)</li> <li>Reports critical errors when unrecognized residues are inside the docking box</li> </ul> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>Root directory containing receptor data.</p> <code>pre_process_receptors</code> <code>bool</code> <p>Whether to remove heteroatoms before Meeko.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for this class.</p> <code>receptor_path</code> <code>str</code> <p>Path to directory containing PDB files.</p> <code>pockets</code> <code>Dict[str, Dict[str, Any]]</code> <p>Docking box specifications loaded from <code>pockets_info.json</code>. Each entry contains 'center' and 'size' keys.</p> <code>cmd</code> <code>str</code> <p>Template command for mk_prepare_receptor.py.</p> Example <pre><code>import ray\nray.init()\n\nprocessor = ReceptorProcess(\n    data_path=\"/path/to/data\",\n    pre_process_receptors=True\n)\n\n# Process all receptors defined in pockets_info.json\nwarnings, errors = processor.process_receptors(use_pbar=True)\n\n# Process specific receptors\nwarnings, errors = processor.process_receptors(\n    receptors=[\"1abc\", \"2def\"],\n    allow_bad_res=True\n)\n</code></pre> Note <p>Requires Ray to be initialized before calling <code>process_receptors()</code>. External dependencies: Meeko, AutoGrid4, OpenMM (optional), PDBFixer (optional).</p> Source code in <code>mol_gen_docking/data/meeko_process.py</code> <pre><code>class ReceptorProcess:\n    \"\"\"Receptor processing pipeline for preparing proteins for AutoDock-GPU molecular docking.\n\n    This class provides a complete workflow for processing receptor PDB files through\n    Meeko's `mk_prepare_receptor.py` tool and AutoGrid4. It handles common issues like\n    unrecognized residues and heteroatoms, and supports parallel processing via Ray.\n\n    The processing pipeline consists of three main steps:\n\n    1. **Preprocessing (optional)**: Remove heteroatoms (ligands, waters, ions) from the\n       receptor using PDBFixer/OpenMM.\n\n    2. **Meeko processing**: Convert PDB to PDBQT format and generate grid parameter\n       files (.gpf) with specified docking box dimensions.\n\n    3. **AutoGrid computation**: Precompute affinity maps for faster docking.\n\n    The class handles edge cases gracefully:\n\n    - Skips already-processed receptors (checks for `_ag.pdbqt` and `_ag.maps.fld`)\n    - Retries with `--allow_bad_res` flag if unrecognized residues are outside the\n      docking box (non-critical)\n    - Reports critical errors when unrecognized residues are inside the docking box\n\n    Attributes:\n        data_path (str): Root directory containing receptor data.\n        pre_process_receptors (bool): Whether to remove heteroatoms before Meeko.\n        logger (logging.Logger): Logger instance for this class.\n        receptor_path (str): Path to directory containing PDB files.\n        pockets (Dict[str, Dict[str, Any]]): Docking box specifications loaded from\n            `pockets_info.json`. Each entry contains 'center' and 'size' keys.\n        cmd (str): Template command for mk_prepare_receptor.py.\n\n    Example:\n        ```python\n        import ray\n        ray.init()\n\n        processor = ReceptorProcess(\n            data_path=\"/path/to/data\",\n            pre_process_receptors=True\n        )\n\n        # Process all receptors defined in pockets_info.json\n        warnings, errors = processor.process_receptors(use_pbar=True)\n\n        # Process specific receptors\n        warnings, errors = processor.process_receptors(\n            receptors=[\"1abc\", \"2def\"],\n            allow_bad_res=True\n        )\n        ```\n\n    Note:\n        Requires Ray to be initialized before calling `process_receptors()`.\n        External dependencies: Meeko, AutoGrid4, OpenMM (optional), PDBFixer (optional).\n    \"\"\"\n\n    def __init__(self, data_path: str, pre_process_receptors: bool = False) -&gt; None:\n        \"\"\"Initialize the receptor processor with data paths and configuration.\n\n        Validates that required files and directories exist, and loads the docking\n        box specifications from `pockets_info.json`.\n\n        Args:\n            data_path (str): Path to the data directory. Must contain:\n\n                - `pdb_files/`: Directory with receptor PDB files\n                - `pockets_info.json`: JSON file mapping receptor names to docking\n                  box specifications with 'center' [x, y, z] and 'size' [sx, sy, sz]\n\n            pre_process_receptors (bool): If True, remove heteroatoms (ligands,\n                waters, ions) from receptors before Meeko processing. Uses PDBFixer.\n                Default: False.\n\n        Raises:\n            AssertionError: If `data_path` does not exist.\n            AssertionError: If `pdb_files/` subdirectory does not exist.\n            AssertionError: If `pockets_info.json` does not exist.\n        \"\"\"\n        self.data_path: str = data_path\n        self.pre_process_receptors: bool = pre_process_receptors\n        self.logger = logging.getLogger(\n            __name__ + \"/\" + self.__class__.__name__,\n        )\n        self.receptor_path = os.path.join(self.data_path, \"pdb_files\")\n\n        assert os.path.exists(data_path), f\"Data path {data_path} does not exist.\"\n        assert os.path.exists(self.receptor_path), (\n            f\"Receptor path {self.receptor_path} does not exist.\"\n        )\n        assert os.path.exists(os.path.join(data_path, \"pockets_info.json\")), (\n            f\"Pockets info file does not exist in {data_path}.\"\n        )\n\n        with open(os.path.join(data_path, \"pockets_info.json\")) as f:\n            self.pockets: Dict[str, Dict[str, Any]] = json.load(f)\n        self.cmd = \"mk_prepare_receptor.py -i {INPUT} -o {OUTPUT} -p -g --box_size {BOX_SIZE} --box_center {BOX_CENTER}\"\n\n    def _run_meeko(\n        self, input_path: str, receptor: str, bad_res: bool = False\n    ) -&gt; Tuple[str, int, str]:\n        \"\"\"Execute Meeko's mk_prepare_receptor.py on a single receptor.\n\n        Runs the Meeko command to convert a PDB file to PDBQT format and generate\n        grid parameter files for AutoDock-GPU. The docking box dimensions are\n        retrieved from the loaded pockets configuration.\n\n        Args:\n            input_path (str): Full path to the input PDB file.\n            receptor (str): Receptor identifier (key in pockets dict) used to\n                look up docking box center and size.\n            bad_res (bool): If True, add `--allow_bad_res` flag to permit\n                unrecognized residues. Default: False.\n\n        Returns:\n            Tuple[str, int, str]: A tuple containing:\n\n                - stderr_text: Standard error output from Meeko (useful for\n                  parsing error messages about unrecognized residues)\n                - returncode: Process return code (0 = success)\n                - output_path: Path to output files (without extension)\n\n        Raises:\n            subprocess.TimeoutExpired: If Meeko takes longer than 300 seconds.\n        \"\"\"\n        output_path = input_path.replace(\".pdb\", \"_ag\")\n\n        box_center = \" \".join([str(x) for x in self.pockets[receptor][\"center\"]])\n        box_size = \" \".join([str(x) for x in self.pockets[receptor][\"size\"]])\n\n        command = self.cmd.format(\n            INPUT=input_path,\n            OUTPUT=output_path,\n            BOX_SIZE=box_size,\n            BOX_CENTER=box_center,\n        )\n        if bad_res:\n            command += \" --allow_bad_res\"\n\n        self.logger.info(f\"Running command: {command}\")\n        process = sp.Popen(\n            command,\n            shell=True,\n            stdout=sp.DEVNULL,\n            stderr=sp.PIPE,\n            preexec_fn=os.setpgrp,\n        )\n        _, stderr = process.communicate(\n            timeout=300,\n        )\n\n        stderr_text = stderr.decode(\"utf-8\")\n\n        return stderr_text, process.returncode, output_path\n\n    def meeko_process(\n        self, receptor: str, allow_bad_res: bool = False\n    ) -&gt; Tuple[int, str]:\n        \"\"\"Process a receptor with Meeko, handling unrecognized residue errors.\n\n        This method implements smart error handling for Meeko failures:\n\n        1. First attempts processing without `--allow_bad_res`\n        2. If unrecognized residues are found, checks if they're inside the docking box\n        3. If outside the box: retries with `--allow_bad_res` (returns 0)\n        4. If inside the box: retries with `--allow_bad_res` but flags as warning (returns 1)\n\n        Args:\n            receptor (str): Receptor identifier matching a key in `pockets_info.json`\n                and a file `{receptor}.pdb` in the pdb_files directory.\n            allow_bad_res (bool): If True, always use `--allow_bad_res` flag.\n                Default: False.\n\n        Returns:\n            Tuple[int, str]: A tuple containing:\n\n                - status code:\n                    - 0: Success (no issues or issues outside docking box)\n                    - 1: Warning (unrecognized residues inside docking box)\n                - output_path: Path to the processed output files (without extension)\n\n        Raises:\n            ValueError: If Meeko fails for reasons other than unrecognized residues.\n        \"\"\"\n        input_path = os.path.join(self.receptor_path, f\"{receptor}.pdb\")\n        stderr_text, returncode, output_path = self._run_meeko(\n            input_path, receptor, allow_bad_res\n        )\n        if returncode != 0:\n            # Check if failing resiudes are inside the docking box\n            failed_residues = set()\n            for line in stderr_text.splitlines():\n                match = re.search(\n                    r\"No template matched for residue_key='(\\w+:\\d+)'\", line\n                )\n                if match:\n                    failed_residues.add(match.group(1))\n\n            if failed_residues:\n                residues_in_box = check_failed_residues_in_box(\n                    input_path,\n                    failed_residues,\n                    self.pockets[receptor][\"center\"],\n                    self.pockets[receptor][\"size\"],\n                )\n                if residues_in_box == []:\n                    _, returncode, output_path = self._run_meeko(\n                        input_path, receptor, bad_res=True\n                    )\n                    return returncode, output_path\n                else:\n                    _, returncode, output_path = self._run_meeko(\n                        input_path, receptor, bad_res=True\n                    )\n                    return 1, output_path\n            else:\n                raise ValueError(\n                    f\"Error in Meeko processing {receptor}:\\n{stderr_text}\"\n                )\n        return 0, output_path\n\n    def _run_autogrid(self, path: str) -&gt; None:\n        \"\"\"Run AutoGrid4 to precompute affinity maps for a processed receptor.\n\n        Executes `autogrid4` on the grid parameter file (.gpf) generated by Meeko.\n        The resulting map files are required for AutoDock-GPU docking.\n\n        Args:\n            path (str): Path to the receptor files (with or without extension).\n                The basename is extracted and used to locate the .gpf file\n                in the receptor_path directory.\n\n        Raises:\n            RuntimeError: If AutoGrid4 returns a non-zero exit code.\n\n        Note:\n            AutoGrid4 must be installed and available in PATH.\n        \"\"\"\n        path = os.path.basename(path)\n        grid_command = f\"autogrid4 -p {path}.gpf -l {path}.glg -d\"\n        self.logger.info(f\"Running command: {grid_command}\")\n        process = sp.Popen(\n            grid_command,\n            shell=True,\n            stdout=sp.PIPE,\n            stderr=sp.PIPE,\n            cwd=self.receptor_path,\n        )\n        stdout, stderr = process.communicate()\n        if process.returncode != 0:\n            self.logger.error(\n                f\"Error in running AutoGrid on {path}:\\n{stderr.decode()}\"\n            )\n            raise RuntimeError(f\"AutoGrid failed for {path}\")\n        self.logger.info(f\"Successfully ran AutoGrid on {path}\")\n\n    def remove_heterogenous(self, receptor: str) -&gt; None:\n        \"\"\"Remove heteroatoms from a receptor PDB file using PDBFixer.\n\n        Removes all heteroatoms including ligands, water molecules, and ions\n        from the receptor structure. The cleaned structure overwrites the\n        original PDB file.\n\n        Args:\n            receptor (str): Receptor identifier. The file\n                `{receptor_path}/{receptor}.pdb` will be modified in-place.\n\n        Note:\n            Requires OpenMM and PDBFixer packages.\n\n        Warning:\n            This method modifies the original PDB file. Make a backup if needed.\n        \"\"\"\n        from openmm.app import PDBFile\n        from pdbfixer import PDBFixer\n\n        pdb_file = os.path.join(self.receptor_path, f\"{receptor}.pdb\")\n\n        fixer = PDBFixer(filename=pdb_file)\n        fixer.removeHeterogens(True)\n\n        PDBFile.writeFile(fixer.topology, fixer.positions, open(pdb_file, \"w\"))\n\n    def process_receptors(\n        self,\n        receptors: list[str] = [],\n        allow_bad_res: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; Tuple[list[str], list[str]]:\n        \"\"\"Process multiple receptors in parallel using Ray.\n\n        Orchestrates the full processing pipeline for a batch of receptors:\n\n        1. Filters out already-processed receptors (with existing `_ag.pdbqt`\n           and `_ag.maps.fld` files)\n        2. Optionally removes heteroatoms (if `pre_process_receptors=True`)\n        3. Runs Meeko processing with error recovery\n        4. Runs AutoGrid4 for successful conversions\n        5. Collects and categorizes failures\n\n        Processing is parallelized using Ray remote functions, with each receptor\n        allocated 4 CPU cores.\n\n        Args:\n            receptors (list[str]): List of receptor identifiers to process.\n                If empty, processes all receptors in `pockets_info.json`.\n                Default: [].\n            allow_bad_res (bool): If True, always use `--allow_bad_res` flag\n                in Meeko. Default: False.\n            use_pbar (bool): If True, display a tqdm progress bar via Ray.\n                Default: False.\n\n        Returns:\n            Tuple[list[str], list[str]]: Two lists of receptor identifiers:\n\n                - warnings: Receptors with non-critical issues (unrecognized\n                  residues inside docking box, but processing completed)\n                - errors: Receptors with critical failures (exceptions during\n                  processing)\n\n        Raises:\n            AssertionError: If any receptor in the list is not found in\n                `pockets_info.json`.\n\n        Note:\n            Ray must be initialized before calling this method.\n        \"\"\"\n\n        @ray.remote(num_cpus=4)\n        def process_receptor(\n            receptor: str,\n            pbar: Any,\n            allow_bad_res: bool = False,\n        ) -&gt; int:\n            \"\"\"\n            Outputs the level of the error:\n            0: Success\n            1: Failed residues inside the box\n            2: Other errors (critical)\n            \"\"\"\n            try:\n                if self.pre_process_receptors:\n                    self.remove_heterogenous(receptor)\n                result, processed_path = self.meeko_process(receptor, allow_bad_res)\n                if result &lt;= 1:\n                    self._run_autogrid(processed_path)\n                if pbar is not None:\n                    pbar.update.remote(1)\n            except Exception as e:\n                self.logger.error(f\"Error processing {receptor}: {e}\")\n                if pbar is not None:\n                    pbar.update.remote(1)\n                return 2\n            return result\n\n        if receptors == []:\n            receptors = list(self.pockets.keys())\n        else:\n            assert all(r in self.pockets for r in receptors), (\n                \"Some receptors are not in pockets_info.json: \"\n                + \",\".join([r for r in receptors if r not in self.pockets])\n            )\n\n        remote_tqdm = ray.remote(tqdm_ray.tqdm)\n        if use_pbar:\n            pbar = remote_tqdm.remote(total=len(receptors), desc=\"Processing receptors\")\n        else:\n            pbar = None\n        # Find receptors that already have a _ag.pdbqt and_ag.maps.fld file\n        receptors_to_process = []\n        for receptor in receptors:\n            ag_pdbqt_path = os.path.join(self.receptor_path, f\"{receptor}_ag.pdbqt\")\n            ag_maps_fld_path = os.path.join(\n                self.receptor_path, f\"{receptor}_ag.maps.fld\"\n            )\n            if not (os.path.exists(ag_pdbqt_path) and os.path.exists(ag_maps_fld_path)):\n                receptors_to_process.append(receptor)\n            else:\n                self.logger.info(f\"Receptor {receptor} already processed. Skipping.\")\n                if use_pbar:\n                    pbar.update.remote(1)  # type: ignore\n        if len(receptors_to_process) == 0:\n            self.logger.info(\"All receptors already processed.\")\n            if use_pbar:\n                pbar.close.remote()  # type: ignore\n            return [], []\n\n        self.logger.info(f\"Processing {len(receptors_to_process)} receptors.\")\n        futures = [\n            process_receptor.remote(receptor, pbar, allow_bad_res)\n            for receptor in receptors_to_process\n        ]\n        results = ray.get(futures)\n\n        missed_receptors_1 = [\n            receptor\n            for receptor, success in zip(receptors_to_process, results)\n            if success == 1\n        ]\n        missed_receptors_2 = [\n            receptor\n            for receptor, success in zip(receptors_to_process, results)\n            if success == 2\n        ]\n        return missed_receptors_1, missed_receptors_2\n</code></pre>"},{"location":"api/data/meeko_process/#mol_gen_docking.data.meeko_process.ReceptorProcess.__init__","title":"<code>__init__(data_path, pre_process_receptors=False)</code>","text":"<p>Initialize the receptor processor with data paths and configuration.</p> <p>Validates that required files and directories exist, and loads the docking box specifications from <code>pockets_info.json</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the data directory. Must contain:</p> <ul> <li><code>pdb_files/</code>: Directory with receptor PDB files</li> <li><code>pockets_info.json</code>: JSON file mapping receptor names to docking   box specifications with 'center' [x, y, z] and 'size' [sx, sy, sz]</li> </ul> required <code>pre_process_receptors</code> <code>bool</code> <p>If True, remove heteroatoms (ligands, waters, ions) from receptors before Meeko processing. Uses PDBFixer. Default: False.</p> <code>False</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>data_path</code> does not exist.</p> <code>AssertionError</code> <p>If <code>pdb_files/</code> subdirectory does not exist.</p> <code>AssertionError</code> <p>If <code>pockets_info.json</code> does not exist.</p> Source code in <code>mol_gen_docking/data/meeko_process.py</code> <pre><code>def __init__(self, data_path: str, pre_process_receptors: bool = False) -&gt; None:\n    \"\"\"Initialize the receptor processor with data paths and configuration.\n\n    Validates that required files and directories exist, and loads the docking\n    box specifications from `pockets_info.json`.\n\n    Args:\n        data_path (str): Path to the data directory. Must contain:\n\n            - `pdb_files/`: Directory with receptor PDB files\n            - `pockets_info.json`: JSON file mapping receptor names to docking\n              box specifications with 'center' [x, y, z] and 'size' [sx, sy, sz]\n\n        pre_process_receptors (bool): If True, remove heteroatoms (ligands,\n            waters, ions) from receptors before Meeko processing. Uses PDBFixer.\n            Default: False.\n\n    Raises:\n        AssertionError: If `data_path` does not exist.\n        AssertionError: If `pdb_files/` subdirectory does not exist.\n        AssertionError: If `pockets_info.json` does not exist.\n    \"\"\"\n    self.data_path: str = data_path\n    self.pre_process_receptors: bool = pre_process_receptors\n    self.logger = logging.getLogger(\n        __name__ + \"/\" + self.__class__.__name__,\n    )\n    self.receptor_path = os.path.join(self.data_path, \"pdb_files\")\n\n    assert os.path.exists(data_path), f\"Data path {data_path} does not exist.\"\n    assert os.path.exists(self.receptor_path), (\n        f\"Receptor path {self.receptor_path} does not exist.\"\n    )\n    assert os.path.exists(os.path.join(data_path, \"pockets_info.json\")), (\n        f\"Pockets info file does not exist in {data_path}.\"\n    )\n\n    with open(os.path.join(data_path, \"pockets_info.json\")) as f:\n        self.pockets: Dict[str, Dict[str, Any]] = json.load(f)\n    self.cmd = \"mk_prepare_receptor.py -i {INPUT} -o {OUTPUT} -p -g --box_size {BOX_SIZE} --box_center {BOX_CENTER}\"\n</code></pre>"},{"location":"api/data/meeko_process/#mol_gen_docking.data.meeko_process.ReceptorProcess.meeko_process","title":"<code>meeko_process(receptor, allow_bad_res=False)</code>","text":"<p>Process a receptor with Meeko, handling unrecognized residue errors.</p> <p>This method implements smart error handling for Meeko failures:</p> <ol> <li>First attempts processing without <code>--allow_bad_res</code></li> <li>If unrecognized residues are found, checks if they're inside the docking box</li> <li>If outside the box: retries with <code>--allow_bad_res</code> (returns 0)</li> <li>If inside the box: retries with <code>--allow_bad_res</code> but flags as warning (returns 1)</li> </ol> <p>Parameters:</p> Name Type Description Default <code>receptor</code> <code>str</code> <p>Receptor identifier matching a key in <code>pockets_info.json</code> and a file <code>{receptor}.pdb</code> in the pdb_files directory.</p> required <code>allow_bad_res</code> <code>bool</code> <p>If True, always use <code>--allow_bad_res</code> flag. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[int, str]</code> <p>Tuple[int, str]: A tuple containing:</p> <ul> <li>status code:<ul> <li>0: Success (no issues or issues outside docking box)</li> <li>1: Warning (unrecognized residues inside docking box)</li> </ul> </li> <li>output_path: Path to the processed output files (without extension)</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If Meeko fails for reasons other than unrecognized residues.</p> Source code in <code>mol_gen_docking/data/meeko_process.py</code> <pre><code>def meeko_process(\n    self, receptor: str, allow_bad_res: bool = False\n) -&gt; Tuple[int, str]:\n    \"\"\"Process a receptor with Meeko, handling unrecognized residue errors.\n\n    This method implements smart error handling for Meeko failures:\n\n    1. First attempts processing without `--allow_bad_res`\n    2. If unrecognized residues are found, checks if they're inside the docking box\n    3. If outside the box: retries with `--allow_bad_res` (returns 0)\n    4. If inside the box: retries with `--allow_bad_res` but flags as warning (returns 1)\n\n    Args:\n        receptor (str): Receptor identifier matching a key in `pockets_info.json`\n            and a file `{receptor}.pdb` in the pdb_files directory.\n        allow_bad_res (bool): If True, always use `--allow_bad_res` flag.\n            Default: False.\n\n    Returns:\n        Tuple[int, str]: A tuple containing:\n\n            - status code:\n                - 0: Success (no issues or issues outside docking box)\n                - 1: Warning (unrecognized residues inside docking box)\n            - output_path: Path to the processed output files (without extension)\n\n    Raises:\n        ValueError: If Meeko fails for reasons other than unrecognized residues.\n    \"\"\"\n    input_path = os.path.join(self.receptor_path, f\"{receptor}.pdb\")\n    stderr_text, returncode, output_path = self._run_meeko(\n        input_path, receptor, allow_bad_res\n    )\n    if returncode != 0:\n        # Check if failing resiudes are inside the docking box\n        failed_residues = set()\n        for line in stderr_text.splitlines():\n            match = re.search(\n                r\"No template matched for residue_key='(\\w+:\\d+)'\", line\n            )\n            if match:\n                failed_residues.add(match.group(1))\n\n        if failed_residues:\n            residues_in_box = check_failed_residues_in_box(\n                input_path,\n                failed_residues,\n                self.pockets[receptor][\"center\"],\n                self.pockets[receptor][\"size\"],\n            )\n            if residues_in_box == []:\n                _, returncode, output_path = self._run_meeko(\n                    input_path, receptor, bad_res=True\n                )\n                return returncode, output_path\n            else:\n                _, returncode, output_path = self._run_meeko(\n                    input_path, receptor, bad_res=True\n                )\n                return 1, output_path\n        else:\n            raise ValueError(\n                f\"Error in Meeko processing {receptor}:\\n{stderr_text}\"\n            )\n    return 0, output_path\n</code></pre>"},{"location":"api/data/meeko_process/#mol_gen_docking.data.meeko_process.ReceptorProcess.process_receptors","title":"<code>process_receptors(receptors=[], allow_bad_res=False, use_pbar=False)</code>","text":"<p>Process multiple receptors in parallel using Ray.</p> <p>Orchestrates the full processing pipeline for a batch of receptors:</p> <ol> <li>Filters out already-processed receptors (with existing <code>_ag.pdbqt</code>    and <code>_ag.maps.fld</code> files)</li> <li>Optionally removes heteroatoms (if <code>pre_process_receptors=True</code>)</li> <li>Runs Meeko processing with error recovery</li> <li>Runs AutoGrid4 for successful conversions</li> <li>Collects and categorizes failures</li> </ol> <p>Processing is parallelized using Ray remote functions, with each receptor allocated 4 CPU cores.</p> <p>Parameters:</p> Name Type Description Default <code>receptors</code> <code>list[str]</code> <p>List of receptor identifiers to process. If empty, processes all receptors in <code>pockets_info.json</code>. Default: [].</p> <code>[]</code> <code>allow_bad_res</code> <code>bool</code> <p>If True, always use <code>--allow_bad_res</code> flag in Meeko. Default: False.</p> <code>False</code> <code>use_pbar</code> <code>bool</code> <p>If True, display a tqdm progress bar via Ray. Default: False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[list[str], list[str]]</code> <p>Tuple[list[str], list[str]]: Two lists of receptor identifiers:</p> <ul> <li>warnings: Receptors with non-critical issues (unrecognized   residues inside docking box, but processing completed)</li> <li>errors: Receptors with critical failures (exceptions during   processing)</li> </ul> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any receptor in the list is not found in <code>pockets_info.json</code>.</p> Note <p>Ray must be initialized before calling this method.</p> Source code in <code>mol_gen_docking/data/meeko_process.py</code> <pre><code>def process_receptors(\n    self,\n    receptors: list[str] = [],\n    allow_bad_res: bool = False,\n    use_pbar: bool = False,\n) -&gt; Tuple[list[str], list[str]]:\n    \"\"\"Process multiple receptors in parallel using Ray.\n\n    Orchestrates the full processing pipeline for a batch of receptors:\n\n    1. Filters out already-processed receptors (with existing `_ag.pdbqt`\n       and `_ag.maps.fld` files)\n    2. Optionally removes heteroatoms (if `pre_process_receptors=True`)\n    3. Runs Meeko processing with error recovery\n    4. Runs AutoGrid4 for successful conversions\n    5. Collects and categorizes failures\n\n    Processing is parallelized using Ray remote functions, with each receptor\n    allocated 4 CPU cores.\n\n    Args:\n        receptors (list[str]): List of receptor identifiers to process.\n            If empty, processes all receptors in `pockets_info.json`.\n            Default: [].\n        allow_bad_res (bool): If True, always use `--allow_bad_res` flag\n            in Meeko. Default: False.\n        use_pbar (bool): If True, display a tqdm progress bar via Ray.\n            Default: False.\n\n    Returns:\n        Tuple[list[str], list[str]]: Two lists of receptor identifiers:\n\n            - warnings: Receptors with non-critical issues (unrecognized\n              residues inside docking box, but processing completed)\n            - errors: Receptors with critical failures (exceptions during\n              processing)\n\n    Raises:\n        AssertionError: If any receptor in the list is not found in\n            `pockets_info.json`.\n\n    Note:\n        Ray must be initialized before calling this method.\n    \"\"\"\n\n    @ray.remote(num_cpus=4)\n    def process_receptor(\n        receptor: str,\n        pbar: Any,\n        allow_bad_res: bool = False,\n    ) -&gt; int:\n        \"\"\"\n        Outputs the level of the error:\n        0: Success\n        1: Failed residues inside the box\n        2: Other errors (critical)\n        \"\"\"\n        try:\n            if self.pre_process_receptors:\n                self.remove_heterogenous(receptor)\n            result, processed_path = self.meeko_process(receptor, allow_bad_res)\n            if result &lt;= 1:\n                self._run_autogrid(processed_path)\n            if pbar is not None:\n                pbar.update.remote(1)\n        except Exception as e:\n            self.logger.error(f\"Error processing {receptor}: {e}\")\n            if pbar is not None:\n                pbar.update.remote(1)\n            return 2\n        return result\n\n    if receptors == []:\n        receptors = list(self.pockets.keys())\n    else:\n        assert all(r in self.pockets for r in receptors), (\n            \"Some receptors are not in pockets_info.json: \"\n            + \",\".join([r for r in receptors if r not in self.pockets])\n        )\n\n    remote_tqdm = ray.remote(tqdm_ray.tqdm)\n    if use_pbar:\n        pbar = remote_tqdm.remote(total=len(receptors), desc=\"Processing receptors\")\n    else:\n        pbar = None\n    # Find receptors that already have a _ag.pdbqt and_ag.maps.fld file\n    receptors_to_process = []\n    for receptor in receptors:\n        ag_pdbqt_path = os.path.join(self.receptor_path, f\"{receptor}_ag.pdbqt\")\n        ag_maps_fld_path = os.path.join(\n            self.receptor_path, f\"{receptor}_ag.maps.fld\"\n        )\n        if not (os.path.exists(ag_pdbqt_path) and os.path.exists(ag_maps_fld_path)):\n            receptors_to_process.append(receptor)\n        else:\n            self.logger.info(f\"Receptor {receptor} already processed. Skipping.\")\n            if use_pbar:\n                pbar.update.remote(1)  # type: ignore\n    if len(receptors_to_process) == 0:\n        self.logger.info(\"All receptors already processed.\")\n        if use_pbar:\n            pbar.close.remote()  # type: ignore\n        return [], []\n\n    self.logger.info(f\"Processing {len(receptors_to_process)} receptors.\")\n    futures = [\n        process_receptor.remote(receptor, pbar, allow_bad_res)\n        for receptor in receptors_to_process\n    ]\n    results = ray.get(futures)\n\n    missed_receptors_1 = [\n        receptor\n        for receptor, success in zip(receptors_to_process, results)\n        if success == 1\n    ]\n    missed_receptors_2 = [\n        receptor\n        for receptor, success in zip(receptors_to_process, results)\n        if success == 2\n    ]\n    return missed_receptors_1, missed_receptors_2\n</code></pre>"},{"location":"api/data/meeko_process/#mol_gen_docking.data.meeko_process.ReceptorProcess.remove_heterogenous","title":"<code>remove_heterogenous(receptor)</code>","text":"<p>Remove heteroatoms from a receptor PDB file using PDBFixer.</p> <p>Removes all heteroatoms including ligands, water molecules, and ions from the receptor structure. The cleaned structure overwrites the original PDB file.</p> <p>Parameters:</p> Name Type Description Default <code>receptor</code> <code>str</code> <p>Receptor identifier. The file <code>{receptor_path}/{receptor}.pdb</code> will be modified in-place.</p> required Note <p>Requires OpenMM and PDBFixer packages.</p> Warning <p>This method modifies the original PDB file. Make a backup if needed.</p> Source code in <code>mol_gen_docking/data/meeko_process.py</code> <pre><code>def remove_heterogenous(self, receptor: str) -&gt; None:\n    \"\"\"Remove heteroatoms from a receptor PDB file using PDBFixer.\n\n    Removes all heteroatoms including ligands, water molecules, and ions\n    from the receptor structure. The cleaned structure overwrites the\n    original PDB file.\n\n    Args:\n        receptor (str): Receptor identifier. The file\n            `{receptor_path}/{receptor}.pdb` will be modified in-place.\n\n    Note:\n        Requires OpenMM and PDBFixer packages.\n\n    Warning:\n        This method modifies the original PDB file. Make a backup if needed.\n    \"\"\"\n    from openmm.app import PDBFile\n    from pdbfixer import PDBFixer\n\n    pdb_file = os.path.join(self.receptor_path, f\"{receptor}.pdb\")\n\n    fixer = PDBFixer(filename=pdb_file)\n    fixer.removeHeterogens(True)\n\n    PDBFile.writeFile(fixer.topology, fixer.positions, open(pdb_file, \"w\"))\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/","title":"Diversity-Aware Top-k Metric","text":""},{"location":"api/evaluation/diversity_aware_top_k/#overview","title":"Overview","text":"<p>The diversity-aware top-k metric is an advanced evaluation metric for molecular generation tasks that balances both quality and chemical diversity. Unlike the standard top-k metric which only considers scores, this metric enforces that selected molecules are sufficiently different from each other.</p>"},{"location":"api/evaluation/diversity_aware_top_k/#key-concepts","title":"Key Concepts","text":""},{"location":"api/evaluation/diversity_aware_top_k/#diversity-constraint","title":"Diversity Constraint","text":"<p>The metric uses a similarity threshold (parameter <code>t</code>) to enforce diversity:</p> <ul> <li>Selected molecules must have Tanimoto similarity &lt; t with each other</li> <li>Lower <code>t</code> values enforce stricter diversity (e.g., t=0.1 requires very different molecules), and will be increasingly challenging as <code>t</code> decreases.</li> <li>Higher <code>t</code> values allow more similar molecules (e.g., t=0.9 is more lenient), and the results of the diversity metric should converge to the top-k score as <code>t</code> approaches 1 (see Top-k Metric).</li> </ul>"},{"location":"api/evaluation/diversity_aware_top_k/#greedy-selection-algorithm","title":"Greedy Selection Algorithm","text":"<p>The selection process uses a greedy approach:</p> <ol> <li>Sort molecules by score (highest first)</li> <li>Select the highest-scoring molecule</li> <li> <p>For each remaining molecule (in descending score order):</p> <ul> <li>If it's sufficiently different from all selected molecules, select it</li> <li>Otherwise, skip it and check the next candidate</li> </ul> </li> <li> <p>Stop when k molecules are selected or all candidates are evaluated</p> </li> </ol>"},{"location":"api/evaluation/diversity_aware_top_k/#padding-mechanism","title":"Padding Mechanism","text":"<p>If fewer than k diverse molecules are found, the remaining slots are padded with 0.0 scores, penalizing models unable to generate k molecules meeting the diversity-constraint.</p>"},{"location":"api/evaluation/diversity_aware_top_k/#molecular-fingerprints","title":"Molecular Fingerprints","text":"<p>The metric uses molecular fingerprints to compute similarity:</p> <ul> <li>Default: ECFP4-1024 (Extended Connectivity Fingerprint, diameter 4, 1024 bits)</li> <li>Supported: ECFP, MACCS, RDKit, Gobbi2D, Avalon</li> <li>See Fingerprint Utilities for details</li> </ul>"},{"location":"api/evaluation/diversity_aware_top_k/#usage-examples","title":"Usage Examples","text":""},{"location":"api/evaluation/diversity_aware_top_k/#basic-usage-with-smiles","title":"Basic Usage with SMILES","text":"<pre><code>from mol_gen_docking.evaluation.diversity_aware_top_k import diversity_aware_top_k\n\n# Generated molecules as SMILES\nsmiles = [\n    \"c1ccccc1\",                           # (score: 8.5)\n    \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",        # (score: 9.2)\n    \"c1ccc2ccccc2c1\",                    # (score: 8.0) - similar to benzene\n    \"CCO\"                                 # (score: 6.5)\n]\n\nscores = [8.5, 9.2, 8.0, 6.5]\n\n# Select top 2 molecules with similarity threshold 0.7\n# (molecules must have similarity &lt; 0.7, i.e., distance &gt; 0.3)\nmetric = diversity_aware_top_k(\n    smiles,\n    scores,\n    k=2,\n    t=0.9,\n    fingerprint_name=\"ecfp4-1024\"\n)\nprint(f\"Diversity-aware top-2 score: {metric}\")\n# Output:\n# &gt;&gt;&gt; Diversity-aware top-2 score: 8.85\nmetric = diversity_aware_top_k(\n    smiles,\n    scores,\n    k=2,\n    t=0.05,\n    fingerprint_name=\"ecfp4-1024\"\n)\nprint(f\"Diversity-aware top-2 score: {metric}\")\n\n# Output:\n# &gt;&gt;&gt; Diversity-aware top-2 score: 4.6\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/#using-rdkit-mol-objects","title":"Using RDKit Mol Objects","text":"<pre><code>from mol_gen_docking.evaluation.diversity_aware_top_k import diversity_aware_top_k\nfrom rdkit import Chem\n\n# Convert to Mol objects\nmols = [Chem.MolFromSmiles(smi) for smi in smiles]\n\n# Same interface works with Mol objects\nmetric = diversity_aware_top_k(mols, scores, k=3, t=0.7)\nprint(metric)\n\n# Output:\n# &gt;&gt;&gt; 8.566666666666666\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/#using-pre-computed-similarity-matrix","title":"Using Pre-computed Similarity Matrix","text":"<pre><code>import numpy as np\nfrom mol_gen_docking.evaluation.diversity_aware_top_k import diversity_aware_top_k\n\n# Pre-computed similarity matrix (diagonal = 1.0)\nsim_matrix = np.array([\n    [1.0, 0.3, 0.9, 0.2],   # benzene\n    [0.3, 1.0, 0.4, 0.6],   # ibuprofen\n    [0.9, 0.4, 1.0, 0.3],   # naphthalene\n    [0.2, 0.6, 0.3, 1.0]    # ethanol\n])\n\nscores = [8.5, 9.2, 8.0, 6.5]\n\n# Use similarity matrix directly\nmetric = diversity_aware_top_k(\n    sim_matrix,\n    scores,\n    k=2,\n    t=0.7\n)\nprint(metric)\n\n# Output:\n# &gt;&gt;&gt; 8.85\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/#function-reference","title":"Function Reference","text":"<p>Diversity-aware top-k evaluation metric for molecular generation.</p> <p>This module implements a diversity-aware variant of the top-k metric that selects molecules not only based on their scores but also on their chemical diversity. It ensures selected molecules are sufficiently different from each other, preventing the selection of similar redundant compounds.</p>"},{"location":"api/evaluation/diversity_aware_top_k/#mol_gen_docking.evaluation.diversity_aware_top_k.div_aware_top_k_from_dist","title":"<code>div_aware_top_k_from_dist(dist, weights, k, t)</code>","text":"<p>Select at most k molecules with highest weights while enforcing minimum distance.</p> <p>This function implements a greedy selection algorithm that selects molecules with the highest weights while ensuring each selected molecule is at distance (dissimilarity) of at least t from all previously selected molecules.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>ndarray[float]</code> <p>Condensed distance matrix (1D array of upper triangle distances). This should be from scipy.spatial.distance.squareform or similar. Distance values should be in range [0, 1] where 0 = identical, 1 = completely different.</p> required <code>weights</code> <code>ndarray[float]</code> <p>1D array of weights/scores for each molecule. Higher weights are selected first. Must have length n where n*(n-1)/2 == len(dist).</p> required <code>k</code> <code>int</code> <p>Maximum number of molecules to select.</p> required <code>t</code> <code>float</code> <p>Minimum distance threshold. Selected molecules must be at distance &gt;= t from each other (i.e., dissimilarity &gt;= t).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>1D NumPy array of indices of selected molecules, sorted by weight (descending).</p> <code>ndarray</code> <p>Array may contain fewer than k elements if not enough molecules satisfy</p> <code>ndarray</code> <p>the distance constraint.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the distance matrix size doesn't match the weights array.</p> Example <pre><code>import numpy as np\nfrom mol_gen_docking.evaluation.diversity_aware_top_k import div_aware_top_k_from_dist\n\n# 3 molecules: dist matrix has 3 pairwise distances\ndist = np.array([0.3, 0.8, 0.2])  # condensed distance matrix\nweights = np.array([8.5, 9.2, 7.1])\n\nselected = div_aware_top_k_from_dist(dist, weights, k=2, t=0.5)\nprint(f\"Selected indices: {selected}\")  # Molecules at distance &gt;= 0.5\n</code></pre> Notes <ul> <li>Uses a greedy algorithm: sorts by weight and selects molecules in order</li> <li>Once a molecule is selected, it acts as a constraint for future selections</li> <li>No backtracking: if a high-weight molecule can't be selected due to distance   constraints, it's skipped (lower-weight candidates are checked next)</li> </ul> Source code in <code>mol_gen_docking/evaluation/diversity_aware_top_k.py</code> <pre><code>def div_aware_top_k_from_dist(\n    dist: np.ndarray[float],\n    weights: np.ndarray[float],\n    k: int,\n    t: float,\n) -&gt; np.ndarray:\n    \"\"\"Select at most k molecules with highest weights while enforcing minimum distance.\n\n    This function implements a greedy selection algorithm that selects molecules\n    with the highest weights while ensuring each selected molecule is at distance\n    (dissimilarity) of at least t from all previously selected molecules.\n\n    Args:\n        dist: Condensed distance matrix (1D array of upper triangle distances).\n            This should be from scipy.spatial.distance.squareform or similar.\n            Distance values should be in range [0, 1] where 0 = identical, 1 = completely different.\n        weights: 1D array of weights/scores for each molecule. Higher weights are selected first.\n            Must have length n where n*(n-1)/2 == len(dist).\n        k: Maximum number of molecules to select.\n        t: Minimum distance threshold. Selected molecules must be at distance &gt;= t\n            from each other (i.e., dissimilarity &gt;= t).\n\n    Returns:\n        1D NumPy array of indices of selected molecules, sorted by weight (descending).\n        Array may contain fewer than k elements if not enough molecules satisfy\n        the distance constraint.\n\n    Raises:\n        AssertionError: If the distance matrix size doesn't match the weights array.\n\n    Example:\n        ```python\n        import numpy as np\n        from mol_gen_docking.evaluation.diversity_aware_top_k import div_aware_top_k_from_dist\n\n        # 3 molecules: dist matrix has 3 pairwise distances\n        dist = np.array([0.3, 0.8, 0.2])  # condensed distance matrix\n        weights = np.array([8.5, 9.2, 7.1])\n\n        selected = div_aware_top_k_from_dist(dist, weights, k=2, t=0.5)\n        print(f\"Selected indices: {selected}\")  # Molecules at distance &gt;= 0.5\n        ```\n\n    Notes:\n        - Uses a greedy algorithm: sorts by weight and selects molecules in order\n        - Once a molecule is selected, it acts as a constraint for future selections\n        - No backtracking: if a high-weight molecule can't be selected due to distance\n          constraints, it's skipped (lower-weight candidates are checked next)\n    \"\"\"\n    n = len(weights)\n    assert n * (n - 1) // 2 == len(dist), (\n        \"Distance matrix size does not match number of weights\"\n    )\n    selected: List[int] = []\n    sorted_indices = np.argsort(-weights)  # Sort indices by descending weights\n    dist_mat = squareform(dist)\n    dist_mat = dist_mat &lt; t\n\n    for idx in sorted_indices:\n        if len(selected) &gt;= k:\n            break\n        is_idx_too_close = dist_mat[idx, selected].any() if selected else False\n        if not is_idx_too_close:\n            selected.append(idx)\n    return np.array(selected)\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/#mol_gen_docking.evaluation.diversity_aware_top_k.diversity_aware_top_k","title":"<code>diversity_aware_top_k(mols, scores, k, t, fingerprint_name='ecfp4-1024')</code>","text":"<p>Calculate diversity-aware top-k metric for molecular generation.</p> <p>This function computes a diversity-aware top-k metric that selects up to k molecules with the highest scores, subject to the constraint that selected molecules must have chemical similarity below a threshold (i.e., dissimilarity above 1-t). This prevents selecting multiple similar molecules and encourages chemical diversity.</p> <p>Parameters:</p> Name Type Description Default <code>mols</code> <code>List[Mol] | List[str] | ndarray</code> <p>List of molecules in one of three formats: - List of SMILES strings (str) - List of RDKit Mol objects (Chem.Mol) - 2D NumPy array representing a similarity matrix</p> required <code>scores</code> <code>Sequence[float | int]</code> <p>Sequence of scores corresponding to each molecule (e.g., docking scores). Must have the same length as mols (unless mols is a similarity matrix).</p> required <code>k</code> <code>int</code> <p>Maximum number of molecules to select.</p> required <code>t</code> <code>float</code> <p>Similarity threshold (range 0.0 to 1.0). Selected molecules must have Tanimoto similarity &lt; t to be considered diverse enough. Lower values enforce higher diversity.</p> required <code>fingerprint_name</code> <code>Optional[str]</code> <p>Name of the molecular fingerprint to use for similarity calculation. Only used when mols are SMILES or Mol objects. Default is \"ecfp4-1024\". See mol_gen_docking.evaluation.fingeprints_utils.fp_name_to_fn for options.</p> <code>'ecfp4-1024'</code> <p>Returns:</p> Type Description <code>float</code> <p>Average score of the selected k diverse molecules. If fewer than k molecules</p> <code>float</code> <p>are selected due to diversity constraints, unselected slots are padded with 0.0.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If mols and scores have different lengths, or if input types are inconsistent.</p> Example <pre><code>from mol_gen_docking.evaluation.diversity_aware_top_k import diversity_aware_top_k\nfrom rdkit import Chem\n\n# Using SMILES strings\nsmiles = [\n    \"c1ccccc1\",                              # benzene\n    \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",           # ibuprofen\n    \"c1ccc2ccccc2c1\",                       # naphthalene (similar to benzene)\n    \"CCO\"                                    # ethanol\n]\nscores = [8.5, 9.2, 8.0, 6.5]\n\n# Select top 2 molecules with similarity threshold 0.8\nmetric = diversity_aware_top_k(\n    smiles, scores, k=2, t=0.8, fingerprint_name=\"ecfp4-1024\"\n)\nprint(f\"Diversity-aware top-2 score: {metric}\")\n\n# Using a pre-computed similarity matrix\nsim_matrix = np.array([\n    [1.0, 0.3, 0.9, 0.2],\n    [0.3, 1.0, 0.4, 0.6],\n    [0.9, 0.4, 1.0, 0.3],\n    [0.2, 0.6, 0.3, 1.0]\n])\nmetric = diversity_aware_top_k(\n    sim_matrix, scores, k=2, t=0.8\n)\n</code></pre> Notes <ul> <li>The function converts similarity matrices to distance matrices (1 - similarity)</li> <li>Higher t values (closer to 1.0) allow selection of more similar molecules</li> <li>Lower t values enforce stricter diversity constraints</li> <li>If a similarity matrix is provided directly, it should be a 2D NumPy array   with diagonal elements equal to 1.0</li> <li>Padding with 0.0 for unselected slots means diversity constraints can   result in lower average scores than unconstrained top-k</li> </ul> References <p>This metric is commonly used in molecular generation benchmarks to evaluate both quality and diversity of generated molecules (e.g., De Novo Generation task).</p> Source code in <code>mol_gen_docking/evaluation/diversity_aware_top_k.py</code> <pre><code>def diversity_aware_top_k(\n    mols: List[Chem.Mol] | List[str] | np.ndarray,\n    scores: Sequence[float | int],\n    k: int,\n    t: float,\n    fingerprint_name: Optional[str] = \"ecfp4-1024\",\n) -&gt; float:\n    \"\"\"Calculate diversity-aware top-k metric for molecular generation.\n\n    This function computes a diversity-aware top-k metric that selects up to k molecules\n    with the highest scores, subject to the constraint that selected molecules must\n    have chemical similarity below a threshold (i.e., dissimilarity above 1-t).\n    This prevents selecting multiple similar molecules and encourages chemical diversity.\n\n    Args:\n        mols: List of molecules in one of three formats:\n            - List of SMILES strings (str)\n            - List of RDKit Mol objects (Chem.Mol)\n            - 2D NumPy array representing a similarity matrix\n        scores: Sequence of scores corresponding to each molecule (e.g., docking scores).\n            Must have the same length as mols (unless mols is a similarity matrix).\n        k: Maximum number of molecules to select.\n        t: Similarity threshold (range 0.0 to 1.0). Selected molecules must have\n            Tanimoto similarity &lt; t to be considered diverse enough.\n            Lower values enforce higher diversity.\n        fingerprint_name: Name of the molecular fingerprint to use for similarity calculation.\n            Only used when mols are SMILES or Mol objects. Default is \"ecfp4-1024\".\n            See mol_gen_docking.evaluation.fingeprints_utils.fp_name_to_fn for options.\n\n    Returns:\n        Average score of the selected k diverse molecules. If fewer than k molecules\n        are selected due to diversity constraints, unselected slots are padded with 0.0.\n\n    Raises:\n        AssertionError: If mols and scores have different lengths, or if input types\n            are inconsistent.\n\n    Example:\n        ```python\n        from mol_gen_docking.evaluation.diversity_aware_top_k import diversity_aware_top_k\n        from rdkit import Chem\n\n        # Using SMILES strings\n        smiles = [\n            \"c1ccccc1\",                              # benzene\n            \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",           # ibuprofen\n            \"c1ccc2ccccc2c1\",                       # naphthalene (similar to benzene)\n            \"CCO\"                                    # ethanol\n        ]\n        scores = [8.5, 9.2, 8.0, 6.5]\n\n        # Select top 2 molecules with similarity threshold 0.8\n        metric = diversity_aware_top_k(\n            smiles, scores, k=2, t=0.8, fingerprint_name=\"ecfp4-1024\"\n        )\n        print(f\"Diversity-aware top-2 score: {metric}\")\n\n        # Using a pre-computed similarity matrix\n        sim_matrix = np.array([\n            [1.0, 0.3, 0.9, 0.2],\n            [0.3, 1.0, 0.4, 0.6],\n            [0.9, 0.4, 1.0, 0.3],\n            [0.2, 0.6, 0.3, 1.0]\n        ])\n        metric = diversity_aware_top_k(\n            sim_matrix, scores, k=2, t=0.8\n        )\n        ```\n\n    Notes:\n        - The function converts similarity matrices to distance matrices (1 - similarity)\n        - Higher t values (closer to 1.0) allow selection of more similar molecules\n        - Lower t values enforce stricter diversity constraints\n        - If a similarity matrix is provided directly, it should be a 2D NumPy array\n          with diagonal elements equal to 1.0\n        - Padding with 0.0 for unselected slots means diversity constraints can\n          result in lower average scores than unconstrained top-k\n\n    References:\n        This metric is commonly used in molecular generation benchmarks to evaluate\n        both quality and diversity of generated molecules (e.g., De Novo Generation task).\n    \"\"\"\n    dist_mat: np.ndarray[float]\n    assert len(mols) == len(scores), \"Mols and scores must have the same length.\"\n\n    if isinstance(mols[0], str) or isinstance(mols[0], Chem.Mol):\n        assert fingerprint_name is not None, (\n            \"Fingerprint name must be provided when mols are SMILES or Mol objects.\"\n        )\n        mols_list: List[Chem.Mol]\n        if isinstance(mols[0], str):\n            assert all(isinstance(smi, str) for smi in mols), (\n                \"All elements must be SMILES strings since the first is a string.\"\n            )\n            mols_list = [Chem.MolFromSmiles(smi) for smi in mols]\n        else:\n            assert all(isinstance(mol, Chem.Mol) for mol in mols), (\n                \"All elements must be RDKit Mol objects since the first is a Mol.\"\n            )\n            mols_list = mols\n        dist_mat = 1 - get_sim_matrix(mols_list, fingerprint_name=fingerprint_name)\n    else:\n        assert isinstance(mols, np.ndarray), \"Unknown type for mols.\"\n        assert mols.ndim == 2, (\n            \"Using distance matrix directly requires a 2D numpy array.\"\n        )\n        assert (mols.diagonal() == 1.0).all(), (\n            \"Similarity matrix diagonal must be all zeros.\"\n        )\n        dist_mat = squareform(1 - mols)\n\n    idxs = div_aware_top_k_from_dist(dist_mat, np.array(scores), k, 1 - t)\n\n    scores_arr = np.array(\n        [scores[idx] for idx in idxs] + [0.0 for _ in range(len(idxs), k)]\n    )\n    out_val: float = np.mean(scores_arr)\n    return out_val\n</code></pre>"},{"location":"api/evaluation/diversity_aware_top_k/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Confusing similarity and distance: Remember t is similarity threshold, distance = 1 - similarity</li> <li>Invalid SMILES: Always validate SMILES strings before passing to function</li> <li>Threshold interpretation: Lower t = stricter diversity, not lenient</li> </ol>"},{"location":"api/evaluation/fingeprints_utils/","title":"Fingerprint Utilities","text":""},{"location":"api/evaluation/fingeprints_utils/#supported-fingerprints","title":"Supported Fingerprints","text":"<p>The module supports several standard fingerprint types used in cheminformatics:</p> Fingerprint Type Description Bits ECFP Structural Extended Connectivity Fingerprint (Morgan) Configurable MACCS Structural MACCS Keys fingerprint 166 RDKit Structural RDKit topological fingerprint 2048 Gobbi2D Pharmacophore Gobbi pharmacophore 2D fingerprint Variable Avalon Structural Avalon fingerprint Variable"},{"location":"api/evaluation/fingeprints_utils/#ecfp-parameters","title":"ECFP Parameters","text":"<p>ECFP (Extended Connectivity Fingerprint, also known as Morgan fingerprint) fingerprints are specified as:</p> <ul> <li>Format: <code>\"ecfp{diameter}-{nbits}\"</code></li> <li>Examples: <code>\"ecfp2-1024\"</code>, <code>\"ecfp4-1024\"</code>, <code>\"ecfp6-2048\"</code></li> <li>Diameter: Topological diameter (internally converted to radius = diameter/2)</li> <li>Bits: Number of bits in the fingerprint (256, 512, 1024, 2048, etc.)</li> </ul>"},{"location":"api/evaluation/fingeprints_utils/#tanimoto-similarity","title":"Tanimoto Similarity","text":"<p>The module uses Tanimoto (Jaccard) similarity to measure molecular similarity: - Range: 0.0 (completely dissimilar) to 1.0 (identical) - Formula: |A \u2229 B| / |A \u222a B| - Suitable for bit vectors and sparse representations</p>"},{"location":"api/evaluation/fingeprints_utils/#function-reference","title":"Function Reference","text":"<p>Molecular fingerprint utilities for similarity calculations.</p> <p>This module provides utilities for computing molecular fingerprints and similarity matrices. It supports multiple fingerprint types (ECFP, MACCS, RDKit, Gobbi2D, Avalon) and computes Tanimoto similarity between molecules for use in diversity-aware metrics.</p>"},{"location":"api/evaluation/fingeprints_utils/#mol_gen_docking.evaluation.fingeprints_utils.fp_name_to_fn","title":"<code>fp_name_to_fn(fp_name)</code>","text":"<p>Convert a fingerprint name to a function that generates that fingerprint.</p> <p>This function returns a callable that takes an RDKit Mol object and returns the corresponding fingerprint. It supports several standard fingerprint types used in cheminformatics.</p> <p>Parameters:</p> Name Type Description Default <code>fp_name</code> <code>str</code> <p>Name of the fingerprint to use. Supported options: - \"ecfp{diameter}-{nbits}\": Extended Connectivity Fingerprint (Morgan).   Examples: \"ecfp2-1024\", \"ecfp4-1024\", \"ecfp6-2048\". - \"maccs\": MACCS Keys fingerprint (166 bit). - \"rdkit\": RDKit fingerprint. - \"Gobbi2d\": Gobbi pharmacophore 2D fingerprint. - \"Avalon\": Avalon fingerprint.</p> required <p>Returns:</p> Type Description <code>Callable[[Mol], ExplicitBitVect]</code> <p>A callable function that takes an RDKit Mol object and returns</p> <code>Callable[[Mol], ExplicitBitVect]</code> <p>an ExplicitBitVect fingerprint.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the fingerprint name is not recognized or has invalid format.</p> <code>AssertionError</code> <p>If the ECFP parameters don't match expected format.</p> Example <pre><code>from mol_gen_docking.evaluation.fingeprints_utils import fp_name_to_fn\nfrom rdkit import Chem\n\n# Get the fingerprint function\nfp_fn = fp_name_to_fn(\"ecfp4-1024\")\n\n# Generate fingerprints for molecules\nmol = Chem.MolFromSmiles(\"c1ccccc1\")\nfp = fp_fn(mol)\n</code></pre> Notes <ul> <li>ECFP diameter is specified as topological diameter (divided by 2 internally   to get the radius for Morgan fingerprints)</li> <li>All returned fingerprints are explicit bit vectors for similarity calculations</li> </ul> Source code in <code>mol_gen_docking/evaluation/fingeprints_utils.py</code> <pre><code>def fp_name_to_fn(\n    fp_name: str,\n) -&gt; Callable[[Chem.Mol], DataStructs.cDataStructs.ExplicitBitVect]:\n    \"\"\"Convert a fingerprint name to a function that generates that fingerprint.\n\n    This function returns a callable that takes an RDKit Mol object and returns\n    the corresponding fingerprint. It supports several standard fingerprint types\n    used in cheminformatics.\n\n    Args:\n        fp_name: Name of the fingerprint to use. Supported options:\n            - \"ecfp{diameter}-{nbits}\": Extended Connectivity Fingerprint (Morgan).\n              Examples: \"ecfp2-1024\", \"ecfp4-1024\", \"ecfp6-2048\".\n            - \"maccs\": MACCS Keys fingerprint (166 bit).\n            - \"rdkit\": RDKit fingerprint.\n            - \"Gobbi2d\": Gobbi pharmacophore 2D fingerprint.\n            - \"Avalon\": Avalon fingerprint.\n\n    Returns:\n        A callable function that takes an RDKit Mol object and returns\n        an ExplicitBitVect fingerprint.\n\n    Raises:\n        ValueError: If the fingerprint name is not recognized or has invalid format.\n        AssertionError: If the ECFP parameters don't match expected format.\n\n    Example:\n        ```python\n        from mol_gen_docking.evaluation.fingeprints_utils import fp_name_to_fn\n        from rdkit import Chem\n\n        # Get the fingerprint function\n        fp_fn = fp_name_to_fn(\"ecfp4-1024\")\n\n        # Generate fingerprints for molecules\n        mol = Chem.MolFromSmiles(\"c1ccccc1\")\n        fp = fp_fn(mol)\n        ```\n\n    Notes:\n        - ECFP diameter is specified as topological diameter (divided by 2 internally\n          to get the radius for Morgan fingerprints)\n        - All returned fingerprints are explicit bit vectors for similarity calculations\n    \"\"\"\n\n    if fp_name.startswith(\"ecfp\"):\n        d = int(fp_name[4])\n        n_bits = int(fp_name.split(\"-\")[1])\n        assert fp_name == f\"ecfp{d}-{n_bits}\", f\"Invalid fingerprint name: {fp_name}\"\n\n        def fp_fn(mol: Chem.Mol) -&gt; DataStructs.cDataStructs.ExplicitBitVect:\n            return AllChem.GetMorganFingerprintAsBitVect(mol, d // 2, n_bits)\n\n        return fp_fn\n    elif fp_name == \"maccs\":\n\n        def fp_fn(mol: Chem.Mol) -&gt; DataStructs.cDataStructs.ExplicitBitVect:\n            return Chem.rdMolDescriptors.GetMACCSKeysFingerprint(mol)\n\n        return fp_fn\n    elif fp_name == \"rdkit\":\n\n        def fp_fn(mol: Chem.Mol) -&gt; DataStructs.cDataStructs.ExplicitBitVect:\n            return Chem.RDKFingerprint(mol)\n\n        return fp_fn\n    elif fp_name == \"Gobbi2d\":\n        from rdkit.Chem.Pharm2D import Generate, Gobbi_Pharm2D\n\n        def fp_fn(mol: Chem.Mol) -&gt; DataStructs.cDataStructs.ExplicitBitVect:\n            return Generate.Gen2DFingerprint(mol, Gobbi_Pharm2D.factory)\n\n        return fp_fn\n    elif fp_name == \"Avalon\":\n        from rdkit.Avalon import pyAvalonTools\n\n        def fp_fn(mol: Chem.Mol) -&gt; DataStructs.cDataStructs.ExplicitBitVect:\n            return pyAvalonTools.GetAvalonFP(mol)\n\n        return fp_fn\n    else:\n        raise ValueError(f\"Unknown fingerprint name: {fp_name}\")\n</code></pre>"},{"location":"api/evaluation/fingeprints_utils/#mol_gen_docking.evaluation.fingeprints_utils.get_sim_matrix","title":"<code>get_sim_matrix(mols, fingerprint_name='ecfp4-1024')</code>","text":"<p>Compute a pairwise similarity matrix for a list of molecules.</p> <p>This function computes Tanimoto similarities between all pairs of molecules using the specified fingerprint method. The result is a condensed distance matrix (upper triangle only) in NumPy array format.</p> <p>Parameters:</p> Name Type Description Default <code>mols</code> <code>list[Mol]</code> <p>List of RDKit Mol objects.</p> required <code>fingerprint_name</code> <code>str</code> <p>Name of the fingerprint to use for similarity calculation. Default is \"ecfp4-1024\" (ECFP with diameter 4 and 1024 bits). See fp_name_to_fn for supported options.</p> <code>'ecfp4-1024'</code> <p>Returns:</p> Type Description <code>ndarray[float]</code> <p>A 1D NumPy array containing the upper triangle of the pairwise similarity</p> <code>ndarray[float]</code> <p>matrix. The array is in condensed form (as used by scipy.spatial.distance.squareform).</p> <code>ndarray[float]</code> <p>For n molecules, the length is n*(n-1)/2.</p> Example <pre><code>from mol_gen_docking.evaluation.fingeprints_utils import get_sim_matrix\nfrom rdkit import Chem\n\nsmiles = [\"c1ccccc1\", \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\", \"CCO\"]\nmols = [Chem.MolFromSmiles(smi) for smi in smiles]\n\nsim_matrix = get_sim_matrix(mols, fingerprint_name=\"ecfp4-1024\")\nprint(f\"Shape: {sim_matrix.shape}\")  # (3,) for 3 molecules\n</code></pre> Notes <ul> <li>The returned array is in condensed form. Use scipy.spatial.distance.squareform   to convert to a full square similarity matrix.</li> <li>Similarity values range from 0.0 (completely dissimilar) to 1.0 (identical)</li> <li>The matrix is symmetric, so only the upper triangle is computed</li> </ul> Source code in <code>mol_gen_docking/evaluation/fingeprints_utils.py</code> <pre><code>def get_sim_matrix(\n    mols: list[Chem.Mol],\n    fingerprint_name: str = \"ecfp4-1024\",\n) -&gt; np.ndarray[float]:\n    \"\"\"Compute a pairwise similarity matrix for a list of molecules.\n\n    This function computes Tanimoto similarities between all pairs of molecules\n    using the specified fingerprint method. The result is a condensed distance\n    matrix (upper triangle only) in NumPy array format.\n\n    Args:\n        mols: List of RDKit Mol objects.\n        fingerprint_name: Name of the fingerprint to use for similarity calculation.\n            Default is \"ecfp4-1024\" (ECFP with diameter 4 and 1024 bits).\n            See fp_name_to_fn for supported options.\n\n    Returns:\n        A 1D NumPy array containing the upper triangle of the pairwise similarity\n        matrix. The array is in condensed form (as used by scipy.spatial.distance.squareform).\n        For n molecules, the length is n*(n-1)/2.\n\n    Example:\n        ```python\n        from mol_gen_docking.evaluation.fingeprints_utils import get_sim_matrix\n        from rdkit import Chem\n\n        smiles = [\"c1ccccc1\", \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\", \"CCO\"]\n        mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n\n        sim_matrix = get_sim_matrix(mols, fingerprint_name=\"ecfp4-1024\")\n        print(f\"Shape: {sim_matrix.shape}\")  # (3,) for 3 molecules\n        ```\n\n    Notes:\n        - The returned array is in condensed form. Use scipy.spatial.distance.squareform\n          to convert to a full square similarity matrix.\n        - Similarity values range from 0.0 (completely dissimilar) to 1.0 (identical)\n        - The matrix is symmetric, so only the upper triangle is computed\n    \"\"\"\n    fp_fn = fp_name_to_fn(fingerprint_name)\n    fps = [fp_fn(mol) for mol in mols]\n    sim_mat = [\n        np.array(DataStructs.BulkTanimotoSimilarity(fp, fps[i + 1 :]))\n        for i, fp in enumerate(fps[:-1])\n    ]\n    matrix: np.ndarray[float] = np.concatenate(sim_mat)\n    return matrix\n</code></pre>"},{"location":"api/evaluation/top_k/","title":"Top-k Metric","text":""},{"location":"api/evaluation/top_k/#overview","title":"Overview","text":"<p>The top-k metric is a standard evaluation metric for molecular generation tasks. It measures the average quality of the top k unique molecules from a set of generated candidates, ensuring that duplicate molecules are not counted multiple times.</p> <p>Note</p>"},{"location":"api/evaluation/top_k/#uniqueness-constraint","title":"Uniqueness Constraint","text":"<p>The top-k metric enforces uniqueness by:</p> <ul> <li>Converting all molecules to canonical SMILES representation</li> <li>Removing duplicate molecules</li> <li>Selecting the k molecules with the highest scores</li> </ul>"},{"location":"api/evaluation/top_k/#padding-mechanism","title":"Padding Mechanism","text":"<p>If fewer than k unique molecules are available (i.e the model cannot generate as many candidates), the remaining slots are padded with 0.0 scores.</p>"},{"location":"api/evaluation/top_k/#usage-examples","title":"Usage Examples","text":""},{"location":"api/evaluation/top_k/#basic-usage-with-smiles","title":"Basic Usage with SMILES","text":"<pre><code>from mol_gen_docking.evaluation.top_k import top_k\n\n# List of generated molecules as SMILES\nsmiles = [\n    \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",\n    \"c1ccccc1\",\n    \"CCO\",\n    \"CC(C)Cc1ccc(cc1)C(C)C(=O)O\" # Ibuprofen duplicate but different smiles\n]\n\n# Docking scores for each molecule\nscores = [8.5, 6.2, 6.1, 8.5]\n\n# Calculate top-2 score\nmetric = top_k(smiles, scores, k=2)\nprint(f\"Top-2 score: {metric}\")\n# Output:\n# &gt;&gt;&gt; 7.35\n</code></pre>"},{"location":"api/evaluation/top_k/#using-rdkit-mol-objects","title":"Using RDKit Mol Objects","text":"<pre><code>from mol_gen_docking.evaluation.top_k import top_k\nfrom rdkit import Chem\n\n# Convert to Mol objects\nmols = [Chem.MolFromSmiles(smi) for smi in smiles]\n\n# top_k automatically canonicalizes Mol objects\nmetric = top_k(mols, scores, k=2)\nprint(metric)\n\n# Output:\n# &gt;&gt;&gt; 7.35\n</code></pre>"},{"location":"api/evaluation/top_k/#without-canonicalization","title":"Without Canonicalization","text":"<pre><code># If SMILES strings are already canonical\nmetric = top_k(smiles, scores, k=2, canonicalize=False)\nprint(metric)\n\n# Output:\n# &gt;&gt;&gt; 8.5 # Since both ibuprofen entries are considered unique without canonicalization\n</code></pre>"},{"location":"api/evaluation/top_k/#function-reference","title":"Function Reference","text":"<p>Top-k evaluation metric for molecular generation tasks.</p> <p>This module implements the standard top-k metric for evaluating molecular generation models. The metric measures the average quality of the top k unique molecules from a set of generated candidates.</p>"},{"location":"api/evaluation/top_k/#mol_gen_docking.evaluation.top_k.top_k","title":"<code>top_k(mols, scores, k, canonicalize=True)</code>","text":"<p>Calculate the top-k metric for molecular generation.</p> <p>This function computes the average score of the top k unique molecules from a set of candidates. It first deduplicates molecules using canonical SMILES representation, then selects the k molecules with the highest scores. This metric is useful for evaluating the quality of generated molecules.</p> <p>Parameters:</p> Name Type Description Default <code>mols</code> <code>List[str] | List[Mol]</code> <p>List of molecules as SMILES strings or RDKit Mol objects.</p> required <code>scores</code> <code>List[float]</code> <p>List of scores corresponding to each molecule (e.g., docking scores, binding affinity). Must have the same length as mols.</p> required <code>k</code> <code>int</code> <p>Number of top molecules to consider. If fewer than k unique molecules are provided, the remaining slots are filled with 0.0 scores.</p> required <code>canonicalize</code> <code>bool</code> <p>Whether to canonicalize SMILES strings before comparison. If True, molecules are converted to canonical SMILES for deduplication. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>Average score of the top k unique molecules. If fewer than k unique molecules</p> <code>float</code> <p>are available, the average includes padding with 0.0 scores.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If mols and scores have different lengths.</p> Example <pre><code>from mol_gen_docking.evaluation.top_k import top_k\nfrom rdkit import Chem\n\n# Using SMILES strings\nsmiles = [\"CC(C)Cc1ccc(cc1)C(C)C(O)=O\", \"c1ccccc1\"]\nscores = [8.5, 7.2]\nmetric = top_k(smiles, scores, k=2)\nprint(f\"Top-2 score: {metric}\")\n\n# Using RDKit Mol objects\nmols = [Chem.MolFromSmiles(smi) for smi in smiles]\nmetric = top_k(mols, scores, k=2)\n</code></pre> Notes <ul> <li>Duplicate molecules are automatically detected and removed using canonical SMILES</li> <li>If k is larger than the number of unique molecules, remaining slots are filled with 0.0</li> <li>The final metric is the average of the k highest scores</li> </ul> Source code in <code>mol_gen_docking/evaluation/top_k.py</code> <pre><code>def top_k(\n    mols: List[str] | List[Chem.Mol],\n    scores: List[float],\n    k: int,\n    canonicalize: bool = True,\n) -&gt; float:\n    \"\"\"Calculate the top-k metric for molecular generation.\n\n    This function computes the average score of the top k unique molecules from a set\n    of candidates. It first deduplicates molecules using canonical SMILES representation,\n    then selects the k molecules with the highest scores. This metric is useful for\n    evaluating the quality of generated molecules.\n\n    Args:\n        mols: List of molecules as SMILES strings or RDKit Mol objects.\n        scores: List of scores corresponding to each molecule (e.g., docking scores,\n            binding affinity). Must have the same length as mols.\n        k: Number of top molecules to consider. If fewer than k unique molecules are\n            provided, the remaining slots are filled with 0.0 scores.\n        canonicalize: Whether to canonicalize SMILES strings before comparison.\n            If True, molecules are converted to canonical SMILES for deduplication.\n            Default is True.\n\n    Returns:\n        Average score of the top k unique molecules. If fewer than k unique molecules\n        are available, the average includes padding with 0.0 scores.\n\n    Raises:\n        AssertionError: If mols and scores have different lengths.\n\n    Example:\n        ```python\n        from mol_gen_docking.evaluation.top_k import top_k\n        from rdkit import Chem\n\n        # Using SMILES strings\n        smiles = [\"CC(C)Cc1ccc(cc1)C(C)C(O)=O\", \"c1ccccc1\"]\n        scores = [8.5, 7.2]\n        metric = top_k(smiles, scores, k=2)\n        print(f\"Top-2 score: {metric}\")\n\n        # Using RDKit Mol objects\n        mols = [Chem.MolFromSmiles(smi) for smi in smiles]\n        metric = top_k(mols, scores, k=2)\n        ```\n\n    Notes:\n        - Duplicate molecules are automatically detected and removed using canonical SMILES\n        - If k is larger than the number of unique molecules, remaining slots are filled with 0.0\n        - The final metric is the average of the k highest scores\n    \"\"\"\n    smi_list: List[str]\n    if canonicalize or isinstance(mols[0], Chem.Mol):\n        if isinstance(mols[0], str):\n            mols_list = [Chem.MolFromSmiles(smi) for smi in mols]\n        else:\n            mols_list = mols\n        smi_list = [Chem.MolToSmiles(mol, canonical=True) for mol in mols_list]\n    else:\n        smi_list = mols\n\n    # Drop ducplicates and keep idxs\n    seen = set()\n    unique_idxs = []\n    for idx, smi in enumerate(smi_list):\n        if smi not in seen:\n            seen.add(smi)\n            unique_idxs.append(idx)\n    unique_scores = [scores[idx] for idx in unique_idxs] + [\n        0.0 for _ in range(len(unique_idxs), k)\n    ]\n    unique_scores = sorted(unique_scores, reverse=True)[:k]\n    return sum(unique_scores) / k\n</code></pre>"},{"location":"api/evaluation/top_k/#benchmark-context","title":"Benchmark Context","text":"<p>In the MolGenDocking project, the top-k metric is used to evaluate molecular generation models on raw-quality without diversity constraints. We assess how well a model can generate multiple high-scoring molecules, possibly from a same chemical serie.</p> <p>See Diversity-Aware Top-k for a variant that also enforces chemical diversity.</p>"},{"location":"api/reward/generation_verifier/","title":"Generation Verifier","text":"<p>The <code>GenerationVerifier</code> computes rewards for de novo molecular generation tasks, evaluating generated molecules against property optimization criteria such as docking scores, QED, synthetic accessibility, and other molecular descriptors.</p>"},{"location":"api/reward/generation_verifier/#overview","title":"Overview","text":"<p>The Generation Verifier supports:</p> <ul> <li>Multi-property Optimization: Optimize multiple properties simultaneously</li> <li>Docking Score Computation: GPU-accelerated molecular docking with AutoDock</li> <li>RDKit Descriptors: QED, SA score, LogP, molecular weight, etc.</li> <li>SMILES Extraction: Robust parsing of SMILES from model completions</li> </ul>"},{"location":"api/reward/generation_verifier/#supported-properties","title":"Supported Properties","text":"Property Type Description Docking Targets Slow (GPU) Binding affinity to protein pockets Physico-Chemical Properties Fast QED, SA score, Molecular Weight, ..."},{"location":"api/reward/generation_verifier/#smiles-extraction","title":"SMILES Extraction","text":"<p>The verifier extracts SMILES from completions using:</p> <ol> <li>Answer Tags: Content between <code>&lt;answer&gt;</code> and <code>&lt;/answer&gt;</code> tags</li> <li>Pattern Matching: Identifies possible valid SMILES patterns (extracted word with no characters outside SMILES charset, and that contains at least one <code>C</code> character, or multiple <code>c</code>)</li> <li>Validation: Verifies molecules with RDKit</li> </ol>"},{"location":"api/reward/generation_verifier/#extraction-failures","title":"Extraction Failures","text":"Failure Reason Description <code>no_answer</code> No answer tags found <code>no_smiles</code> No SMILES-like strings in answer <code>no_valid_smiles</code> SMILES strings are invalid <code>multiple_smiles</code> Multiple valid SMILES found (ambiguous) <p>Generation verifier for de novo molecular generation tasks.</p> <p>This module provides the GenerationVerifier class which computes rewards for molecular generation based on property optimization objectives such as docking scores, QED, synthetic accessibility, and other molecular descriptors.</p>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.DockingConfigModel","title":"<code>DockingConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for docking configuration.</p> <p>This model defines the configuration parameters for docking operations, providing validation and documentation for all docking options.</p> <p>Attributes:</p> Name Type Description <code>exhaustiveness</code> <code>int</code> <p>Docking exhaustiveness parameter.</p> <code>n_cpu</code> <code>int</code> <p>Number of CPUs to use for docking.</p> <code>docking_oracle</code> <code>Literal['pyscreener', 'autodock_gpu']</code> <p>Type of docking oracle to use (\"pyscreener\" or \"autodock_gpu\").</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class DockingConfigModel(BaseModel):\n    \"\"\"Pydantic model for docking configuration.\n\n    This model defines the configuration parameters for docking operations,\n    providing validation and documentation for all docking options.\n\n    Attributes:\n        exhaustiveness: Docking exhaustiveness parameter.\n        n_cpu: Number of CPUs to use for docking.\n        docking_oracle: Type of docking oracle to use (\"pyscreener\" or \"autodock_gpu\").\n    \"\"\"\n\n    exhaustiveness: int = Field(\n        default=8,\n        gt=1,\n        description=\"Docking exhaustiveness parameter\",\n    )\n\n    n_cpu: int = Field(\n        default=8,\n        gt=1,\n        description=\"Number of CPUs to use for docking\",\n    )\n\n    docking_oracle: Literal[\"pyscreener\", \"autodock_gpu\"] = Field(\n        default=\"autodock_gpu\",\n        description='Type of docking oracle: \"pyscreener\" or \"autodock_gpu\"',\n    )\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.DockingGPUConfigModel","title":"<code>DockingGPUConfigModel</code>","text":"<p>               Bases: <code>DockingConfigModel</code></p> <p>Pydantic model for AutoDock GPU docking configuration.</p> <p>This model defines the configuration parameters specific to the AutoDock GPU docking software, providing validation and documentation for all options.</p> <p>Attributes:</p> Name Type Description <code>exhaustiveness</code> <code>int</code> <p>Docking exhaustiveness parameter.</p> <code>n_cpu</code> <code>int</code> <p>Number of CPUs to use for docking.</p> <code>docking_oracle</code> <code>Literal['pyscreener', 'autodock_gpu']</code> <p>Type of docking oracle to use (must be \"autodock_gpu\").</p> <code>vina_mode</code> <code>str</code> <p>Command mode for AutoDock GPU.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class DockingGPUConfigModel(DockingConfigModel):\n    \"\"\"Pydantic model for AutoDock GPU docking configuration.\n\n    This model defines the configuration parameters specific to the AutoDock GPU\n    docking software, providing validation and documentation for all options.\n\n    Attributes:\n        exhaustiveness: Docking exhaustiveness parameter.\n        n_cpu: Number of CPUs to use for docking.\n        docking_oracle: Type of docking oracle to use (must be \"autodock_gpu\").\n        vina_mode: Command mode for AutoDock GPU.\n    \"\"\"\n\n    vina_mode: str = Field(\n        default=\"autodock_gpu_256wi\",\n        description=\"Command mode for AutoDock GPU\",\n    )\n\n    @model_validator(mode=\"after\")\n    def check_vina_mode(self) -&gt; \"DockingGPUConfigModel\":\n        assert self.docking_oracle == \"autodock_gpu\", (\n            \"vina_mode is only valid for autodock_gpu docking_oracle\"\n        )\n        return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.PyscreenerConfigModel","title":"<code>PyscreenerConfigModel</code>","text":"<p>               Bases: <code>DockingConfigModel</code></p> <p>Pydantic model for PyScreener docking configuration.</p> <p>This model defines the configuration parameters specific to the PyScreener docking software, providing validation and documentation for all options.</p> <p>Attributes:</p> Name Type Description <code>exhaustiveness</code> <code>int</code> <p>Docking exhaustiveness parameter.</p> <code>n_cpu</code> <code>int</code> <p>Number of CPUs to use for docking.</p> <code>docking_oracle</code> <code>Literal['pyscreener', 'autodock_gpu']</code> <p>Type of docking oracle to use (must be \"pyscreener\").</p> <code>software_class</code> <code>Literal['vina', 'qvina', 'smina', 'psovina', 'dock', 'dock6', 'ucsfdock']</code> <p>Docking software class to use with PyScreener.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class PyscreenerConfigModel(DockingConfigModel):\n    \"\"\"Pydantic model for PyScreener docking configuration.\n\n    This model defines the configuration parameters specific to the PyScreener\n    docking software, providing validation and documentation for all options.\n\n    Attributes:\n        exhaustiveness: Docking exhaustiveness parameter.\n        n_cpu: Number of CPUs to use for docking.\n        docking_oracle: Type of docking oracle to use (must be \"pyscreener\").\n        software_class: Docking software class to use with PyScreener.\n    \"\"\"\n\n    software_class: Literal[\n        \"vina\",\n        \"qvina\",\n        \"smina\",\n        \"psovina\",\n        \"dock\",\n        \"dock6\",\n        \"ucsfdock\",\n    ] = Field(\n        default=\"vina\",\n        description=\"Docking software class to use with PyScreener\",\n    )\n\n    @model_validator(mode=\"after\")\n    def check_software_class(self) -&gt; \"PyscreenerConfigModel\":\n        assert self.docking_oracle == \"pyscreener\", (\n            \"software_class is only valid for pyscreener docking_oracle\"\n        )\n        return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.input_metadata.GenerationVerifierInputMetadataModel","title":"<code>GenerationVerifierInputMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input metadata model for generation verifier.</p> <p>Defines the verification criteria for molecular generation tasks, including properties to optimize, objectives for each property, and target values.</p> <p>Attributes:</p> Name Type Description <code>properties</code> <code>List[str]</code> <p>List of property names to verify (e.g., \"QED\", \"SA\", \"docking_target_name\"). Each property should be a valid molecular descriptor or a docking target name. Must have the same length as objectives and target.</p> <code>objectives</code> <code>List[GenerationObjT]</code> <p>List of objectives for each property. Must have the same length as properties and target. Valid values: - \"maximize\": Reward increases with property value - \"minimize\": Reward increases as property value decreases - \"above\": Reward is 1.0 if property &gt;= target, 0.0 otherwise - \"below\": Reward is 1.0 if property &lt;= target, 0.0 otherwise</p> <code>target</code> <code>List[float]</code> <p>List of target values for each property. Must have the same length as properties and objectives. For \"maximize\"/\"minimize\": Used as reference point for rescaling (when enabled) For \"above\"/\"below\": Used as threshold for binary reward computation</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/input_metadata.py</code> <pre><code>class GenerationVerifierInputMetadataModel(BaseModel):\n    \"\"\"Input metadata model for generation verifier.\n\n    Defines the verification criteria for molecular generation tasks, including\n    properties to optimize, objectives for each property, and target values.\n\n    Attributes:\n        properties: List of property names to verify (e.g., \"QED\", \"SA\", \"docking_target_name\").\n            Each property should be a valid molecular descriptor or a docking target name.\n            Must have the same length as objectives and target.\n\n        objectives: List of objectives for each property.\n            Must have the same length as properties and target.\n            Valid values:\n            - \"maximize\": Reward increases with property value\n            - \"minimize\": Reward increases as property value decreases\n            - \"above\": Reward is 1.0 if property &gt;= target, 0.0 otherwise\n            - \"below\": Reward is 1.0 if property &lt;= target, 0.0 otherwise\n\n        target: List of target values for each property.\n            Must have the same length as properties and objectives.\n            For \"maximize\"/\"minimize\": Used as reference point for rescaling (when enabled)\n            For \"above\"/\"below\": Used as threshold for binary reward computation\n    \"\"\"\n\n    properties: List[str] = Field(\n        ...,\n        description=\"List of property names to verify.\",\n    )\n    objectives: List[GenerationObjT] = Field(\n        ...,\n        description=\"List of objectives for each property: maximize, minimize, above, or below.\",\n    )\n    target: List[float] = Field(\n        ...,\n        description=\"List of target values for each property.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def validate_properties(self) -&gt; \"GenerationVerifierInputMetadataModel\":\n        \"\"\"Validate that properties, objectives, and target have the same length.\"\"\"\n        if not (len(self.properties) == len(self.objectives) == len(self.target)):\n            raise ValueError(\n                \"Length of properties, objectives, and target must be the same.\"\n            )\n        return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.input_metadata.GenerationVerifierInputMetadataModel.validate_properties","title":"<code>validate_properties()</code>","text":"<p>Validate that properties, objectives, and target have the same length.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/input_metadata.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_properties(self) -&gt; \"GenerationVerifierInputMetadataModel\":\n    \"\"\"Validate that properties, objectives, and target have the same length.\"\"\"\n    if not (len(self.properties) == len(self.objectives) == len(self.target)):\n        raise ValueError(\n            \"Length of properties, objectives, and target must be the same.\"\n        )\n    return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.GenerationVerifierOutputModel","title":"<code>GenerationVerifierOutputModel</code>","text":"<p>               Bases: <code>VerifierOutputModel</code></p> <p>Output model for generation verifier results.</p> <p>Attributes:</p> Name Type Description <code>reward</code> <code>float</code> <p>The computed reward for the generation verification.</p> <code>parsed_answer</code> <code>str</code> <p>The parsed answer extracted from the model completion.</p> <code>verifier_metadata</code> <code>GenerationVerifierMetadataModel</code> <p>Metadata related to the generation verification process.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class GenerationVerifierOutputModel(VerifierOutputModel):\n    \"\"\"Output model for generation verifier results.\n\n    Attributes:\n        reward: The computed reward for the generation verification.\n        parsed_answer: The parsed answer extracted from the model completion.\n        verifier_metadata: Metadata related to the generation verification process.\n    \"\"\"\n\n    reward: float = Field(\n        ...,\n        description=\"The computed reward for the generation verification.\",\n    )\n    parsed_answer: str = Field(\n        ..., description=\"The parsed answer extracted from the model completion.\"\n    )\n    verifier_metadata: GenerationVerifierMetadataModel = Field(\n        ...,\n        description=\"Metadata related to the generation verification process.\",\n    )\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.GenerationVerifierMetadataModel","title":"<code>GenerationVerifierMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata model for generation verifier results.</p> <p>Contains detailed information about the generation verification process, including all extracted SMILES, their individual rewards, and any extraction failures.</p> <p>Attributes:</p> Name Type Description <code>properties</code> <code>List[str]</code> <p>List of property names that were evaluated (e.g., \"docking_score\", \"QED\", \"SA\"). Each property corresponds to a molecular descriptor or docking target that was optimized.</p> <code>individual_rewards</code> <code>List[float]</code> <p>List of individual rewards for each property in the properties list. Each value is typically in [0.0, 1.0] range when rescaling is enabled, representing how well the molecule satisfies each property objective.</p> <code>all_smi_rewards</code> <code>List[float]</code> <p>List of rewards for all SMILES found in the completion. When multiple SMILES are extracted, each gets its own reward. The final reward is typically the best among these values.</p> <code>all_smi</code> <code>List[str]</code> <p>List of all SMILES strings extracted from the completion. May contain multiple SMILES if the model generated several molecules. Empty if SMILES extraction failed.</p> <code>smiles_extraction_failure</code> <code>str</code> <p>Error message if SMILES extraction failed. Empty string if extraction was successful. Common values include:</p> <ul> <li>\"no_smiles\": No valid SMILES found in the completion</li> <li>\"multiple_smiles_in_boxed\": Multiple SMILES found when only one was expected</li> <li>\"invalid_smiles\": SMILES string found but not chemically valid</li> </ul> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class GenerationVerifierMetadataModel(BaseModel):\n    \"\"\"Metadata model for generation verifier results.\n\n    Contains detailed information about the generation verification process,\n    including all extracted SMILES, their individual rewards, and any extraction failures.\n\n    Attributes:\n        properties: List of property names that were evaluated (e.g., \"docking_score\", \"QED\", \"SA\").\n            Each property corresponds to a molecular descriptor or docking target that was optimized.\n\n        individual_rewards: List of individual rewards for each property in the properties list.\n            Each value is typically in [0.0, 1.0] range when rescaling is enabled, representing\n            how well the molecule satisfies each property objective.\n\n        all_smi_rewards: List of rewards for all SMILES found in the completion.\n            When multiple SMILES are extracted, each gets its own reward. The final reward\n            is typically the best among these values.\n\n        all_smi: List of all SMILES strings extracted from the completion.\n            May contain multiple SMILES if the model generated several molecules.\n            Empty if SMILES extraction failed.\n\n        smiles_extraction_failure: Error message if SMILES extraction failed.\n            Empty string if extraction was successful. Common values include:\n\n            - \"no_smiles\": No valid SMILES found in the completion\n            - \"multiple_smiles_in_boxed\": Multiple SMILES found when only one was expected\n            - \"invalid_smiles\": SMILES string found but not chemically valid\n    \"\"\"\n\n    properties: List[str] = Field(\n        default_factory=list,\n        description=\"List of property names that were evaluated.\",\n    )\n    individual_rewards: List[float] = Field(\n        default_factory=list,\n        description=\"List of individual rewards for each property.\",\n    )\n    all_smi_rewards: List[float] = Field(\n        default_factory=list,\n        description=\"List of rewards for all SMILES in the completion.\",\n    )\n    all_smi: List[str] = Field(\n        default_factory=list,\n        description=\"List of all SMILES strings in the completion.\",\n    )\n    smiles_extraction_failure: str = Field(\n        default=\"\",\n        description=\"Error message if there was a failure in extracting SMILES from the completion.\",\n        frozen=False,\n    )\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.GenerationVerifierConfigModel","title":"<code>GenerationVerifierConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for generation verifier configuration.</p> <p>This model defines the configuration parameters for the GenerationVerifier class, providing validation and documentation for all configuration options.</p> <p>Attributes:</p> Name Type Description <code>path_to_mappings</code> <code>str</code> <p>Optional path to property mappings and docking targets configuration directory.              Should contain 'names_mapping.json' and 'docking_targets.json' files.</p> <code>reward</code> <code>Literal['property', 'valid_smiles']</code> <p>Type of reward to compute. Either \"property\" for property-based rewards or \"valid_smiles\"     for validity-based rewards.</p> <code>rescale</code> <code>bool</code> <p>Whether to rescale the rewards to a normalized range.</p> <code>oracle_kwargs</code> <code>DockingGPUConfigModel | PyscreenerConfigModel</code> <p>Dictionary of keyword arguments to pass to the docking oracle. Can include:</p> <pre><code>       - exhaustiveness: Docking exhaustiveness parameter\n       - n_cpu: Number of CPUs for docking\n       - docking_oracle: Type of docking oracle (\"pyscreener\" or \"autodock_gpu\")\n       - vina_mode: Command mode for AutoDock GPU\n</code></pre> <code>docking_concurrency_per_gpu</code> <code>int</code> <p>Number of concurrent docking runs to allow per GPU.                          Default is 2 (uses ~1GB per run on 80GB GPU).</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class GenerationVerifierConfigModel(BaseModel):\n    \"\"\"Pydantic model for generation verifier configuration.\n\n    This model defines the configuration parameters for the GenerationVerifier class,\n    providing validation and documentation for all configuration options.\n\n    Attributes:\n        path_to_mappings: Optional path to property mappings and docking targets configuration directory.\n                         Should contain 'names_mapping.json' and 'docking_targets.json' files.\n        reward: Type of reward to compute. Either \"property\" for property-based rewards or \"valid_smiles\"\n                for validity-based rewards.\n        rescale: Whether to rescale the rewards to a normalized range.\n        oracle_kwargs: Dictionary of keyword arguments to pass to the docking oracle. Can include:\n\n                       - exhaustiveness: Docking exhaustiveness parameter\n                       - n_cpu: Number of CPUs for docking\n                       - docking_oracle: Type of docking oracle (\"pyscreener\" or \"autodock_gpu\")\n                       - vina_mode: Command mode for AutoDock GPU\n        docking_concurrency_per_gpu: Number of concurrent docking runs to allow per GPU.\n                                     Default is 2 (uses ~1GB per run on 80GB GPU).\n    \"\"\"\n\n    path_to_mappings: str = Field(\n        description=\"Path to property mappings and docking targets configuration directory (must contain names_mapping.json and docking_targets.json)\",\n    )\n\n    reward: Literal[\"property\", \"valid_smiles\"] = Field(\n        default=\"property\",\n        description='Reward type: \"property\" for property-based or \"valid_smiles\" for validity-based rewards',\n    )\n\n    rescale: bool = Field(\n        default=True,\n        description=\"Whether to rescale rewards to a normalized range\",\n    )\n\n    oracle_kwargs: DockingGPUConfigModel | PyscreenerConfigModel = Field(\n        default_factory=DockingGPUConfigModel,\n        description=\"Keyword arguments for the docking oracle (exhaustiveness, n_cpu, docking_oracle, vina_mode, etc.)\",\n    )\n\n    docking_concurrency_per_gpu: int = Field(\n        default=2,\n        gt=0,\n        description=\"Number of concurrent docking runs per GPU (each uses ~1GB on 80GB GPU)\",\n    )\n\n    parsing_method: Literal[\"none\", \"answer_tags\", \"boxed\"] = Field(\n        default=\"answer_tags\",\n        description=\"Method to parse model completions for SMILES or property values.\",\n    )\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n\n        arbitrary_types_allowed = True\n        json_schema_extra = {\n            \"example\": {\n                \"path_to_mappings\": \"data/molgendata\",\n                \"reward\": \"property\",\n                \"rescale\": True,\n                \"oracle_kwargs\": {\n                    \"exhaustiveness\": 8,\n                    \"n_cpu\": 8,\n                    \"docking_oracle\": \"autodock_gpu\",\n                    \"vina_mode\": \"autodock_gpu_256wi\",\n                },\n                \"docking_concurrency_per_gpu\": 2,\n            }\n        }\n\n    @model_validator(mode=\"after\")\n    def check_mappings_path(self) -&gt; \"GenerationVerifierConfigModel\":\n        \"\"\"Validate that the path_to_mappings exists and contains required files.\"\"\"\n        if self.path_to_mappings is not None:\n            if not os.path.exists(self.path_to_mappings):\n                raise ValueError(\n                    f\"Path to mappings {self.path_to_mappings} does not exist.\"\n                )\n            names_mapping_path = os.path.join(\n                self.path_to_mappings, \"names_mapping.json\"\n            )\n            docking_targets_path = os.path.join(\n                self.path_to_mappings, \"docking_targets.json\"\n            )\n            if not os.path.exists(names_mapping_path):\n                raise ValueError(\n                    f\"names_mapping.json not found at {names_mapping_path}\"\n                )\n            if not os.path.exists(docking_targets_path):\n                raise ValueError(\n                    f\"docking_targets.json not found at {docking_targets_path}\"\n                )\n        return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.GenerationVerifierConfigModel.Config","title":"<code>Config</code>","text":"<p>Pydantic configuration.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>class Config:\n    \"\"\"Pydantic configuration.\"\"\"\n\n    arbitrary_types_allowed = True\n    json_schema_extra = {\n        \"example\": {\n            \"path_to_mappings\": \"data/molgendata\",\n            \"reward\": \"property\",\n            \"rescale\": True,\n            \"oracle_kwargs\": {\n                \"exhaustiveness\": 8,\n                \"n_cpu\": 8,\n                \"docking_oracle\": \"autodock_gpu\",\n                \"vina_mode\": \"autodock_gpu_256wi\",\n            },\n            \"docking_concurrency_per_gpu\": 2,\n        }\n    }\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier_pydantic_model.GenerationVerifierConfigModel.check_mappings_path","title":"<code>check_mappings_path()</code>","text":"<p>Validate that the path_to_mappings exists and contains required files.</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier_pydantic_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_mappings_path(self) -&gt; \"GenerationVerifierConfigModel\":\n    \"\"\"Validate that the path_to_mappings exists and contains required files.\"\"\"\n    if self.path_to_mappings is not None:\n        if not os.path.exists(self.path_to_mappings):\n            raise ValueError(\n                f\"Path to mappings {self.path_to_mappings} does not exist.\"\n            )\n        names_mapping_path = os.path.join(\n            self.path_to_mappings, \"names_mapping.json\"\n        )\n        docking_targets_path = os.path.join(\n            self.path_to_mappings, \"docking_targets.json\"\n        )\n        if not os.path.exists(names_mapping_path):\n            raise ValueError(\n                f\"names_mapping.json not found at {names_mapping_path}\"\n            )\n        if not os.path.exists(docking_targets_path):\n            raise ValueError(\n                f\"docking_targets.json not found at {docking_targets_path}\"\n            )\n    return self\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier","title":"<code>GenerationVerifier</code>","text":"<p>               Bases: <code>Verifier</code></p> <p>Verifier for de novo molecular generation tasks.</p> <p>This verifier computes rewards for generated molecules based on how well they meet specified property optimization criteria. It supports multiple property types including docking scores, QED, SA score, and RDKit descriptors.</p> <p>The verifier uses Ray for parallel computation and supports GPU-accelerated docking calculations when configured with AutoDock GPU.</p> <p>Attributes:</p> Name Type Description <code>verifier_config</code> <code>GenerationVerifierConfigModel</code> <p>Configuration for the generation verifier.</p> <code>property_name_mapping</code> <p>Mapping of property names to oracle names.</p> <code>docking_target_list</code> <p>List of valid docking target names.</p> <code>oracles</code> <code>Dict[str, OracleWrapper]</code> <p>Cache of oracle instances for property computation.</p> <code>debug</code> <p>If True, enables debug mode with additional logging.</p> Example <pre><code>from mol_gen_docking.reward.verifiers import (\n    GenerationVerifier,\n    GenerationVerifierConfigModel,\n    BatchVerifiersInputModel,\n    GenerationVerifierInputMetadataModel\n)\n\nconfig = GenerationVerifierConfigModel(\n    path_to_mappings=\"data/molgendata\",\n    reward=\"property\"\n)\nverifier = GenerationVerifier(config)\n\ninputs = BatchVerifiersInputModel(\n    completions=[\"&lt;answer&gt;CCO&lt;/answer&gt;\"],\n    metadatas=[GenerationVerifierInputMetadataModel(\n        properties=[\"QED\"], objectives=[\"maximize\"], target=[0.0]\n    )]\n)\nresults = verifier.get_score(inputs)\n</code></pre> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>class GenerationVerifier(Verifier):\n    \"\"\"Verifier for de novo molecular generation tasks.\n\n    This verifier computes rewards for generated molecules based on how well\n    they meet specified property optimization criteria. It supports multiple\n    property types including docking scores, QED, SA score, and RDKit descriptors.\n\n    The verifier uses Ray for parallel computation and supports GPU-accelerated\n    docking calculations when configured with AutoDock GPU.\n\n    Attributes:\n        verifier_config: Configuration for the generation verifier.\n        property_name_mapping: Mapping of property names to oracle names.\n        docking_target_list: List of valid docking target names.\n        oracles: Cache of oracle instances for property computation.\n        debug: If True, enables debug mode with additional logging.\n\n    Example:\n        ```python\n        from mol_gen_docking.reward.verifiers import (\n            GenerationVerifier,\n            GenerationVerifierConfigModel,\n            BatchVerifiersInputModel,\n            GenerationVerifierInputMetadataModel\n        )\n\n        config = GenerationVerifierConfigModel(\n            path_to_mappings=\"data/molgendata\",\n            reward=\"property\"\n        )\n        verifier = GenerationVerifier(config)\n\n        inputs = BatchVerifiersInputModel(\n            completions=[\"&lt;answer&gt;CCO&lt;/answer&gt;\"],\n            metadatas=[GenerationVerifierInputMetadataModel(\n                properties=[\"QED\"], objectives=[\"maximize\"], target=[0.0]\n            )]\n        )\n        results = verifier.get_score(inputs)\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        verifier_config: GenerationVerifierConfigModel,\n    ):\n        \"\"\"Initialize the GenerationVerifier.\n\n        Args:\n            verifier_config: Configuration containing paths to mappings,\n                reward type, and docking oracle settings.\n        \"\"\"\n        super().__init__(verifier_config)\n        self.verifier_config: GenerationVerifierConfigModel = verifier_config\n        self.logger = logging.getLogger(\"GenerationVerifier\")\n\n        with open(\n            os.path.join(verifier_config.path_to_mappings, \"names_mapping.json\")\n        ) as f:\n            property_name_mapping = json.load(f)\n        with open(\n            os.path.join(verifier_config.path_to_mappings, \"docking_targets.json\")\n        ) as f:\n            docking_target_list = json.load(f)\n\n        self.property_name_mapping = property_name_mapping\n        self.docking_target_list = docking_target_list\n        self.slow_props = docking_target_list  # + [\"GSK3B\", \"JNK3\", \"DRD2\"]\n\n        self.oracles: Dict[str, OracleWrapper] = {}\n        self.debug = False  # Only for tests\n\n    def get_smiles_from_completion(self, comp: str) -&gt; Tuple[List[str], str]:\n        \"\"\"Extract SMILES strings from a model completion.\n\n        This method parses a model completion to extract valid SMILES strings.\n        It handles various formats including answer tags and markdown formatting.\n\n        Args:\n            comp: The model completion string to parse.\n\n        Returns:\n            Tuple containing:\n                - List of valid SMILES strings found in the completion\n                - Failure reason string (empty if successful, otherwise one of:\n                  \"no_answer\", \"no_smiles\", \"no_valid_smiles\", \"multiple_smiles\")\n\n        Example:\n            ```python\n            smiles, failure = verifier.get_smiles_from_completion(\"&lt;answer&gt;CCO&lt;/answer&gt;\")\n            # smiles = [\"CCO\"], failure = \"\"\n            ```\n        \"\"\"\n        comp = comp.strip()\n        reason: str = \"\"\n        comp = self.parse_answer(comp)\n\n        # Now we identify which elements are possibly SMILES\n        # First we split the completion by newlines and spaces\n        # Then we filter by removing any string that does not contain \"C\"\n        valid_smiles_pattern = re.compile(r\"^[A-Za-z0-9=#:\\+\\-\\[\\]\\(\\)/\\\\@.%]+$\")\n        mkd_pattern = re.compile(r\"^(\\*\\*|[-*'])(.+)\\1$\")\n\n        def filter_smiles(x: str) -&gt; str:\n            x = x.replace(\"&lt;|im_end|&gt;\", \"\")\n            if len(x) &lt; 3:\n                return \"\"\n            # Check if the string is encapsulated in some kind of markdown\n            m = mkd_pattern.match(x)\n            x = m.group(2) if m else x\n            if len(x) &lt; 3:\n                return \"\"\n            if (\n                \"C\" in x\n                or x.count(\"c\") &gt; 2\n                and valid_smiles_pattern.fullmatch(x) is not None\n            ):\n                return x\n            return \"\"\n\n        # Finally we remove any string that is not a valid SMILES\n        def test_is_valid_batch(smis: list[str]) -&gt; list[bool]:\n            RDLogger.DisableLog(\"rdApp.*\")\n            results = []\n            for smi in smis:\n                if len(smi) &gt;= 130:\n                    results.append(False)\n                    continue\n                try:\n                    mol = Chem.MolFromSmiles(smi)\n                    if mol is None:\n                        results.append(False)\n                        continue\n                    if has_bridged_bond(mol):  ### WE REMOVE BRIDGED MOLS\n                        results.append(False)\n                        continue\n                    Chem.MolToMolBlock(mol)\n                    results.append(True)\n                except Exception:\n                    results.append(False)\n            return results\n\n        s_poss = [filter_smiles(x) for x in re.split(\"\\n| |\\\\.|\\t|:|`|'|,\", comp)]\n        s_poss = [x for x in s_poss if x != \"\"]\n        s_poss = list(set(s_poss))\n\n        if len(s_poss) == 0:\n            if reason == \"\":\n                reason = \"no_smiles\"\n            return [], reason\n\n        is_valid: List[bool] = test_is_valid_batch(s_poss)\n\n        s_spl = [x for (x, val) in zip(s_poss, is_valid) if val]\n        if s_spl == [] and reason == \"\":\n            reason = \"no_valid_smiles\"\n        elif len(s_spl) &gt; 1:\n            reason = \"multiple_smiles\"\n        elif reason == \"\":\n            reason = \"\"\n        return s_spl, reason\n\n    def get_all_completions_smiles(\n        self, completions: List[str]\n    ) -&gt; Tuple[List[List[str]], List[str]]:\n        \"\"\"Extract SMILES from multiple completions.\n\n        Args:\n            completions: List of model completion strings.\n\n        Returns:\n            Tuple containing:\n                - List of SMILES lists (one per completion)\n                - List of failure reasons (one per completion)\n        \"\"\"\n        smiles = []\n        failures = []\n        for completion in completions:\n            if isinstance(completion, list):\n                assert len(completion) == 1\n                completion = completion[0]\n            if isinstance(completion, dict):\n                assert \"content\" in completion\n                completion = completion[\"content\"]\n            smi, failure = self.get_smiles_from_completion(completion)\n            smiles.append(smi)\n            failures.append(failure)\n        return smiles, failures\n\n    def fill_df_properties(self, df_properties: pd.DataFrame) -&gt; None:\n        \"\"\"Compute property values for all molecules in a DataFrame.\n\n        This method fills in the 'value' column of the DataFrame with computed\n        property values using the appropriate oracles. It uses Ray for parallel\n        computation, with GPU resources allocated for docking calculations.\n\n        Args:\n            df_properties: DataFrame with columns ['smiles', 'property', 'value',\n                'obj', 'target_value', 'id_completion']. The 'value' column will\n                be filled with computed property values.\n        \"\"\"\n\n        def _get_property(\n            smiles: List[str],\n            prop: str,\n            rescale: bool = True,\n            kwargs: Dict[str, Any] = {},\n        ) -&gt; List[float]:\n            \"\"\"\n            Get property reward\n            \"\"\"\n            oracle_fn = self.oracles.get(\n                prop,\n                get_oracle(\n                    prop,\n                    path_to_data=self.verifier_config.path_to_mappings\n                    if self.verifier_config.path_to_mappings\n                    else \"\",\n                    docking_target_list=self.docking_target_list,\n                    property_name_mapping=self.property_name_mapping,\n                    **kwargs,\n                ),\n            )\n            if prop not in self.oracles:\n                self.oracles[prop] = oracle_fn\n            property_reward: np.ndarray | float = oracle_fn(smiles, rescale=rescale)\n            assert isinstance(property_reward, np.ndarray)\n\n            return [float(p) for p in property_reward]\n\n        _get_property_fast = ray.remote(num_cpus=0)(_get_property)\n        _get_property_long = ray.remote(\n            num_cpus=1,\n            num_gpus=float(\"gpu\" in self.verifier_config.oracle_kwargs.docking_oracle)\n            / self.verifier_config.docking_concurrency_per_gpu,\n        )(_get_property)\n\n        all_properties = df_properties[\"property\"].unique().tolist()\n        prop_smiles = {\n            p: df_properties[df_properties[\"property\"] == p][\"smiles\"].unique().tolist()\n            for p in all_properties\n        }\n\n        values_job = []\n        for p in all_properties:\n            # If the reward is long to compute, use ray\n            smiles = prop_smiles[p]\n            if p in self.slow_props:\n                _get_property_remote = _get_property_long\n            else:\n                _get_property_remote = _get_property_fast\n\n            values_job.append(\n                _get_property_remote.remote(\n                    smiles,\n                    p,\n                    rescale=self.verifier_config.rescale,\n                    kwargs=self.verifier_config.oracle_kwargs.model_dump(),\n                )\n            )\n        all_values = ray.get(values_job)\n        for idx_p, p in enumerate(all_properties):\n            values = all_values[idx_p]\n            smiles = prop_smiles[p]\n            for s, v in zip(smiles, values):\n                df_properties.loc[\n                    (df_properties[\"smiles\"] == s) &amp; (df_properties[\"property\"] == p),\n                    \"value\",\n                ] = v\n\n    def get_reward(self, row: pd.Series) -&gt; float:\n        \"\"\"Compute reward for a single property-molecule pair.\n\n        This method computes the reward based on the objective type:\n        - \"below\": 1.0 if property &lt;= target, else 0.0\n        - \"above\": 1.0 if property &gt;= target, else 0.0\n        - \"maximize\": Returns the property value directly\n        - \"minimize\": Returns 1 - property value\n        - \"equal\": Returns clipped value based on squared error\n\n        Args:\n            row: DataFrame row containing 'obj', 'value', 'target_value', 'property'.\n\n        Returns:\n            Computed reward value (typically 0.0 to 1.0).\n        \"\"\"\n        reward: float = 0\n        obj = row[\"obj\"]\n        mol_prop = row[\"value\"]\n        target_value = row[\"target_value\"]\n        prop = row[\"property\"]\n        is_docking = prop in self.docking_target_list\n        # Replace 0 docking score by the worst outcome\n        if is_docking and prop == 0.0:\n            return 0.0\n        if self.verifier_config.rescale:\n            target_value = rescale_property_values(\n                prop, target_value, docking=is_docking\n            )\n        if obj == \"below\":\n            reward += float(mol_prop &lt;= target_value)\n        elif obj == \"above\":\n            reward += float(mol_prop &gt;= target_value)\n        elif obj == \"maximize\":\n            reward += mol_prop\n        elif obj == \"minimize\":\n            reward += 1 - mol_prop\n        elif obj == \"equal\":\n            reward += np.clip(1 - 100 * (mol_prop - target_value) ** 2, 0, 1)\n        return float(reward)\n\n    def _get_prop_to_smiles_dataframe(\n        self,\n        smiles_list_per_completion: List[List[str]],\n        objectives: List[dict[str, Tuple[GenerationObjT, float]]],\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create a DataFrame mapping properties to SMILES for batch processing.\n\n        Args:\n            smiles_list_per_completion: List of SMILES lists, one per completion.\n            objectives: List of objective dictionaries mapping property names\n                to (objective_type, target_value) tuples.\n\n        Returns:\n            DataFrame with columns: smiles, property, value, obj, target_value, id_completion.\n        \"\"\"\n        df_properties = pd.DataFrame(\n            [\n                (s, p, None, obj, target_value, i)\n                for i, (props, smiles_list) in enumerate(\n                    zip(objectives, smiles_list_per_completion)\n                )\n                for s in smiles_list\n                for p, (obj, target_value) in props.items()\n            ],\n            columns=[\n                \"smiles\",\n                \"property\",\n                \"value\",\n                \"obj\",\n                \"target_value\",\n                \"id_completion\",\n            ],\n        )\n        return df_properties\n\n    def get_score(\n        self, inputs: BatchVerifiersInputModel\n    ) -&gt; List[GenerationVerifierOutputModel]:\n        \"\"\"Compute generation rewards for a batch of completions.\n\n        This method extracts SMILES from completions, computes property values,\n        and calculates rewards based on the specified objectives. The final reward\n        is the geometric mean of per-property rewards.\n\n        Args:\n            inputs: Batch of completions and metadata for verification.\n\n        Returns:\n            List of GenerationVerifierOutputModel containing rewards and metadata\n            for each completion.\n\n        Notes:\n            - If reward type is \"valid_smiles\", returns 1.0 for valid single SMILES\n            - Multiple SMILES in a completion result in 0.0 reward\n            - Uses geometric mean to aggregate multi-property rewards\n        \"\"\"\n        smiles_per_completion, extraction_failures = self.get_all_completions_smiles(\n            inputs.completions\n        )\n        if self.verifier_config.reward == \"valid_smiles\":\n            return [\n                GenerationVerifierOutputModel(\n                    reward=float(len(smis) == 1),\n                    parsed_answer=self.parse_answer(\"; \".join(smis)),\n                    verifier_metadata=GenerationVerifierMetadataModel(\n                        smiles_extraction_failure=fail\n                    ),\n                )\n                for smis, fail in zip(smiles_per_completion, extraction_failures)\n            ]\n        assert all(\n            isinstance(meta, GenerationVerifierInputMetadataModel)\n            for meta in inputs.metadatas\n        )\n        metadatas: List[GenerationVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n        objectives = []\n        for m in metadatas:\n            props = {}\n            for p, obj, target in zip(m.properties, m.objectives, m.target):\n                props[p] = (obj, float(target))\n            objectives.append(props)\n\n        df_properties = self._get_prop_to_smiles_dataframe(\n            smiles_per_completion, objectives\n        )\n        self.fill_df_properties(df_properties)\n        df_properties[\"reward\"] = df_properties.apply(\n            lambda x: self.get_reward(x), axis=1\n        )\n\n        output_models = []\n        for id_completion, smiles in enumerate(smiles_per_completion):\n            properties: List[str] = []\n            individual_rewards: List[float] = []\n            compl_reward: List[float] = []\n            if len(smiles) &gt; 0:\n                for idx_s, s in enumerate(smiles):\n                    rows_completion = df_properties[\n                        (df_properties[\"id_completion\"] == id_completion)\n                        &amp; (df_properties[\"smiles\"] == s)\n                    ]\n                    rewards_l = rows_completion[\"reward\"].to_numpy().clip(0, 1)\n                    reward = np.power(\n                        rewards_l.prod(), (1 / len(rewards_l))\n                    )  # Geometric mean\n                    if idx_s == 0:\n                        for i in range(len(rows_completion[\"smiles\"])):\n                            properties.append(rows_completion[\"property\"].iloc[i])\n                            individual_rewards.append(rows_completion[\"reward\"].iloc[i])\n\n                    if self.verifier_config.rescale and not self.debug:\n                        reward = np.clip(reward, 0, 1)\n                    compl_reward.append(float(reward))\n            else:\n                reward = 0\n                compl_reward = [0.0]\n\n            if np.isnan(reward) or reward is None:\n                self.logger.warning(\n                    f\"Warning: Reward is None or NaN for completion id {id_completion} with smiles {smiles}\\n\"\n                )\n                reward = 0.0\n            if len(smiles) &gt; 1:\n                reward = 0.0\n\n            # Create the output model\n            output_model = GenerationVerifierOutputModel(\n                reward=float(reward),\n                parsed_answer=self.parse_answer(\"; \".join(smiles)),\n                verifier_metadata=GenerationVerifierMetadataModel(\n                    properties=properties,\n                    individual_rewards=individual_rewards,\n                    all_smi_rewards=compl_reward,\n                    all_smi=smiles,\n                    smiles_extraction_failure=extraction_failures[id_completion],\n                ),\n            )\n            output_models.append(output_model)\n\n        return output_models\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.__init__","title":"<code>__init__(verifier_config)</code>","text":"<p>Initialize the GenerationVerifier.</p> <p>Parameters:</p> Name Type Description Default <code>verifier_config</code> <code>GenerationVerifierConfigModel</code> <p>Configuration containing paths to mappings, reward type, and docking oracle settings.</p> required Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def __init__(\n    self,\n    verifier_config: GenerationVerifierConfigModel,\n):\n    \"\"\"Initialize the GenerationVerifier.\n\n    Args:\n        verifier_config: Configuration containing paths to mappings,\n            reward type, and docking oracle settings.\n    \"\"\"\n    super().__init__(verifier_config)\n    self.verifier_config: GenerationVerifierConfigModel = verifier_config\n    self.logger = logging.getLogger(\"GenerationVerifier\")\n\n    with open(\n        os.path.join(verifier_config.path_to_mappings, \"names_mapping.json\")\n    ) as f:\n        property_name_mapping = json.load(f)\n    with open(\n        os.path.join(verifier_config.path_to_mappings, \"docking_targets.json\")\n    ) as f:\n        docking_target_list = json.load(f)\n\n    self.property_name_mapping = property_name_mapping\n    self.docking_target_list = docking_target_list\n    self.slow_props = docking_target_list  # + [\"GSK3B\", \"JNK3\", \"DRD2\"]\n\n    self.oracles: Dict[str, OracleWrapper] = {}\n    self.debug = False  # Only for tests\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.fill_df_properties","title":"<code>fill_df_properties(df_properties)</code>","text":"<p>Compute property values for all molecules in a DataFrame.</p> <p>This method fills in the 'value' column of the DataFrame with computed property values using the appropriate oracles. It uses Ray for parallel computation, with GPU resources allocated for docking calculations.</p> <p>Parameters:</p> Name Type Description Default <code>df_properties</code> <code>DataFrame</code> <p>DataFrame with columns ['smiles', 'property', 'value', 'obj', 'target_value', 'id_completion']. The 'value' column will be filled with computed property values.</p> required Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def fill_df_properties(self, df_properties: pd.DataFrame) -&gt; None:\n    \"\"\"Compute property values for all molecules in a DataFrame.\n\n    This method fills in the 'value' column of the DataFrame with computed\n    property values using the appropriate oracles. It uses Ray for parallel\n    computation, with GPU resources allocated for docking calculations.\n\n    Args:\n        df_properties: DataFrame with columns ['smiles', 'property', 'value',\n            'obj', 'target_value', 'id_completion']. The 'value' column will\n            be filled with computed property values.\n    \"\"\"\n\n    def _get_property(\n        smiles: List[str],\n        prop: str,\n        rescale: bool = True,\n        kwargs: Dict[str, Any] = {},\n    ) -&gt; List[float]:\n        \"\"\"\n        Get property reward\n        \"\"\"\n        oracle_fn = self.oracles.get(\n            prop,\n            get_oracle(\n                prop,\n                path_to_data=self.verifier_config.path_to_mappings\n                if self.verifier_config.path_to_mappings\n                else \"\",\n                docking_target_list=self.docking_target_list,\n                property_name_mapping=self.property_name_mapping,\n                **kwargs,\n            ),\n        )\n        if prop not in self.oracles:\n            self.oracles[prop] = oracle_fn\n        property_reward: np.ndarray | float = oracle_fn(smiles, rescale=rescale)\n        assert isinstance(property_reward, np.ndarray)\n\n        return [float(p) for p in property_reward]\n\n    _get_property_fast = ray.remote(num_cpus=0)(_get_property)\n    _get_property_long = ray.remote(\n        num_cpus=1,\n        num_gpus=float(\"gpu\" in self.verifier_config.oracle_kwargs.docking_oracle)\n        / self.verifier_config.docking_concurrency_per_gpu,\n    )(_get_property)\n\n    all_properties = df_properties[\"property\"].unique().tolist()\n    prop_smiles = {\n        p: df_properties[df_properties[\"property\"] == p][\"smiles\"].unique().tolist()\n        for p in all_properties\n    }\n\n    values_job = []\n    for p in all_properties:\n        # If the reward is long to compute, use ray\n        smiles = prop_smiles[p]\n        if p in self.slow_props:\n            _get_property_remote = _get_property_long\n        else:\n            _get_property_remote = _get_property_fast\n\n        values_job.append(\n            _get_property_remote.remote(\n                smiles,\n                p,\n                rescale=self.verifier_config.rescale,\n                kwargs=self.verifier_config.oracle_kwargs.model_dump(),\n            )\n        )\n    all_values = ray.get(values_job)\n    for idx_p, p in enumerate(all_properties):\n        values = all_values[idx_p]\n        smiles = prop_smiles[p]\n        for s, v in zip(smiles, values):\n            df_properties.loc[\n                (df_properties[\"smiles\"] == s) &amp; (df_properties[\"property\"] == p),\n                \"value\",\n            ] = v\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.get_all_completions_smiles","title":"<code>get_all_completions_smiles(completions)</code>","text":"<p>Extract SMILES from multiple completions.</p> <p>Parameters:</p> Name Type Description Default <code>completions</code> <code>List[str]</code> <p>List of model completion strings.</p> required <p>Returns:</p> Type Description <code>Tuple[List[List[str]], List[str]]</code> <p>Tuple containing: - List of SMILES lists (one per completion) - List of failure reasons (one per completion)</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def get_all_completions_smiles(\n    self, completions: List[str]\n) -&gt; Tuple[List[List[str]], List[str]]:\n    \"\"\"Extract SMILES from multiple completions.\n\n    Args:\n        completions: List of model completion strings.\n\n    Returns:\n        Tuple containing:\n            - List of SMILES lists (one per completion)\n            - List of failure reasons (one per completion)\n    \"\"\"\n    smiles = []\n    failures = []\n    for completion in completions:\n        if isinstance(completion, list):\n            assert len(completion) == 1\n            completion = completion[0]\n        if isinstance(completion, dict):\n            assert \"content\" in completion\n            completion = completion[\"content\"]\n        smi, failure = self.get_smiles_from_completion(completion)\n        smiles.append(smi)\n        failures.append(failure)\n    return smiles, failures\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.get_reward","title":"<code>get_reward(row)</code>","text":"<p>Compute reward for a single property-molecule pair.</p> <p>This method computes the reward based on the objective type: - \"below\": 1.0 if property &lt;= target, else 0.0 - \"above\": 1.0 if property &gt;= target, else 0.0 - \"maximize\": Returns the property value directly - \"minimize\": Returns 1 - property value - \"equal\": Returns clipped value based on squared error</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>DataFrame row containing 'obj', 'value', 'target_value', 'property'.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Computed reward value (typically 0.0 to 1.0).</p> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def get_reward(self, row: pd.Series) -&gt; float:\n    \"\"\"Compute reward for a single property-molecule pair.\n\n    This method computes the reward based on the objective type:\n    - \"below\": 1.0 if property &lt;= target, else 0.0\n    - \"above\": 1.0 if property &gt;= target, else 0.0\n    - \"maximize\": Returns the property value directly\n    - \"minimize\": Returns 1 - property value\n    - \"equal\": Returns clipped value based on squared error\n\n    Args:\n        row: DataFrame row containing 'obj', 'value', 'target_value', 'property'.\n\n    Returns:\n        Computed reward value (typically 0.0 to 1.0).\n    \"\"\"\n    reward: float = 0\n    obj = row[\"obj\"]\n    mol_prop = row[\"value\"]\n    target_value = row[\"target_value\"]\n    prop = row[\"property\"]\n    is_docking = prop in self.docking_target_list\n    # Replace 0 docking score by the worst outcome\n    if is_docking and prop == 0.0:\n        return 0.0\n    if self.verifier_config.rescale:\n        target_value = rescale_property_values(\n            prop, target_value, docking=is_docking\n        )\n    if obj == \"below\":\n        reward += float(mol_prop &lt;= target_value)\n    elif obj == \"above\":\n        reward += float(mol_prop &gt;= target_value)\n    elif obj == \"maximize\":\n        reward += mol_prop\n    elif obj == \"minimize\":\n        reward += 1 - mol_prop\n    elif obj == \"equal\":\n        reward += np.clip(1 - 100 * (mol_prop - target_value) ** 2, 0, 1)\n    return float(reward)\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.get_score","title":"<code>get_score(inputs)</code>","text":"<p>Compute generation rewards for a batch of completions.</p> <p>This method extracts SMILES from completions, computes property values, and calculates rewards based on the specified objectives. The final reward is the geometric mean of per-property rewards.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>BatchVerifiersInputModel</code> <p>Batch of completions and metadata for verification.</p> required <p>Returns:</p> Type Description <code>List[GenerationVerifierOutputModel]</code> <p>List of GenerationVerifierOutputModel containing rewards and metadata</p> <code>List[GenerationVerifierOutputModel]</code> <p>for each completion.</p> Notes <ul> <li>If reward type is \"valid_smiles\", returns 1.0 for valid single SMILES</li> <li>Multiple SMILES in a completion result in 0.0 reward</li> <li>Uses geometric mean to aggregate multi-property rewards</li> </ul> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def get_score(\n    self, inputs: BatchVerifiersInputModel\n) -&gt; List[GenerationVerifierOutputModel]:\n    \"\"\"Compute generation rewards for a batch of completions.\n\n    This method extracts SMILES from completions, computes property values,\n    and calculates rewards based on the specified objectives. The final reward\n    is the geometric mean of per-property rewards.\n\n    Args:\n        inputs: Batch of completions and metadata for verification.\n\n    Returns:\n        List of GenerationVerifierOutputModel containing rewards and metadata\n        for each completion.\n\n    Notes:\n        - If reward type is \"valid_smiles\", returns 1.0 for valid single SMILES\n        - Multiple SMILES in a completion result in 0.0 reward\n        - Uses geometric mean to aggregate multi-property rewards\n    \"\"\"\n    smiles_per_completion, extraction_failures = self.get_all_completions_smiles(\n        inputs.completions\n    )\n    if self.verifier_config.reward == \"valid_smiles\":\n        return [\n            GenerationVerifierOutputModel(\n                reward=float(len(smis) == 1),\n                parsed_answer=self.parse_answer(\"; \".join(smis)),\n                verifier_metadata=GenerationVerifierMetadataModel(\n                    smiles_extraction_failure=fail\n                ),\n            )\n            for smis, fail in zip(smiles_per_completion, extraction_failures)\n        ]\n    assert all(\n        isinstance(meta, GenerationVerifierInputMetadataModel)\n        for meta in inputs.metadatas\n    )\n    metadatas: List[GenerationVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n    objectives = []\n    for m in metadatas:\n        props = {}\n        for p, obj, target in zip(m.properties, m.objectives, m.target):\n            props[p] = (obj, float(target))\n        objectives.append(props)\n\n    df_properties = self._get_prop_to_smiles_dataframe(\n        smiles_per_completion, objectives\n    )\n    self.fill_df_properties(df_properties)\n    df_properties[\"reward\"] = df_properties.apply(\n        lambda x: self.get_reward(x), axis=1\n    )\n\n    output_models = []\n    for id_completion, smiles in enumerate(smiles_per_completion):\n        properties: List[str] = []\n        individual_rewards: List[float] = []\n        compl_reward: List[float] = []\n        if len(smiles) &gt; 0:\n            for idx_s, s in enumerate(smiles):\n                rows_completion = df_properties[\n                    (df_properties[\"id_completion\"] == id_completion)\n                    &amp; (df_properties[\"smiles\"] == s)\n                ]\n                rewards_l = rows_completion[\"reward\"].to_numpy().clip(0, 1)\n                reward = np.power(\n                    rewards_l.prod(), (1 / len(rewards_l))\n                )  # Geometric mean\n                if idx_s == 0:\n                    for i in range(len(rows_completion[\"smiles\"])):\n                        properties.append(rows_completion[\"property\"].iloc[i])\n                        individual_rewards.append(rows_completion[\"reward\"].iloc[i])\n\n                if self.verifier_config.rescale and not self.debug:\n                    reward = np.clip(reward, 0, 1)\n                compl_reward.append(float(reward))\n        else:\n            reward = 0\n            compl_reward = [0.0]\n\n        if np.isnan(reward) or reward is None:\n            self.logger.warning(\n                f\"Warning: Reward is None or NaN for completion id {id_completion} with smiles {smiles}\\n\"\n            )\n            reward = 0.0\n        if len(smiles) &gt; 1:\n            reward = 0.0\n\n        # Create the output model\n        output_model = GenerationVerifierOutputModel(\n            reward=float(reward),\n            parsed_answer=self.parse_answer(\"; \".join(smiles)),\n            verifier_metadata=GenerationVerifierMetadataModel(\n                properties=properties,\n                individual_rewards=individual_rewards,\n                all_smi_rewards=compl_reward,\n                all_smi=smiles,\n                smiles_extraction_failure=extraction_failures[id_completion],\n            ),\n        )\n        output_models.append(output_model)\n\n    return output_models\n</code></pre>"},{"location":"api/reward/generation_verifier/#mol_gen_docking.reward.verifiers.generation_reward.generation_verifier.GenerationVerifier.get_smiles_from_completion","title":"<code>get_smiles_from_completion(comp)</code>","text":"<p>Extract SMILES strings from a model completion.</p> <p>This method parses a model completion to extract valid SMILES strings. It handles various formats including answer tags and markdown formatting.</p> <p>Parameters:</p> Name Type Description Default <code>comp</code> <code>str</code> <p>The model completion string to parse.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], str]</code> <p>Tuple containing: - List of valid SMILES strings found in the completion - Failure reason string (empty if successful, otherwise one of:   \"no_answer\", \"no_smiles\", \"no_valid_smiles\", \"multiple_smiles\")</p> Example <pre><code>smiles, failure = verifier.get_smiles_from_completion(\"&lt;answer&gt;CCO&lt;/answer&gt;\")\n# smiles = [\"CCO\"], failure = \"\"\n</code></pre> Source code in <code>mol_gen_docking/reward/verifiers/generation_reward/generation_verifier.py</code> <pre><code>def get_smiles_from_completion(self, comp: str) -&gt; Tuple[List[str], str]:\n    \"\"\"Extract SMILES strings from a model completion.\n\n    This method parses a model completion to extract valid SMILES strings.\n    It handles various formats including answer tags and markdown formatting.\n\n    Args:\n        comp: The model completion string to parse.\n\n    Returns:\n        Tuple containing:\n            - List of valid SMILES strings found in the completion\n            - Failure reason string (empty if successful, otherwise one of:\n              \"no_answer\", \"no_smiles\", \"no_valid_smiles\", \"multiple_smiles\")\n\n    Example:\n        ```python\n        smiles, failure = verifier.get_smiles_from_completion(\"&lt;answer&gt;CCO&lt;/answer&gt;\")\n        # smiles = [\"CCO\"], failure = \"\"\n        ```\n    \"\"\"\n    comp = comp.strip()\n    reason: str = \"\"\n    comp = self.parse_answer(comp)\n\n    # Now we identify which elements are possibly SMILES\n    # First we split the completion by newlines and spaces\n    # Then we filter by removing any string that does not contain \"C\"\n    valid_smiles_pattern = re.compile(r\"^[A-Za-z0-9=#:\\+\\-\\[\\]\\(\\)/\\\\@.%]+$\")\n    mkd_pattern = re.compile(r\"^(\\*\\*|[-*'])(.+)\\1$\")\n\n    def filter_smiles(x: str) -&gt; str:\n        x = x.replace(\"&lt;|im_end|&gt;\", \"\")\n        if len(x) &lt; 3:\n            return \"\"\n        # Check if the string is encapsulated in some kind of markdown\n        m = mkd_pattern.match(x)\n        x = m.group(2) if m else x\n        if len(x) &lt; 3:\n            return \"\"\n        if (\n            \"C\" in x\n            or x.count(\"c\") &gt; 2\n            and valid_smiles_pattern.fullmatch(x) is not None\n        ):\n            return x\n        return \"\"\n\n    # Finally we remove any string that is not a valid SMILES\n    def test_is_valid_batch(smis: list[str]) -&gt; list[bool]:\n        RDLogger.DisableLog(\"rdApp.*\")\n        results = []\n        for smi in smis:\n            if len(smi) &gt;= 130:\n                results.append(False)\n                continue\n            try:\n                mol = Chem.MolFromSmiles(smi)\n                if mol is None:\n                    results.append(False)\n                    continue\n                if has_bridged_bond(mol):  ### WE REMOVE BRIDGED MOLS\n                    results.append(False)\n                    continue\n                Chem.MolToMolBlock(mol)\n                results.append(True)\n            except Exception:\n                results.append(False)\n        return results\n\n    s_poss = [filter_smiles(x) for x in re.split(\"\\n| |\\\\.|\\t|:|`|'|,\", comp)]\n    s_poss = [x for x in s_poss if x != \"\"]\n    s_poss = list(set(s_poss))\n\n    if len(s_poss) == 0:\n        if reason == \"\":\n            reason = \"no_smiles\"\n        return [], reason\n\n    is_valid: List[bool] = test_is_valid_batch(s_poss)\n\n    s_spl = [x for (x, val) in zip(s_poss, is_valid) if val]\n    if s_spl == [] and reason == \"\":\n        reason = \"no_valid_smiles\"\n    elif len(s_spl) &gt; 1:\n        reason = \"multiple_smiles\"\n    elif reason == \"\":\n        reason = \"\"\n    return s_spl, reason\n</code></pre>"},{"location":"api/reward/generation_verifier/#related","title":"Related","text":"<ul> <li>Molecular Verifier - Main orchestrator</li> <li>Property Verifier - Molecular property prediction tasks</li> <li>Reaction Verifier - Reaction prediction and retro-synthesis tasks</li> </ul>"},{"location":"api/reward/mol_prop_verifier/","title":"Property Verifier","text":"<p>The <code>MolPropVerifier</code> computes rewards for molecular property prediction tasks, supporting both regression and classification objectives.</p>"},{"location":"api/reward/mol_prop_verifier/#overview","title":"Overview","text":"<p>The Property Verifier evaluates model predictions of molecular properties such as:</p> <ul> <li>ADMET Properties: Absorption, Distribution, Metabolism, Excretion, Toxicity</li> <li>Physicochemical Properties: Solubility, LogP, pKa</li> <li>Biological Activity: IC50, EC50, binding affinity</li> </ul>"},{"location":"api/reward/mol_prop_verifier/#answer-extraction","title":"Answer Extraction","text":"<p>The verifier extracts answers from completions using:</p> <ol> <li>Answer Tags: Content between <code>&lt;answer&gt;</code> and <code>&lt;/answer&gt;</code> tags</li> <li>Value Parsing: Extracts numeric values or boolean keywords</li> </ol>"},{"location":"api/reward/mol_prop_verifier/#supported-answer-formats","title":"Supported Answer Formats","text":"Format Parsed Value <code>0.75</code> 0.75 (float) <code>42</code> 42 (int) <code>True</code>, <code>Yes</code> 1 <code>False</code>, <code>No</code> 0"},{"location":"api/reward/mol_prop_verifier/#invalid-answers","title":"Invalid Answers","text":"<p>Answers are considered invalid (reward = 0.0) if: - No answer tags found - Multiple values in answer - No numeric/boolean value found</p> <p>Molecular property verifier for property prediction tasks.</p> <p>This module provides the MolPropVerifier class which computes rewards for molecular property prediction tasks, supporting both regression and classification objectives.</p>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier_pydantic_model.MolPropVerifierConfigModel","title":"<code>MolPropVerifierConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for molecular verifier configuration.</p> <p>This model defines the configuration parameters for the MolecularVerifier class, providing validation and documentation for all configuration options.</p> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier_pydantic_model.py</code> <pre><code>class MolPropVerifierConfigModel(BaseModel):\n    \"\"\"Pydantic model for molecular verifier configuration.\n\n    This model defines the configuration parameters for the MolecularVerifier class,\n    providing validation and documentation for all configuration options.\n    \"\"\"\n\n    reward: Literal[\"property\", \"valid_smiles\"] = Field(\n        default=\"property\",\n        description='Reward type: \"property\" for property-based or \"valid_smiles\" for validity-based rewards',\n    )\n    parsing_method: Literal[\"none\", \"answer_tags\", \"boxed\"] = Field(\n        default=\"answer_tags\",\n        description=\"Method to parse model completions for SMILES or property values.\",\n    )\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n\n        arbitrary_types_allowed = True\n        validate_assignment = True\n        json_schema_extra = {\n            \"example\": {\n                \"reward\": \"property\",\n                \"parsing_method\": \"answer_tags\",\n            }\n        }\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier_pydantic_model.MolPropVerifierConfigModel.Config","title":"<code>Config</code>","text":"<p>Pydantic configuration.</p> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier_pydantic_model.py</code> <pre><code>class Config:\n    \"\"\"Pydantic configuration.\"\"\"\n\n    arbitrary_types_allowed = True\n    validate_assignment = True\n    json_schema_extra = {\n        \"example\": {\n            \"reward\": \"property\",\n            \"parsing_method\": \"answer_tags\",\n        }\n    }\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.input_metadata.MolPropVerifierInputMetadataModel","title":"<code>MolPropVerifierInputMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input metadata model for molecular property verifier.</p> <p>Defines the verification criteria for molecular property prediction tasks, including the type of objective (regression or classification), properties being predicted, target values, and normalization parameters.</p> <p>Attributes:</p> Name Type Description <code>objectives</code> <code>List[MolPropObjT]</code> <p>List of objective types for each property. Valid values: - \"regression\": Continuous value prediction (e.g., predicting logP, molecular weight) - \"classification\": Binary classification (0 or 1) Must have the same length as properties and target.</p> <code>properties</code> <code>List[str]</code> <p>List of molecular property names being predicted. Optional, used for tracking and logging purposes. Examples: \"logP\", \"MW\", \"toxicity\", \"solubility\"</p> <code>target</code> <code>List[float | int]</code> <p>List of target values for each property to verify against. For regression: The ground truth continuous value For classification: The ground truth class (0 or 1) Must have the same length as objectives.</p> <code>norm_var</code> <code>float | int | None</code> <p>Normalization variance for regression objectives. Optional parameter used to compute normalized rewards for regression tasks. The reward is computed as: reward = max(0, 1 - |predicted - target| / norm_var) If None, no normalization is applied and absolute error is used directly.</p> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/input_metadata.py</code> <pre><code>class MolPropVerifierInputMetadataModel(BaseModel):\n    \"\"\"Input metadata model for molecular property verifier.\n\n    Defines the verification criteria for molecular property prediction tasks,\n    including the type of objective (regression or classification), properties\n    being predicted, target values, and normalization parameters.\n\n    Attributes:\n        objectives: List of objective types for each property.\n            Valid values:\n            - \"regression\": Continuous value prediction (e.g., predicting logP, molecular weight)\n            - \"classification\": Binary classification (0 or 1)\n            Must have the same length as properties and target.\n\n        properties: List of molecular property names being predicted.\n            Optional, used for tracking and logging purposes.\n            Examples: \"logP\", \"MW\", \"toxicity\", \"solubility\"\n\n        target: List of target values for each property to verify against.\n            For regression: The ground truth continuous value\n            For classification: The ground truth class (0 or 1)\n            Must have the same length as objectives.\n\n        norm_var: Normalization variance for regression objectives.\n            Optional parameter used to compute normalized rewards for regression tasks.\n            The reward is computed as: reward = max(0, 1 - |predicted - target| / norm_var)\n            If None, no normalization is applied and absolute error is used directly.\n    \"\"\"\n\n    objectives: List[MolPropObjT] = Field(\n        ...,\n        description=\"The type of objective for the property: regression or classification.\",\n    )\n    properties: List[str] = Field(\n        default_factory=list,\n        description=\"The molecular properties to be verified.\",\n    )\n    target: List[float | int] = Field(\n        ...,\n        description=\"The target value for the molecular property to verify against.\",\n    )\n    norm_var: float | int | None = Field(\n        default=None,\n        description=\"Normalization variance for regression objectives.\",\n    )\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier_pydantic_model.MolPropVerifierOutputModel","title":"<code>MolPropVerifierOutputModel</code>","text":"<p>               Bases: <code>VerifierOutputModel</code></p> <p>Output model for molecular property verifier results.</p> <p>Attributes:</p> Name Type Description <code>reward</code> <code>float</code> <p>The computed reward for the molecular property verification.</p> <code>parsed_answer</code> <code>str</code> <p>The parsed answer extracted from the model completion.</p> <code>verifier_metadata</code> <code>MolPropVerifierMetadataModel</code> <p>Metadata related to the molecular property verification process.</p> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier_pydantic_model.py</code> <pre><code>class MolPropVerifierOutputModel(VerifierOutputModel):\n    \"\"\"Output model for molecular property verifier results.\n\n    Attributes:\n        reward: The computed reward for the molecular property verification.\n        parsed_answer: The parsed answer extracted from the model completion.\n        verifier_metadata: Metadata related to the molecular property verification process.\n    \"\"\"\n\n    reward: float = Field(\n        ...,\n        description=\"The computed reward for the molecular property verification.\",\n    )\n    parsed_answer: str = Field(\n        ...,\n        description=\"The parsed answer from the model completion.\",\n    )\n    verifier_metadata: MolPropVerifierMetadataModel = Field(\n        ...,\n        description=\"Metadata related to the molecular property verification process.\",\n    )\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier_pydantic_model.MolPropVerifierMetadataModel","title":"<code>MolPropVerifierMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata model for molecular property verifier results.</p> <p>Contains detailed information about the property prediction verification process, including the extracted answer and whether extraction was successful.</p> <p>Attributes:</p> Name Type Description <code>extracted_answer</code> <code>float</code> <p>The numerical answer extracted from the model completion. For regression tasks, this is the predicted property value. For classification tasks, this is the predicted class (0 or 1).</p> <code>extraction_success</code> <code>bool</code> <p>Indicates whether the answer extraction was successful. False if the answer could not be parsed from the completion text, or if the answer format was invalid (e.g., non-numeric for regression).</p> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier_pydantic_model.py</code> <pre><code>class MolPropVerifierMetadataModel(BaseModel):\n    \"\"\"Metadata model for molecular property verifier results.\n\n    Contains detailed information about the property prediction verification process,\n    including the extracted answer and whether extraction was successful.\n\n    Attributes:\n        extracted_answer: The numerical answer extracted from the model completion.\n            For regression tasks, this is the predicted property value.\n            For classification tasks, this is the predicted class (0 or 1).\n\n        extraction_success: Indicates whether the answer extraction was successful.\n            False if the answer could not be parsed from the completion text,\n            or if the answer format was invalid (e.g., non-numeric for regression).\n    \"\"\"\n\n    extracted_answer: float = Field(\n        ...,\n        description=\"The extracted answer string from the model completion.\",\n    )\n    extraction_success: bool = Field(\n        ...,\n        description=\"Indicates whether the answer extraction was successful.\",\n    )\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier.MolPropVerifier","title":"<code>MolPropVerifier</code>","text":"<p>               Bases: <code>Verifier</code></p> <p>Verifier for molecular property prediction tasks.</p> <p>This verifier computes rewards for property prediction based on how close the predicted value is to the ground truth. It supports both regression tasks (using normalized squared error) and classification tasks (using exact match accuracy).</p> <p>Attributes:</p> Name Type Description <code>verifier_config</code> <code>MolPropVerifierConfigModel</code> <p>Configuration for the property verifier.</p> <code>logger</code> <p>Logger instance for the verifier.</p> Example <pre><code>from mol_gen_docking.reward.verifiers import (\n    MolPropVerifier,\n    MolPropVerifierConfigModel,\n    BatchVerifiersInputModel,\n    MolPropVerifierInputMetadataModel\n)\n\nconfig = MolPropVerifierConfigModel(reward=\"property\")\nverifier = MolPropVerifier(config)\n\ninputs = BatchVerifiersInputModel(\n    completions=[\"&lt;answer&gt;0.75&lt;/answer&gt;\"],\n    metadatas=[MolPropVerifierInputMetadataModel(\n        objectives=[\"regression\"],\n        target=[0.8],\n        norm_var=0.1\n    )]\n)\nresults = verifier.get_score(inputs)\n</code></pre> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier.py</code> <pre><code>class MolPropVerifier(Verifier):\n    \"\"\"Verifier for molecular property prediction tasks.\n\n    This verifier computes rewards for property prediction based on how close\n    the predicted value is to the ground truth. It supports both regression\n    tasks (using normalized squared error) and classification tasks (using\n    exact match accuracy).\n\n    Attributes:\n        verifier_config: Configuration for the property verifier.\n        logger: Logger instance for the verifier.\n\n    Example:\n        ```python\n        from mol_gen_docking.reward.verifiers import (\n            MolPropVerifier,\n            MolPropVerifierConfigModel,\n            BatchVerifiersInputModel,\n            MolPropVerifierInputMetadataModel\n        )\n\n        config = MolPropVerifierConfigModel(reward=\"property\")\n        verifier = MolPropVerifier(config)\n\n        inputs = BatchVerifiersInputModel(\n            completions=[\"&lt;answer&gt;0.75&lt;/answer&gt;\"],\n            metadatas=[MolPropVerifierInputMetadataModel(\n                objectives=[\"regression\"],\n                target=[0.8],\n                norm_var=0.1\n            )]\n        )\n        results = verifier.get_score(inputs)\n        ```\n    \"\"\"\n\n    # ==========================================================================\n    # Regex patterns for number extraction\n    # ==========================================================================\n\n    # Base patterns (building blocks)\n    SIGN_PATTERN = r\"[-\u2212+]?\"\n    INTEGER_PATTERN = r\"\\d+\"\n    DECIMAL_PATTERN = r\"(?:\\.\\d+)?\"\n    BASE_FLOAT_PATTERN = rf\"{SIGN_PATTERN}{INTEGER_PATTERN}{DECIMAL_PATTERN}\"\n\n    # Multiplication symbol pattern: \"x\", \"\u00d7\", or \"\\times\" (single or double backslash)\n    MULT_PATTERN = r\"(?:\\\\{1,2}times|[x\u00d7])\"\n\n    # Unicode superscript digits and signs for patterns like \"10\u207b\u2076\"\n    UNICODE_SUPERSCRIPT_SIGN = r\"[\u207a\u207b]?\"\n    UNICODE_SUPERSCRIPT_DIGITS = r\"[\u2070\u00b9\u00b2\u00b3\u2074\u2075\u2076\u2077\u2078\u2079]+\"\n\n    # Pattern for scientific notation with &lt;sup&gt; tags: \"1.0 x 10&lt;sup&gt;-6&lt;/sup&gt;\"\n    SCI_SUP_PATTERN = (\n        rf\"{BASE_FLOAT_PATTERN}\\s*[x\u00d7]\\s*10\\s*&lt;sup&gt;{BASE_FLOAT_PATTERN}&lt;/sup&gt;\"\n    )\n\n    # Pattern for Unicode superscript notation: \"2.1 \u00d7 10\u207b\u2076\"\n    SCI_UNICODE_PATTERN = rf\"{BASE_FLOAT_PATTERN}\\s*{MULT_PATTERN}\\s*10{UNICODE_SUPERSCRIPT_SIGN}{UNICODE_SUPERSCRIPT_DIGITS}\"\n\n    # Pattern for LaTeX scientific notation: \"1.3 \\times 10^{-5}\" or \"1.7 \u00d7 10^{-4}\"\n    SCI_LATEX_PATTERN = (\n        rf\"{BASE_FLOAT_PATTERN}\\s*{MULT_PATTERN}\\s*10\\s*\\^\\s*\\{{{BASE_FLOAT_PATTERN}\\}}\"\n    )\n\n    # Pattern for caret notation without braces: \"1.3 \u00d7 10^-4\" or \"1.6 x 10^{-5}\"\n    SCI_CARET_PATTERN = rf\"{BASE_FLOAT_PATTERN}\\s*{MULT_PATTERN}\\s*10\\s*\\^?\\s*\\{{?{BASE_FLOAT_PATTERN}\\}}?\"\n\n    # Pattern for plus-minus notation: \"-2.1 \u00b1 0.5\" or \"1.3 +- 4\"\n    PM_PATTERN = rf\"{BASE_FLOAT_PATTERN}\\s*(?:\u00b1|\\+-|\\+/-)\\s*{BASE_FLOAT_PATTERN}\"\n\n    # Pattern for standard scientific notation: \"1e-3\", \"1.5E+6\"\n    SCI_E_PATTERN = rf\"{BASE_FLOAT_PATTERN}[eE]{BASE_FLOAT_PATTERN}\"\n\n    # Pattern for plain float: \"1.0\" or \"-2.5\"\n    FLOAT_PATTERN = BASE_FLOAT_PATTERN\n\n    # Pattern for percentage: \"14%\" should be matched and converted to 0.14\n    PERCENTAGE_PATTERN = rf\"{BASE_FLOAT_PATTERN}%\"\n\n    # Combined number pattern (order matters: most specific first)\n    NUM_PATTERN = rf\"(?:{SCI_SUP_PATTERN}|{SCI_UNICODE_PATTERN}|{SCI_LATEX_PATTERN}|{SCI_CARET_PATTERN}|{PM_PATTERN}|{SCI_E_PATTERN}|{FLOAT_PATTERN})\"\n\n    # Pattern with boundary checks to exclude numbers with slashes or preceded by ^ (possibly between {})\n    NUM_WITH_BOUNDARY_PATTERN = rf\"(?&lt;![/\\w.\\^+\u2212-])(?&lt;!\\^\\{{)(?&lt;!\\^\\()({SCI_SUP_PATTERN}|{SCI_UNICODE_PATTERN}|{SCI_LATEX_PATTERN}|{SCI_CARET_PATTERN}|{PM_PATTERN}|{SCI_E_PATTERN}|{PERCENTAGE_PATTERN}|{FLOAT_PATTERN})(?![%\\d/])(?!\u00b0C)\"\n\n    # ==========================================================================\n    # Extraction patterns (with capture groups for parsing)\n    # ==========================================================================\n\n    # Extraction pattern for standard scientific notation: captures base and exponent\n    SCI_E_EXTRACT = rf\"({BASE_FLOAT_PATTERN})[eE]({BASE_FLOAT_PATTERN})\"\n\n    # Extraction pattern for &lt;sup&gt; notation: captures base and exponent\n    SCI_SUP_EXTRACT = (\n        rf\"({BASE_FLOAT_PATTERN})\\s*[x\u00d7]\\s*10\\s*&lt;sup&gt;({BASE_FLOAT_PATTERN})&lt;/sup&gt;\"\n    )\n\n    # Extraction pattern for Unicode superscript: captures base and exponent\n    SCI_UNICODE_EXTRACT = rf\"({BASE_FLOAT_PATTERN})\\s*{MULT_PATTERN}\\s*10({UNICODE_SUPERSCRIPT_SIGN}{UNICODE_SUPERSCRIPT_DIGITS})\"\n\n    # Extraction pattern for LaTeX notation: captures base and exponent\n    SCI_LATEX_EXTRACT = rf\"({BASE_FLOAT_PATTERN})\\s*{MULT_PATTERN}\\s*10\\s*\\^\\s*\\{{({BASE_FLOAT_PATTERN})\\}}\"\n\n    # Extraction pattern for caret notation: captures base and exponent (with optional braces)\n    SCI_CARET_EXTRACT = rf\"({BASE_FLOAT_PATTERN})\\s*{MULT_PATTERN}\\s*10\\s*\\^?\\s*\\{{?({BASE_FLOAT_PATTERN})\\}}?\"\n\n    # Extraction pattern for plus-minus: captures central value\n    PM_EXTRACT = rf\"({BASE_FLOAT_PATTERN})\\s*(?:\u00b1|\\+-|\\+/-)\\s*{BASE_FLOAT_PATTERN}\"\n\n    # ==========================================================================\n    # Classification answer keywords\n    # ==========================================================================\n\n    CLASSIFICATION_TRUE_KEYWORDS = [\n        \"true\",\n        \"yes\",\n        \"1\",\n        \"1.0\",\n        \"high\",\n        \"highly\",\n        \"likely\",\n        \"y\",\n    ]\n    CLASSIFICATION_FALSE_KEYWORDS = [\"false\", \"no\", \"0\", \"0.0\", \"low\", \"poor\", \"n\"]\n\n    def __init__(self, verifier_config: MolPropVerifierConfigModel) -&gt; None:\n        \"\"\"Initialize the MolPropVerifier.\n\n        Args:\n            verifier_config: Configuration containing reward type settings.\n        \"\"\"\n        super().__init__(verifier_config)\n        self.verifier_config: MolPropVerifierConfigModel = verifier_config\n        self.logger = logging.getLogger(\"MolPropVerifier\")\n\n    def _parse_float_with_sup(self, s: str) -&gt; float:\n        \"\"\"Parse a float that may include scientific notation, percentages, or other formats.\n\n        Handles patterns like:\n        - \"1.0 x 10&lt;sup&gt;-6&lt;/sup&gt;\" -&gt; 1.0e-6\n        - \"1.3 \\times 10^{-5}\" -&gt; 1.3e-5\n        - \"3.97 \\\\times 10^{-5}\" -&gt; 3.97e-5\n        - \"1.7 \u00d7 10^{-4}\" -&gt; 1.7e-4\n        - \"1.6 x 10^-5\" -&gt; 1.6e-5\n        - \"2.1 \u00d7 10\u207b\u2076\" -&gt; 2.1e-6 (Unicode superscript)\n        - \"5.0 \u00d7 10&lt;sup&gt;3&lt;/sup&gt;\" -&gt; 5000.0\n        - \"1e-3\" or \"1E-3\" -&gt; 0.001\n        - \"-2.1 \u00b1 0.5\" -&gt; -2.1 (takes the central value)\n        - \"1.5\" -&gt; 1.5\n        - \"14%\" -&gt; 0.14\n\n        Args:\n            s: String to parse.\n\n        Returns:\n            Parsed float value.\n\n        Raises:\n            ValueError: If string cannot be parsed as a float.\n        \"\"\"\n        s = s.strip()\n        # Check for percentage pattern: \"14%\" -&gt; 0.14\n        if re.match(self.PERCENTAGE_PATTERN, s):\n            perc = s[:-1].replace(\"\u2212\", \"-\")\n            return float(perc) / 100\n\n        out: float\n        # Match standard scientific notation: \"1e-3\" or \"1E-3\"\n        sci_e_match = re.match(self.SCI_E_EXTRACT, s)\n        if sci_e_match:\n            base = float(sci_e_match.group(1).replace(\"\u2212\", \"-\"))\n            exp = float(sci_e_match.group(2).replace(\"\u2212\", \"-\"))\n            out = base * (10**exp)\n            return out\n\n        # Match HTML &lt;sup&gt; pattern: \"1.0 x 10&lt;sup&gt;-6&lt;/sup&gt;\"\n        sup_match = re.match(self.SCI_SUP_EXTRACT, s)\n        if sup_match:\n            base = float(sup_match.group(1).replace(\"\u2212\", \"-\"))\n            exp = float(sup_match.group(2).replace(\"\u2212\", \"-\"))\n            out = base * (10**exp)\n            return out\n\n        # Match Unicode superscript pattern: \"2.1 \u00d7 10\u207b\u2076\"\n        unicode_match = re.match(self.SCI_UNICODE_EXTRACT, s)\n        if unicode_match:\n            base = float(unicode_match.group(1))\n            exp_str = unicode_match.group(2)\n            exp = self._parse_unicode_superscript(exp_str)\n            out = base * (10**exp)\n            return out\n\n        # Match LaTeX scientific notation: \"1.3 \\times 10^{-5}\" or \"1.7 \u00d7 10^{-4}\"\n        latex_match = re.match(self.SCI_LATEX_EXTRACT, s)\n        if latex_match:\n            base = float(latex_match.group(1).replace(\"\u2212\", \"-\"))\n            exp = float(latex_match.group(2).replace(\"\u2212\", \"-\"))\n            out = base * (10**exp)\n            return out\n\n        # Match caret notation: \"1.3 \u00d7 10^-4\" or \"1.0 x 10^6\" or \"1.6 x 10^{-5}\"\n        caret_match = re.match(self.SCI_CARET_EXTRACT, s)\n        if caret_match:\n            base = float(caret_match.group(1).replace(\"\u2212\", \"-\"))\n            exp = float(caret_match.group(2).replace(\"\u2212\", \"-\"))\n            out = base * (10**exp)\n            return out\n\n        # Match \u00b1 pattern: \"-2.1 \u00b1 0.5\" -&gt; take central value (-2.1)\n        pm_match = re.match(self.PM_EXTRACT, s)\n        if pm_match:\n            return float(pm_match.group(1).replace(\"\u2212\", \"-\"))\n\n        # Standard float() handles \"1e-3\", \"1E-3\", \"1.5e-3\", etc.\n        return float(s.replace(\"\u2212\", \"-\"))\n\n    def _parse_unicode_superscript(self, s: str) -&gt; int:\n        \"\"\"Convert Unicode superscript characters to an integer.\n\n        Args:\n            s: String containing Unicode superscript characters (e.g., \"\u207b\u2076\").\n\n        Returns:\n            Integer value represented by the superscript.\n        \"\"\"\n        # Mapping of Unicode superscript characters to their values\n        superscript_map = {\n            \"\u2070\": \"0\",\n            \"\u00b9\": \"1\",\n            \"\u00b2\": \"2\",\n            \"\u00b3\": \"3\",\n            \"\u2074\": \"4\",\n            \"\u2075\": \"5\",\n            \"\u2076\": \"6\",\n            \"\u2077\": \"7\",\n            \"\u2078\": \"8\",\n            \"\u2079\": \"9\",\n            \"\u207a\": \"+\",\n            \"\u207b\": \"-\",\n        }\n        result = \"\".join(superscript_map.get(c, c) for c in s)\n        return int(result)\n\n    def _extract_regression_answer(\n        self, answer_text: str, property: str\n    ) -&gt; Optional[float]:\n        \"\"\"Extract a regression answer from text.\n\n        Handles various formats:\n        - Plain floats: \"1.5\", \"-2.0\"\n        - Scientific notation: \"1e-3\", \"1E-3\", \"1.5e-6\"\n        - LaTeX notation: \"1.3 \\times 10^{-5}\", \"1.7 \u00d7 10^{-4}\"\n        - Caret notation: \"1.3 \u00d7 10^-4\"\n        - Plus-minus notation: \"-2.1 \u00b1 0.5\" -&gt; returns central value (-2.1)\n        - Ranges with 'to': \"1.0 to 2.0\" -&gt; returns average (1.5)\n        - Ranges with '-': \"1.0 - 2.0\" -&gt; returns average (1.5)\n        - Ranges with 'between/and': \"between 1.0 and 2.0\" -&gt; returns average (1.5)\n        - Scientific notation with &lt;sup&gt;: \"1.0 x 10&lt;sup&gt;-6&lt;/sup&gt; to 5.0 x 10&lt;sup&gt;-6&lt;/sup&gt;\"\n\n        Numbers preceded or followed by a slash (e.g., \"0.02 g/100 mL\") are excluded.\n\n        Args:\n            answer_text: The text content from within &lt;answer&gt; tags.\n            property: The property name to look for in \"property = value\" patterns.\n\n        Returns:\n            Extracted float value, or None if extraction fails or is ambiguous.\n        \"\"\"\n        # If \"property = value\", \"property=value\" or \"property is value\" format, extract the value\n        prop_pattern = (\n            rf\"{re.escape(property.lower())}\\s*(?:=|is)\\s*({self.NUM_PATTERN})\"\n        )\n        prop_match = re.search(prop_pattern, answer_text.lower(), flags=re.IGNORECASE)\n        if prop_match:\n            return self._parse_float_with_sup(prop_match.group(1))\n\n        # Check for \"between X and Y\" pattern\n        between_regex = rf\"between\\s*({self.NUM_PATTERN})\\s*and\\s*({self.NUM_PATTERN})\"\n        between_matches = re.findall(between_regex, answer_text, re.IGNORECASE)\n        if len(between_matches) == 1:\n            start, end = between_matches[0]\n            return (\n                self._parse_float_with_sup(start) + self._parse_float_with_sup(end)\n            ) / 2\n\n        # Check for range patterns: \"{num} to {num}\" or \"{num} - {num}\"\n        range_regex = rf\"({self.NUM_PATTERN})(?:\\s+-\\s+|\\s+to\\s+)({self.NUM_PATTERN})\"\n        range_matches = re.findall(range_regex, answer_text)\n\n        if len(range_matches) &gt; 0 and all(\n            range_matches[0] == rm for rm in range_matches\n        ):\n            start, end = range_matches[0]\n            return (\n                self._parse_float_with_sup(start) + self._parse_float_with_sup(end)\n            ) / 2\n\n        # Otherwise extract all float numbers (including scientific notation)\n        # Use negative lookbehind and lookahead to exclude numbers with slashes\n        ys: List[float] = []\n        all_nums = re.findall(self.NUM_WITH_BOUNDARY_PATTERN, answer_text)\n        for num_str in all_nums:\n            try:\n                ys.append(self._parse_float_with_sup(num_str))\n            except ValueError:\n                continue\n\n        if len(ys) == 0:\n            return None\n        if len(ys) &gt; 1 and not all(y == ys[0] for y in ys):\n            return None  # Ambiguous: multiple different values\n\n        return ys[0]\n\n    def _extract_classification_answer(self, answer_text: str) -&gt; Optional[int]:\n        \"\"\"Extract a classification answer from text.\n\n        Handles various formats:\n        - Boolean strings: \"true\", \"yes\" -&gt; 1; \"false\", \"no\" -&gt; 0\n        - Numeric strings: \"1\", \"1.0\" -&gt; 1; \"0\", \"0.0\" -&gt; 0\n\n        Args:\n            answer_text: The text content from within &lt;answer&gt; tags.\n\n        Returns:\n            Extracted int value (0 or 1), or None if extraction fails or is ambiguous.\n        \"\"\"\n        ys: List[int] = []\n        split_answer = re.split(r\"\\n| |\\t|:|`|'|,\", answer_text)\n        for spl in split_answer:\n            if spl.lower().replace(\".\", \"\") in self.CLASSIFICATION_TRUE_KEYWORDS:\n                ys.append(1)\n            elif spl.lower().replace(\".\", \"\") in self.CLASSIFICATION_FALSE_KEYWORDS:\n                ys.append(0)\n\n        if len(ys) == 0:\n            return None\n        if len(ys) &gt; 1 and not all(y == ys[0] for y in ys):\n            return None  # Ambiguous: multiple different values\n\n        return ys[0]\n\n    def get_score(\n        self, inputs: BatchVerifiersInputModel\n    ) -&gt; List[MolPropVerifierOutputModel]:\n        \"\"\"Compute property prediction rewards for a batch of completions.\n\n        This method extracts predicted values from answer tags in completions\n        and computes rewards based on the objective type:\n        - Regression: reward = clip(1 - ((predicted - target) / std)^2, 0, 1)\n        - Classification: reward = 1.0 if predicted == target, else 0.0\n\n        Args:\n            inputs: Batch of completions and metadata for verification.\n\n        Returns:\n            List of MolPropVerifierOutputModel containing rewards and metadata.\n\n        Notes:\n            - Answers must be enclosed in &lt;answer&gt;&lt;/answer&gt; tags\n            - Supports \"true\"/\"yes\" as 1 and \"false\"/\"no\" as 0 for classification\n            - Invalid or missing answers result in 0.0 reward\n        \"\"\"\n        completions = inputs.completions\n        assert all(\n            isinstance(meta, MolPropVerifierInputMetadataModel)\n            for meta in inputs.metadatas\n        )\n        metadatas: List[MolPropVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n        verifier_metadatas: List[MolPropVerifierMetadataModel] = []\n        all_matches: List[str] = []\n        for answer, meta in zip(completions, metadatas):\n            match = self.parse_answer(answer)\n            all_matches.append(match)\n            self.logger.info(f\"Match: {match}\")\n            extracted: float | int | None = None\n            if match != \"\":\n                try:\n                    if meta.objectives[0] == \"classification\":\n                        extracted = self._extract_classification_answer(match)\n                        if extracted is None:\n                            self.logger.info(\n                                f\"Could not extract classification value from: {match}\"\n                            )\n                    elif meta.objectives[0] == \"regression\":\n                        extracted = self._extract_regression_answer(\n                            match, meta.properties[0]\n                        )\n                        if extracted is None:\n                            self.logger.info(\n                                f\"Could not extract regression value from: {match}\"\n                            )\n                    else:\n                        extracted = None\n                    verifier_metadatas.append(\n                        MolPropVerifierMetadataModel(\n                            extracted_answer=extracted,\n                            extraction_success=True,\n                        )\n                    )\n                except ValueError:\n                    verifier_metadatas.append(\n                        MolPropVerifierMetadataModel(\n                            extracted_answer=-10000.0,\n                            extraction_success=False,\n                        )\n                    )\n            else:\n                verifier_metadatas.append(\n                    MolPropVerifierMetadataModel(\n                        extracted_answer=-10000.0,\n                        extraction_success=False,\n                    )\n                )\n\n        if self.verifier_config.reward == \"valid_smiles\":\n            return [\n                MolPropVerifierOutputModel(\n                    reward=float(\n                        isinstance(verifier_meta.extracted_answer, (float, int))\n                    ),\n                    parsed_answer=m,\n                    verifier_metadata=verifier_meta,\n                )\n                for m, verifier_meta in zip(all_matches, verifier_metadatas)\n            ]\n\n        rewards = []\n        for meta, verifier_meta in zip(metadatas, verifier_metadatas):\n            if not verifier_meta.extraction_success:\n                rewards.append(0.0)\n            else:\n                if meta.objectives[0] == \"regression\":\n                    std = meta.norm_var if meta.norm_var is not None else 1.0\n                    rewards.append(\n                        np.clip(\n                            1\n                            - ((verifier_meta.extracted_answer - meta.target[0]) / std)\n                            ** 2,\n                            a_min=0.0,\n                            a_max=1.0,\n                        )\n                    )\n                elif meta.objectives[0] == \"classification\":\n                    rewards.append(\n                        float(verifier_meta.extracted_answer == meta.target[0])\n                    )\n                else:\n                    self.logger.error(f\"Not a valid objective: {meta.objectives[0]}\")\n                    raise NotImplementedError\n\n        self.logger.info(f\"Rewards: {rewards}\")\n        return [\n            MolPropVerifierOutputModel(\n                reward=reward, parsed_answer=m, verifier_metadata=verifier_metadata\n            )\n            for reward, m, verifier_metadata in zip(\n                rewards, all_matches, verifier_metadatas\n            )\n        ]\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier.MolPropVerifier.__init__","title":"<code>__init__(verifier_config)</code>","text":"<p>Initialize the MolPropVerifier.</p> <p>Parameters:</p> Name Type Description Default <code>verifier_config</code> <code>MolPropVerifierConfigModel</code> <p>Configuration containing reward type settings.</p> required Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier.py</code> <pre><code>def __init__(self, verifier_config: MolPropVerifierConfigModel) -&gt; None:\n    \"\"\"Initialize the MolPropVerifier.\n\n    Args:\n        verifier_config: Configuration containing reward type settings.\n    \"\"\"\n    super().__init__(verifier_config)\n    self.verifier_config: MolPropVerifierConfigModel = verifier_config\n    self.logger = logging.getLogger(\"MolPropVerifier\")\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#mol_gen_docking.reward.verifiers.mol_prop_reward.mol_prop_verifier.MolPropVerifier.get_score","title":"<code>get_score(inputs)</code>","text":"<p>Compute property prediction rewards for a batch of completions.</p> <p>This method extracts predicted values from answer tags in completions and computes rewards based on the objective type: - Regression: reward = clip(1 - ((predicted - target) / std)^2, 0, 1) - Classification: reward = 1.0 if predicted == target, else 0.0</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>BatchVerifiersInputModel</code> <p>Batch of completions and metadata for verification.</p> required <p>Returns:</p> Type Description <code>List[MolPropVerifierOutputModel]</code> <p>List of MolPropVerifierOutputModel containing rewards and metadata.</p> Notes <ul> <li>Answers must be enclosed in  tags</li> <li>Supports \"true\"/\"yes\" as 1 and \"false\"/\"no\" as 0 for classification</li> <li>Invalid or missing answers result in 0.0 reward</li> </ul> Source code in <code>mol_gen_docking/reward/verifiers/mol_prop_reward/mol_prop_verifier.py</code> <pre><code>def get_score(\n    self, inputs: BatchVerifiersInputModel\n) -&gt; List[MolPropVerifierOutputModel]:\n    \"\"\"Compute property prediction rewards for a batch of completions.\n\n    This method extracts predicted values from answer tags in completions\n    and computes rewards based on the objective type:\n    - Regression: reward = clip(1 - ((predicted - target) / std)^2, 0, 1)\n    - Classification: reward = 1.0 if predicted == target, else 0.0\n\n    Args:\n        inputs: Batch of completions and metadata for verification.\n\n    Returns:\n        List of MolPropVerifierOutputModel containing rewards and metadata.\n\n    Notes:\n        - Answers must be enclosed in &lt;answer&gt;&lt;/answer&gt; tags\n        - Supports \"true\"/\"yes\" as 1 and \"false\"/\"no\" as 0 for classification\n        - Invalid or missing answers result in 0.0 reward\n    \"\"\"\n    completions = inputs.completions\n    assert all(\n        isinstance(meta, MolPropVerifierInputMetadataModel)\n        for meta in inputs.metadatas\n    )\n    metadatas: List[MolPropVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n    verifier_metadatas: List[MolPropVerifierMetadataModel] = []\n    all_matches: List[str] = []\n    for answer, meta in zip(completions, metadatas):\n        match = self.parse_answer(answer)\n        all_matches.append(match)\n        self.logger.info(f\"Match: {match}\")\n        extracted: float | int | None = None\n        if match != \"\":\n            try:\n                if meta.objectives[0] == \"classification\":\n                    extracted = self._extract_classification_answer(match)\n                    if extracted is None:\n                        self.logger.info(\n                            f\"Could not extract classification value from: {match}\"\n                        )\n                elif meta.objectives[0] == \"regression\":\n                    extracted = self._extract_regression_answer(\n                        match, meta.properties[0]\n                    )\n                    if extracted is None:\n                        self.logger.info(\n                            f\"Could not extract regression value from: {match}\"\n                        )\n                else:\n                    extracted = None\n                verifier_metadatas.append(\n                    MolPropVerifierMetadataModel(\n                        extracted_answer=extracted,\n                        extraction_success=True,\n                    )\n                )\n            except ValueError:\n                verifier_metadatas.append(\n                    MolPropVerifierMetadataModel(\n                        extracted_answer=-10000.0,\n                        extraction_success=False,\n                    )\n                )\n        else:\n            verifier_metadatas.append(\n                MolPropVerifierMetadataModel(\n                    extracted_answer=-10000.0,\n                    extraction_success=False,\n                )\n            )\n\n    if self.verifier_config.reward == \"valid_smiles\":\n        return [\n            MolPropVerifierOutputModel(\n                reward=float(\n                    isinstance(verifier_meta.extracted_answer, (float, int))\n                ),\n                parsed_answer=m,\n                verifier_metadata=verifier_meta,\n            )\n            for m, verifier_meta in zip(all_matches, verifier_metadatas)\n        ]\n\n    rewards = []\n    for meta, verifier_meta in zip(metadatas, verifier_metadatas):\n        if not verifier_meta.extraction_success:\n            rewards.append(0.0)\n        else:\n            if meta.objectives[0] == \"regression\":\n                std = meta.norm_var if meta.norm_var is not None else 1.0\n                rewards.append(\n                    np.clip(\n                        1\n                        - ((verifier_meta.extracted_answer - meta.target[0]) / std)\n                        ** 2,\n                        a_min=0.0,\n                        a_max=1.0,\n                    )\n                )\n            elif meta.objectives[0] == \"classification\":\n                rewards.append(\n                    float(verifier_meta.extracted_answer == meta.target[0])\n                )\n            else:\n                self.logger.error(f\"Not a valid objective: {meta.objectives[0]}\")\n                raise NotImplementedError\n\n    self.logger.info(f\"Rewards: {rewards}\")\n    return [\n        MolPropVerifierOutputModel(\n            reward=reward, parsed_answer=m, verifier_metadata=verifier_metadata\n        )\n        for reward, m, verifier_metadata in zip(\n            rewards, all_matches, verifier_metadatas\n        )\n    ]\n</code></pre>"},{"location":"api/reward/mol_prop_verifier/#related","title":"Related","text":"<ul> <li>Molecular Verifier - Main orchestrator</li> <li>Generation Verifier - De novo generation tasks</li> <li>Reaction Verifier - Reaction prediction and retro-synthesis tasks</li> </ul>"},{"location":"api/reward/molecular_verifier/","title":"Molecular Verifier","text":"<p>The <code>MolecularVerifier</code> is the main orchestrator for reward computation across all molecular tasks. It automatically routes completions to the appropriate task-specific verifier based on metadata.</p>"},{"location":"api/reward/molecular_verifier/#overview","title":"Overview","text":"<p>The MolecularVerifier provides:</p> <ul> <li>Unified Interface: Single entry point for all reward computations</li> <li>Automatic Routing: Routes completions to the correct verifier based on metadata</li> <li>Parallel Processing: Uses Ray for efficient batch processing</li> <li>GPU Support: Supports GPU-accelerated molecular docking</li> </ul>"},{"location":"api/reward/molecular_verifier/#architecture","title":"Architecture","text":"<pre><code>MolecularVerifier\n\u251c\u2500\u2500 GenerationVerifier  \u2192 De novo molecular generation\n\u251c\u2500\u2500 MolPropVerifier     \u2192 Property prediction (regression/classification)\n\u2514\u2500\u2500 ReactionVerifier    \u2192 Chemical reactions &amp; retro-synthesis\n</code></pre>"},{"location":"api/reward/molecular_verifier/#usage","title":"Usage","text":""},{"location":"api/reward/molecular_verifier/#basic-example","title":"Basic Example","text":"<pre><code>from mol_gen_docking.reward import (\n    MolecularVerifier,\n    MolecularVerifierConfigModel,\n)\nfrom mol_gen_docking.reward.verifiers import (\n    GenerationVerifierConfigModel,\n    ReactionVerifierConfigModel,\n    MolPropVerifierConfigModel\n)\n\n# Configure the verifier\nconfig = MolecularVerifierConfigModel(\n    generation_verifier_config=GenerationVerifierConfigModel(\n        path_to_mappings=\"data/molgendata\",\n        reward=\"property\"\n    ),\n    mol_prop_verifier_config=MolPropVerifierConfigModel(),\n    reaction_verifier_config=ReactionVerifierConfigModel()\n)\n\n# Create verifier\nverifier = MolecularVerifier(config)\n\n# Compute rewards for generation task\ncompletions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\"]\nmetadata = [{\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}]\n\nresults = verifier(completions, metadata)\nprint(f\"Rewards: {results.rewards}\")\n# Output:\n# &gt;&gt;&gt; Rewards: [0.1127273579103326]\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mixed-batch-processing","title":"Mixed Batch Processing","text":"<p>The verifier can handle batches with different task types:</p> <pre><code>completions = [\n    \"&lt;answer&gt;CCO&lt;/answer&gt;\",\n    \"&lt;answer&gt;0.75&lt;/answer&gt;\",\n    \"&lt;answer&gt;CCN(CC)C(N)=S + NNC(=O)Cc1ccc(F)cc1 -&gt; C[CH]N(CC)c1nnc(Cc2ccc(F)cc2)[nH]1&lt;/answer&gt;\"\n]\n\nmetadata = [\n    {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n    {\"objectives\": [\"regression\"], \"target\": [0.8], \"norm_var\": 0.1},\n    {\"objectives\": [\"full_path\"], \"target\": [\"C[CH]N(CC)c1nnc(Cc2ccc(F)cc2)[nH]1\"]}\n]\n\nresults = verifier(completions, metadata)\n\nprint(results.rewards)\n# Output:\n# &gt;&gt;&gt; Rewards: [0.1127273579103326, 0.7499999999999996, 1.0]\n</code></pre> <p>Molecular verifier module for computing rewards across different task types.</p> <p>This module provides the MolecularVerifier class which orchestrates reward computation for molecular generation, property prediction, and chemical reaction tasks. It uses Ray for parallel processing and supports GPU-accelerated docking calculations.</p>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier_pydantic_model.MolecularVerifierConfigModel","title":"<code>MolecularVerifierConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for molecular verifier configuration.</p> <p>This model defines the configuration parameters for the MolecularVerifier class, providing validation and documentation for all configuration options.</p> <p>The reward field is automatically propagated to all sub-verifier configurations, ensuring consistent reward type usage across all verifiers.</p> <p>Attributes:</p> Name Type Description <code>parsing_method</code> <code>Literal['none', 'answer_tags', 'boxed']</code> <p>Method to parse model completions:</p> <ul> <li>\"none\": No parsing, use full completion (not recommended, risks of ambiguity in the answer extraction)</li> <li>\"answer_tags\": Extract content within ... tags</li> <li>\"boxed\": Extract content within answer tags and boxed in the '\boxed{...}' LaTeX command</li> </ul> <code>reward</code> <code>Literal['valid_smiles', 'property']</code> <p>Type of reward to compute. Affects all sub-verifiers:</p> <ul> <li>\"property\": Computes property-based rewards (docking scores, molecular properties)</li> <li>\"valid_smiles\": Computes validity rewards (1.0 for valid single SMILES, 0.0 otherwise)</li> </ul> <code>generation_verifier_config</code> <code>Optional[GenerationVerifierConfigModel]</code> <p>Configuration for de novo molecular generation tasks. Required if handling generation tasks. Contains settings for molecular property optimization, docking oracle configuration, and SMILES extraction.</p> <code>mol_prop_verifier_config</code> <code>Optional[MolPropVerifierConfigModel]</code> <p>Configuration for molecular property prediction tasks. Required if handling property prediction tasks. Contains settings for regression and classification objective handling.</p> <code>reaction_verifier_config</code> <code>Optional[ReactionVerifierConfigModel]</code> <p>Configuration for chemical reaction and retro-synthesis tasks. Required if handling reaction tasks. Contains settings for synthesis path validation and reaction SMARTS matching.</p> Notes <ul> <li>At least one sub-verifier configuration should be provided for the verifier to work</li> <li>The propagate_reward_to_subconfigs validator automatically sets the reward type   and parsing_method flag in all sub-verifier configurations</li> <li>Invalid configurations will raise Pydantic ValidationError with detailed messages</li> </ul> Example <pre><code>from mol_gen_docking.reward import MolecularVerifierConfigModel\nfrom mol_gen_docking.reward.verifiers import (\n    GenerationVerifierConfigModel,\n    MolPropVerifierConfigModel,\n    ReactionVerifierConfigModel\n)\n\n# Minimal configuration with generation verifier\nconfig = MolecularVerifierConfigModel(\n    generation_verifier_config=GenerationVerifierConfigModel(\n        path_to_mappings=\"data/molgendata\"\n    )\n)\n\n# Full configuration with all verifiers\nconfig = MolecularVerifierConfigModel(\n    parsing_method=\"answer_tags\",\n    reward=\"property\",\n    generation_verifier_config=GenerationVerifierConfigModel(\n        path_to_mappings=\"data/molgendata\",\n        rescale=True,\n        oracle_kwargs={\n            \"exhaustiveness\": 8,\n            \"n_cpu\": 8,\n            \"docking_oracle\": \"autodock_gpu\",\n            \"vina_mode\": \"autodock_gpu_256wi\"\n        },\n        docking_concurrency_per_gpu=2\n    ),\n    mol_prop_verifier_config=MolPropVerifierConfigModel(\n        reward=\"property\"\n    ),\n    reaction_verifier_config=ReactionVerifierConfigModel(\n        reaction_matrix_path=\"data/rxn_matrix.pkl\",\n        reaction_reward_type=\"tanimoto\"\n    )\n)\n</code></pre> See Also <ul> <li>GenerationVerifierConfigModel: Configuration for generation tasks</li> <li>MolPropVerifierConfigModel: Configuration for property prediction tasks</li> <li>ReactionVerifierConfigModel: Configuration for reaction tasks</li> <li>MolecularVerifier: Main orchestrator class using this config</li> </ul> Source code in <code>mol_gen_docking/reward/molecular_verifier_pydantic_model.py</code> <pre><code>class MolecularVerifierConfigModel(BaseModel):\n    \"\"\"Pydantic model for molecular verifier configuration.\n\n    This model defines the configuration parameters for the MolecularVerifier class,\n    providing validation and documentation for all configuration options.\n\n    The reward field is automatically propagated to all sub-verifier configurations,\n    ensuring consistent reward type usage across all verifiers.\n\n    Attributes:\n        parsing_method: Method to parse model completions:\n\n            - \"none\": No parsing, use full completion (not recommended, risks of ambiguity in the answer extraction)\n            - \"answer_tags\": Extract content within &lt;answer&gt;...&lt;/answer&gt; tags\n            - \"boxed\": Extract content within answer tags and boxed in the '\\boxed{...}' LaTeX command\n        reward: Type of reward to compute. Affects all sub-verifiers:\n\n            - \"property\": Computes property-based rewards (docking scores, molecular properties)\n            - \"valid_smiles\": Computes validity rewards (1.0 for valid single SMILES, 0.0 otherwise)\n        generation_verifier_config: Configuration for de novo molecular generation tasks.\n            Required if handling generation tasks. Contains settings for molecular property\n            optimization, docking oracle configuration, and SMILES extraction.\n        mol_prop_verifier_config: Configuration for molecular property prediction tasks.\n            Required if handling property prediction tasks. Contains settings for regression\n            and classification objective handling.\n        reaction_verifier_config: Configuration for chemical reaction and retro-synthesis tasks.\n            Required if handling reaction tasks. Contains settings for synthesis path validation\n            and reaction SMARTS matching.\n\n    Notes:\n        - At least one sub-verifier configuration should be provided for the verifier to work\n        - The propagate_reward_to_subconfigs validator automatically sets the reward type\n          and parsing_method flag in all sub-verifier configurations\n        - Invalid configurations will raise Pydantic ValidationError with detailed messages\n\n    Example:\n        ```python\n        from mol_gen_docking.reward import MolecularVerifierConfigModel\n        from mol_gen_docking.reward.verifiers import (\n            GenerationVerifierConfigModel,\n            MolPropVerifierConfigModel,\n            ReactionVerifierConfigModel\n        )\n\n        # Minimal configuration with generation verifier\n        config = MolecularVerifierConfigModel(\n            generation_verifier_config=GenerationVerifierConfigModel(\n                path_to_mappings=\"data/molgendata\"\n            )\n        )\n\n        # Full configuration with all verifiers\n        config = MolecularVerifierConfigModel(\n            parsing_method=\"answer_tags\",\n            reward=\"property\",\n            generation_verifier_config=GenerationVerifierConfigModel(\n                path_to_mappings=\"data/molgendata\",\n                rescale=True,\n                oracle_kwargs={\n                    \"exhaustiveness\": 8,\n                    \"n_cpu\": 8,\n                    \"docking_oracle\": \"autodock_gpu\",\n                    \"vina_mode\": \"autodock_gpu_256wi\"\n                },\n                docking_concurrency_per_gpu=2\n            ),\n            mol_prop_verifier_config=MolPropVerifierConfigModel(\n                reward=\"property\"\n            ),\n            reaction_verifier_config=ReactionVerifierConfigModel(\n                reaction_matrix_path=\"data/rxn_matrix.pkl\",\n                reaction_reward_type=\"tanimoto\"\n            )\n        )\n        ```\n\n    See Also:\n        - GenerationVerifierConfigModel: Configuration for generation tasks\n        - MolPropVerifierConfigModel: Configuration for property prediction tasks\n        - ReactionVerifierConfigModel: Configuration for reaction tasks\n        - MolecularVerifier: Main orchestrator class using this config\n    \"\"\"\n\n    parsing_method: Literal[\"none\", \"answer_tags\", \"boxed\"] = Field(\n        default=\"boxed\",\n        description=\"Method to parse model completions for SMILES or property values.\",\n    )\n    reward: Literal[\"valid_smiles\", \"property\"] = Field(\n        default=\"property\",\n        description=\"Type of reward to use for molecular verification.\",\n    )\n    generation_verifier_config: Optional[GenerationVerifierConfigModel] = Field(\n        None,\n        description=\"Configuration for generation verifier, required if reward is 'valid_smiles'.\",\n    )\n    mol_prop_verifier_config: Optional[MolPropVerifierConfigModel] = Field(\n        None,\n        description=\"Configuration for molecular property verifier, required if reward is 'property'.\",\n    )\n    reaction_verifier_config: Optional[ReactionVerifierConfigModel] = Field(\n        None,\n        description=\"Configuration for reaction verifier, required if reward is 'reaction'.\",\n    )\n\n    @model_validator(mode=\"after\")\n    def propagate_fields_to_subconfigs(self) -&gt; \"MolecularVerifierConfigModel\":\n        \"\"\"Propagate the fields shared to all sub-verifier configurations.\"\"\"\n        if self.generation_verifier_config is not None:\n            self.generation_verifier_config.reward = self.reward\n            self.generation_verifier_config.parsing_method = self.parsing_method\n        if self.mol_prop_verifier_config is not None:\n            self.mol_prop_verifier_config.parsing_method = self.parsing_method\n            self.mol_prop_verifier_config.reward = self.reward\n        if self.reaction_verifier_config is not None:\n            ### FOR REACTION VERIFIER, SPECIAL BEHAVIOR:\n            ###     - if parsing_method is \"none\", set it to \"none\" for reaction verifier\n            ###     - else, set it to \"answer_tags\" for reaction verifier (boxed not supported)\n            if self.parsing_method == \"none\":\n                self.reaction_verifier_config.parsing_method = \"none\"\n            else:\n                self.reaction_verifier_config.parsing_method = \"answer_tags\"\n            self.reaction_verifier_config.reward = self.reward\n        return self\n\n    class Config:\n        \"\"\"Pydantic configuration for the MolecularVerifierConfigModel.\"\"\"\n\n        arbitrary_types_allowed = True\n        json_schema_extra = {\n            \"example\": {\n                \"parsing_method\": \"answer_tags\",\n                \"reward\": \"property\",\n                \"generation_verifier_config\": {\n                    \"path_to_mappings\": \"data/molgendata\",\n                    \"rescale\": True,\n                    \"oracle_kwargs\": {\n                        \"exhaustiveness\": 8,\n                        \"n_cpu\": 8,\n                        \"docking_oracle\": \"autodock_gpu\",\n                        \"vina_mode\": \"autodock_gpu_256wi\",\n                    },\n                    \"docking_concurrency_per_gpu\": 2,\n                },\n                \"mol_prop_verifier_config\": {},\n                \"reaction_verifier_config\": {\n                    \"reaction_matrix_path\": \"data/rxn_matrix.pkl\",\n                    \"reaction_reward_type\": \"tanimoto\",\n                },\n            }\n        }\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier_pydantic_model.MolecularVerifierConfigModel.Config","title":"<code>Config</code>","text":"<p>Pydantic configuration for the MolecularVerifierConfigModel.</p> Source code in <code>mol_gen_docking/reward/molecular_verifier_pydantic_model.py</code> <pre><code>class Config:\n    \"\"\"Pydantic configuration for the MolecularVerifierConfigModel.\"\"\"\n\n    arbitrary_types_allowed = True\n    json_schema_extra = {\n        \"example\": {\n            \"parsing_method\": \"answer_tags\",\n            \"reward\": \"property\",\n            \"generation_verifier_config\": {\n                \"path_to_mappings\": \"data/molgendata\",\n                \"rescale\": True,\n                \"oracle_kwargs\": {\n                    \"exhaustiveness\": 8,\n                    \"n_cpu\": 8,\n                    \"docking_oracle\": \"autodock_gpu\",\n                    \"vina_mode\": \"autodock_gpu_256wi\",\n                },\n                \"docking_concurrency_per_gpu\": 2,\n            },\n            \"mol_prop_verifier_config\": {},\n            \"reaction_verifier_config\": {\n                \"reaction_matrix_path\": \"data/rxn_matrix.pkl\",\n                \"reaction_reward_type\": \"tanimoto\",\n            },\n        }\n    }\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier_pydantic_model.MolecularVerifierConfigModel.propagate_fields_to_subconfigs","title":"<code>propagate_fields_to_subconfigs()</code>","text":"<p>Propagate the fields shared to all sub-verifier configurations.</p> Source code in <code>mol_gen_docking/reward/molecular_verifier_pydantic_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef propagate_fields_to_subconfigs(self) -&gt; \"MolecularVerifierConfigModel\":\n    \"\"\"Propagate the fields shared to all sub-verifier configurations.\"\"\"\n    if self.generation_verifier_config is not None:\n        self.generation_verifier_config.reward = self.reward\n        self.generation_verifier_config.parsing_method = self.parsing_method\n    if self.mol_prop_verifier_config is not None:\n        self.mol_prop_verifier_config.parsing_method = self.parsing_method\n        self.mol_prop_verifier_config.reward = self.reward\n    if self.reaction_verifier_config is not None:\n        ### FOR REACTION VERIFIER, SPECIAL BEHAVIOR:\n        ###     - if parsing_method is \"none\", set it to \"none\" for reaction verifier\n        ###     - else, set it to \"answer_tags\" for reaction verifier (boxed not supported)\n        if self.parsing_method == \"none\":\n            self.reaction_verifier_config.parsing_method = \"none\"\n        else:\n            self.reaction_verifier_config.parsing_method = \"answer_tags\"\n        self.reaction_verifier_config.reward = self.reward\n    return self\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier_pydantic_model.BatchMolecularVerifierOutputModel","title":"<code>BatchMolecularVerifierOutputModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Output model for molecular verifier batch results.</p> <p>This model encapsulates the results from batch verification across multiple tasks and completions. It provides both the computed rewards and detailed metadata about each verification process, enabling comprehensive analysis of model performance.</p> <p>The output is structured as parallel lists where each index corresponds to a single completion from the input batch, allowing easy correlation between inputs and outputs.</p> <p>Attributes:</p> Name Type Description <code>rewards</code> <code>list[float]</code> <p>List of computed reward scores, one per input completion. Each reward is a float value typically in the range [0.0, 1.0], though values may exceed 1.0 for maximize objectives depending on the input values. The reward value depends on the task type and objective:</p> <ul> <li>Generation tasks: Geometric mean of per-property rewards</li> <li>Property prediction: 0-1 for classification, 0-1 (clipped) for regression</li> <li>Reaction tasks: 0-1 for ground truth, 0-1 for path validation</li> </ul> <code>parsed_answer</code> <code>list[float]</code> <p>The parsed answer extracted from the model completion.</p> <code>verifier_metadatas</code> <code>list[MolecularVerifierOutputMetadataModel]</code> <p>List of metadata objects from each verification process. Each element corresponds to the reward at the same index. The metadata type depends on which verifier processed the completion:</p> <ul> <li>GenerationVerifierMetadataModel: For generation tasks (SMILES extraction, properties)</li> <li>MolPropVerifierMetadataModel: For property prediction (extracted values)</li> <li>ReactionVerifierMetadataModel: For reaction tasks (validation results)</li> </ul> Notes <ul> <li>The length of rewards and verifier_metadatas must always match the number   of input completions</li> <li>The order of outputs corresponds to the order of inputs</li> <li>Metadata provides detailed diagnostic information about failures (extraction errors,   invalid molecules, validation failures, etc.)</li> <li>All numeric reward values are finite (not NaN or inf)</li> </ul> Example <pre><code>from mol_gen_docking.reward import MolecularVerifier, MolecularVerifierConfigModel\nfrom mol_gen_docking.reward.verifiers import GenerationVerifierConfigModel\n\nconfig = MolecularVerifierConfigModel(\n    generation_verifier_config=GenerationVerifierConfigModel(\n        path_to_mappings=\"data/molgendata\"\n    )\n)\nverifier = MolecularVerifier(config)\n\n# Verify multiple completions\ncompletions = [\n    \"&lt;answer&gt;CCO&lt;/answer&gt;\",\n    \"&lt;answer&gt;c1ccccc1&lt;/answer&gt;\",\n    \"&lt;answer&gt;invalid_smiles&lt;/answer&gt;\"\n]\nmetadata = [\n    {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n    {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n    {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}\n]\n\nresults = verifier(completions, metadata)\n\n# Access results\nprint(f\"Number of completions: {len(results.rewards)}\")\nfor i, (reward, meta) in enumerate(zip(results.rewards, results.verifier_metadatas)):\n    print(f\"Completion {i}: reward={reward:.3f}\")\n    print(f\"  Properties: {meta.properties}\")\n    print(f\"  All SMILES: {meta.all_smi}\")\n    if meta.smiles_extraction_failure:\n        print(f\"  Failure reason: {meta.smiles_extraction_failure}\")\n\n#  Output:\n# &gt;&gt;&gt; Number of completions: 3\n# Completion 0: reward=0.113\n#   Properties: ['QED']\n#   All SMILES: ['CCO']\n# Completion 1: reward=0.173\n#   Properties: ['QED']\n#   All SMILES: ['c1ccccc1']\n# Completion 2: reward=0.000\n#   Properties: []\n#   All SMILES: []\n#   Failure reason: no_smiles\n</code></pre> See Also <ul> <li>MolecularVerifier: Main class that returns this model</li> <li>GenerationVerifierMetadataModel: Metadata for generation tasks</li> <li>MolPropVerifierMetadataModel: Metadata for property prediction tasks</li> <li>ReactionVerifierMetadataModel: Metadata for reaction tasks</li> </ul> Source code in <code>mol_gen_docking/reward/molecular_verifier_pydantic_model.py</code> <pre><code>class BatchMolecularVerifierOutputModel(BaseModel):\n    \"\"\"Output model for molecular verifier batch results.\n\n    This model encapsulates the results from batch verification across multiple tasks\n    and completions. It provides both the computed rewards and detailed metadata about\n    each verification process, enabling comprehensive analysis of model performance.\n\n    The output is structured as parallel lists where each index corresponds to a single\n    completion from the input batch, allowing easy correlation between inputs and outputs.\n\n    Attributes:\n        rewards: List of computed reward scores, one per input completion.\n            Each reward is a float value typically in the range [0.0, 1.0], though values\n            may exceed 1.0 for maximize objectives depending on the input values.\n            The reward value depends on the task type and objective:\n\n            - Generation tasks: Geometric mean of per-property rewards\n            - Property prediction: 0-1 for classification, 0-1 (clipped) for regression\n            - Reaction tasks: 0-1 for ground truth, 0-1 for path validation\n        parsed_answer: The parsed answer extracted from the model completion.\n        verifier_metadatas: List of metadata objects from each verification process.\n            Each element corresponds to the reward at the same index. The metadata type\n            depends on which verifier processed the completion:\n\n            - GenerationVerifierMetadataModel: For generation tasks (SMILES extraction, properties)\n            - MolPropVerifierMetadataModel: For property prediction (extracted values)\n            - ReactionVerifierMetadataModel: For reaction tasks (validation results)\n\n    Notes:\n        - The length of rewards and verifier_metadatas must always match the number\n          of input completions\n        - The order of outputs corresponds to the order of inputs\n        - Metadata provides detailed diagnostic information about failures (extraction errors,\n          invalid molecules, validation failures, etc.)\n        - All numeric reward values are finite (not NaN or inf)\n\n    Example:\n        ```python\n        from mol_gen_docking.reward import MolecularVerifier, MolecularVerifierConfigModel\n        from mol_gen_docking.reward.verifiers import GenerationVerifierConfigModel\n\n        config = MolecularVerifierConfigModel(\n            generation_verifier_config=GenerationVerifierConfigModel(\n                path_to_mappings=\"data/molgendata\"\n            )\n        )\n        verifier = MolecularVerifier(config)\n\n        # Verify multiple completions\n        completions = [\n            \"&lt;answer&gt;CCO&lt;/answer&gt;\",\n            \"&lt;answer&gt;c1ccccc1&lt;/answer&gt;\",\n            \"&lt;answer&gt;invalid_smiles&lt;/answer&gt;\"\n        ]\n        metadata = [\n            {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n            {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n            {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}\n        ]\n\n        results = verifier(completions, metadata)\n\n        # Access results\n        print(f\"Number of completions: {len(results.rewards)}\")\n        for i, (reward, meta) in enumerate(zip(results.rewards, results.verifier_metadatas)):\n            print(f\"Completion {i}: reward={reward:.3f}\")\n            print(f\"  Properties: {meta.properties}\")\n            print(f\"  All SMILES: {meta.all_smi}\")\n            if meta.smiles_extraction_failure:\n                print(f\"  Failure reason: {meta.smiles_extraction_failure}\")\n\n        #  Output:\n        # &gt;&gt;&gt; Number of completions: 3\n        # Completion 0: reward=0.113\n        #   Properties: ['QED']\n        #   All SMILES: ['CCO']\n        # Completion 1: reward=0.173\n        #   Properties: ['QED']\n        #   All SMILES: ['c1ccccc1']\n        # Completion 2: reward=0.000\n        #   Properties: []\n        #   All SMILES: []\n        #   Failure reason: no_smiles\n        ```\n\n\n    See Also:\n        - MolecularVerifier: Main class that returns this model\n        - GenerationVerifierMetadataModel: Metadata for generation tasks\n        - MolPropVerifierMetadataModel: Metadata for property prediction tasks\n        - ReactionVerifierMetadataModel: Metadata for reaction tasks\n    \"\"\"\n\n    rewards: list[float] = Field(\n        ...,\n        description=\"List of computed rewards for the molecular verification.\",\n    )\n    parsed_answers: list[str] = Field(\n        ..., description=\"The parsed answer extracted from the model completion.\"\n    )\n    verifier_metadatas: list[MolecularVerifierOutputMetadataModel] = Field(\n        ...,\n        description=\"List of metadata from each verifier used in the molecular verification.\",\n    )\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier","title":"<code>MolecularVerifier</code>","text":"<p>Main orchestrator for molecular verification and reward computation.</p> <p>This class provides a unified interface for computing rewards across different molecular tasks: de novo generation, property prediction, and chemical reactions. It automatically routes inputs to the appropriate verifier based on metadata.</p> <p>The verifier uses Ray for parallel processing and supports GPU-accelerated molecular docking calculations when configured.</p> <p>Attributes:</p> Name Type Description <code>verifier_config</code> <p>Configuration model containing settings for all verifiers.</p> <code>logger</code> <p>Logger instance for the verifier.</p> Example <pre><code>from mol_gen_docking.reward import MolecularVerifier, MolecularVerifierConfigModel\nfrom mol_gen_docking.reward.verifiers import GenerationVerifierConfigModel\n\nconfig = MolecularVerifierConfigModel(\n    generation_verifier_config=GenerationVerifierConfigModel(\n        path_to_mappings=\"data/molgendata\",\n        reward=\"property\"\n    )\n)\nverifier = MolecularVerifier(config)\n\n# Compute rewards\ncompletions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\", \"&lt;answer&gt;c1ccccc1&lt;/answer&gt;\"]\nmetadata = [\n    {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n    {\"properties\": [\"SA\"], \"objectives\": [\"minimize\"], \"target\": [0.0]}\n]\nresults = verifier(completions, metadata)\nprint(f\"Rewards: {results.rewards}\")\n</code></pre> Source code in <code>mol_gen_docking/reward/molecular_verifier.py</code> <pre><code>class MolecularVerifier:\n    \"\"\"Main orchestrator for molecular verification and reward computation.\n\n    This class provides a unified interface for computing rewards across different\n    molecular tasks: de novo generation, property prediction, and chemical reactions.\n    It automatically routes inputs to the appropriate verifier based on metadata.\n\n    The verifier uses Ray for parallel processing and supports GPU-accelerated\n    molecular docking calculations when configured.\n\n    Attributes:\n        verifier_config: Configuration model containing settings for all verifiers.\n        logger: Logger instance for the verifier.\n\n    Example:\n        ```python\n        from mol_gen_docking.reward import MolecularVerifier, MolecularVerifierConfigModel\n        from mol_gen_docking.reward.verifiers import GenerationVerifierConfigModel\n\n        config = MolecularVerifierConfigModel(\n            generation_verifier_config=GenerationVerifierConfigModel(\n                path_to_mappings=\"data/molgendata\",\n                reward=\"property\"\n            )\n        )\n        verifier = MolecularVerifier(config)\n\n        # Compute rewards\n        completions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\", \"&lt;answer&gt;c1ccccc1&lt;/answer&gt;\"]\n        metadata = [\n            {\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]},\n            {\"properties\": [\"SA\"], \"objectives\": [\"minimize\"], \"target\": [0.0]}\n        ]\n        results = verifier(completions, metadata)\n        print(f\"Rewards: {results.rewards}\")\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        verifier_config: MolecularVerifierConfigModel,\n    ):\n        \"\"\"Initialize the MolecularVerifier.\n\n        Args:\n            verifier_config: Configuration model containing settings for all\n                sub-verifiers (generation, property prediction, reaction).\n        \"\"\"\n        self.verifier_config = verifier_config\n        self.__name__ = \"RewardScorer/MolecularVerifier\"\n        self.remote_tqdm = ray.remote(tqdm_ray.tqdm)\n\n        self._generation_verifier: None | GenerationVerifier = None\n        self._mol_prop_verifier: None | MolPropVerifier = None\n        self._reaction_verifier: None | ReactionVerifier = None\n        self.logger = logging.getLogger(\"RewardScorer\")\n\n        if not ray.is_initialized():\n            ray.init()\n\n    @property\n    def generation_verifier(self) -&gt; GenerationVerifier:\n        \"\"\"Lazy-loaded generation verifier instance.\n\n        Returns:\n            GenerationVerifier: The generation verifier for de novo molecular generation tasks.\n\n        Raises:\n            AssertionError: If generation_verifier_config is not set in the config.\n        \"\"\"\n        if self._generation_verifier is not None:\n            return self._generation_verifier\n        assert self.verifier_config.generation_verifier_config is not None\n        self._generation_verifier = GenerationVerifier(\n            verifier_config=self.verifier_config.generation_verifier_config\n        )\n        return self._generation_verifier\n\n    @property\n    def mol_prop_verifier(self) -&gt; MolPropVerifier:\n        \"\"\"Lazy-loaded molecular property verifier instance.\n\n        Returns:\n            MolPropVerifier: The verifier for molecular property prediction tasks.\n\n        Raises:\n            AssertionError: If mol_prop_verifier_config is not set in the config.\n        \"\"\"\n        if self._mol_prop_verifier is not None:\n            return self._mol_prop_verifier\n        assert self.verifier_config.mol_prop_verifier_config is not None\n        self._mol_prop_verifier = MolPropVerifier(\n            verifier_config=self.verifier_config.mol_prop_verifier_config\n        )\n        return self._mol_prop_verifier\n\n    @property\n    def reaction_verifier(self) -&gt; ReactionVerifier:\n        \"\"\"Lazy-loaded reaction verifier instance.\n\n        Returns:\n            ReactionVerifier: The verifier for chemical reaction and retro-synthesis tasks.\n\n        Raises:\n            AssertionError: If reaction_verifier_config is not set in the config.\n        \"\"\"\n        if self._reaction_verifier is not None:\n            return self._reaction_verifier\n        assert self.verifier_config.reaction_verifier_config is not None\n        self._reaction_verifier = ReactionVerifier(\n            verifier_config=self.verifier_config.reaction_verifier_config\n        )\n\n        return self._reaction_verifier\n\n    def _get_generation_score(\n        self,\n        inputs: BatchVerifiersInputModel,\n        debug: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; List[GenerationVerifierOutputModel]:\n        \"\"\"Compute rewards for molecular generation tasks.\n\n        Args:\n            inputs: Batch of completions and metadata for generation verification.\n            debug: If True, enables debug mode with additional logging.\n            use_pbar: If True, displays a progress bar during computation.\n\n        Returns:\n            List of GenerationVerifierOutputModel containing rewards and metadata.\n        \"\"\"\n        if debug:  # Testing purposes\n            self.generation_verifier.debug = True\n        elif self._generation_verifier is not None:\n            self.generation_verifier.debug = False\n        return self.generation_verifier.get_score(inputs)\n\n    def _get_prop_pred_score(\n        self,\n        inputs: BatchVerifiersInputModel,\n        debug: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; List[MolPropVerifierOutputModel]:\n        \"\"\"Compute rewards for molecular property prediction tasks.\n\n        Args:\n            inputs: Batch of completions and metadata for property verification.\n            debug: If True, enables debug mode with additional logging.\n            use_pbar: If True, displays a progress bar during computation.\n\n        Returns:\n            List of MolPropVerifierOutputModel containing rewards and metadata.\n        \"\"\"\n        return self.mol_prop_verifier.get_score(inputs)\n\n    def _get_reaction_score(\n        self,\n        inputs: BatchVerifiersInputModel,\n        debug: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; List[ReactionVerifierOutputModel]:\n        \"\"\"Compute rewards for chemical reaction tasks.\n\n        Args:\n            inputs: Batch of completions and metadata for reaction verification.\n            debug: If True, enables debug mode with additional logging.\n            use_pbar: If True, displays a progress bar during computation.\n\n        Returns:\n            List of ReactionVerifierOutputModel containing rewards and metadata.\n        \"\"\"\n        return self.reaction_verifier.get_score(inputs)\n\n    def get_score(\n        self,\n        completions: List[Any],\n        metadata: List[Dict[str, Any]],\n        debug: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; BatchMolecularVerifierOutputModel:\n        \"\"\"Compute rewards for a batch of completions.\n\n        This method automatically routes each completion to the appropriate verifier\n        based on its metadata. It supports mixed batches containing generation,\n        property prediction, and reaction tasks.\n\n        Args:\n            completions: List of model completions (strings or structured outputs).\n            metadata: List of metadata dictionaries, one per completion. The metadata\n                determines which verifier is used for each completion.\n            debug: If True, enables debug mode with additional logging.\n            use_pbar: If True, displays a progress bar during computation.\n\n        Returns:\n            BatchMolecularVerifierOutputModel containing:\n                - rewards: List of float rewards for each completion\n                - verifier_metadatas: List of metadata from each verification\n\n        Raises:\n            AssertionError: If completions and metadata have different lengths.\n\n        Example:\n            ```python\n            completions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\"]\n            metadata = [{\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}]\n            results = verifier.get_score(completions, metadata)\n            ```\n        \"\"\"\n        assert len(completions) == len(metadata)\n        obj_to_fn: Dict[\n            str,\n            Callable[\n                [\n                    BatchVerifiersInputModel,\n                    bool,\n                    bool,\n                ],\n                List[Any],\n            ],\n        ] = {\n            \"generation\": self._get_generation_score,\n            \"mol_prop\": self._get_prop_pred_score,\n            \"reaction\": self._get_reaction_score,\n        }\n        idxs: Dict[str, List[int]] = {\"generation\": [], \"mol_prop\": [], \"reaction\": []}\n        completions_per_obj: Dict[str, List[str]] = {\n            \"generation\": [],\n            \"mol_prop\": [],\n            \"reaction\": [],\n        }\n        metadata_per_obj: Dict[str, List[Dict[str, Any]]] = {\n            \"generation\": [],\n            \"mol_prop\": [],\n            \"reaction\": [],\n        }\n        for i, (completion, meta) in enumerate(zip(completions, metadata)):\n            assigned = assign_to_inputs(completion, meta)\n            idxs[assigned].append(i)\n            completions_per_obj[assigned].append(completion)\n            metadata_per_obj[assigned].append(meta)\n\n        rewards = [0.0 for _ in range(len(metadata))]\n        metadata_output = [\n            MolecularVerifierOutputMetadataModel() for _ in range(len(metadata))\n        ]\n        parsed_answers: List[str] = [\"\" for _ in range(len(metadata))]\n        for key, fn in obj_to_fn.items():\n            if len(completions_per_obj[key]) &gt; 0:\n                outputs_obj = fn(\n                    BatchVerifiersInputModel(\n                        completions=completions_per_obj[key],\n                        metadatas=metadata_per_obj[key],\n                    ),\n                    debug,\n                    use_pbar,\n                )\n                for i, output in zip(idxs[key], outputs_obj):\n                    rewards[i] = output.reward\n                    metadata_output[i] = (\n                        MolecularVerifierOutputMetadataModel.model_validate(\n                            {f\"{key}_verifier_metadata\": output.verifier_metadata}\n                        )\n                    )\n                    parsed_answers[i] = output.parsed_answer\n        return BatchMolecularVerifierOutputModel(\n            rewards=rewards,\n            parsed_answers=parsed_answers,\n            verifier_metadatas=metadata_output,\n        )\n\n    def __call__(\n        self,\n        completions: List[Any],\n        metadata: List[Dict[str, Any]],\n        debug: bool = False,\n        use_pbar: bool = False,\n    ) -&gt; BatchMolecularVerifierOutputModel:\n        \"\"\"Call the verifier to compute rewards.\n\n        This is a convenience method that calls get_score().\n\n        Args:\n            completions: List of model completions.\n            metadata: List of metadata dictionaries.\n            debug: If True, enables debug mode.\n            use_pbar: If True, displays a progress bar.\n\n        Returns:\n            BatchMolecularVerifierOutputModel with rewards and metadata.\n        \"\"\"\n        return self.get_score(\n            completions=completions, metadata=metadata, debug=debug, use_pbar=use_pbar\n        )\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.generation_verifier","title":"<code>generation_verifier</code>  <code>property</code>","text":"<p>Lazy-loaded generation verifier instance.</p> <p>Returns:</p> Name Type Description <code>GenerationVerifier</code> <code>GenerationVerifier</code> <p>The generation verifier for de novo molecular generation tasks.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If generation_verifier_config is not set in the config.</p>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.mol_prop_verifier","title":"<code>mol_prop_verifier</code>  <code>property</code>","text":"<p>Lazy-loaded molecular property verifier instance.</p> <p>Returns:</p> Name Type Description <code>MolPropVerifier</code> <code>MolPropVerifier</code> <p>The verifier for molecular property prediction tasks.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If mol_prop_verifier_config is not set in the config.</p>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.reaction_verifier","title":"<code>reaction_verifier</code>  <code>property</code>","text":"<p>Lazy-loaded reaction verifier instance.</p> <p>Returns:</p> Name Type Description <code>ReactionVerifier</code> <code>ReactionVerifier</code> <p>The verifier for chemical reaction and retro-synthesis tasks.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If reaction_verifier_config is not set in the config.</p>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.__call__","title":"<code>__call__(completions, metadata, debug=False, use_pbar=False)</code>","text":"<p>Call the verifier to compute rewards.</p> <p>This is a convenience method that calls get_score().</p> <p>Parameters:</p> Name Type Description Default <code>completions</code> <code>List[Any]</code> <p>List of model completions.</p> required <code>metadata</code> <code>List[Dict[str, Any]]</code> <p>List of metadata dictionaries.</p> required <code>debug</code> <code>bool</code> <p>If True, enables debug mode.</p> <code>False</code> <code>use_pbar</code> <code>bool</code> <p>If True, displays a progress bar.</p> <code>False</code> <p>Returns:</p> Type Description <code>BatchMolecularVerifierOutputModel</code> <p>BatchMolecularVerifierOutputModel with rewards and metadata.</p> Source code in <code>mol_gen_docking/reward/molecular_verifier.py</code> <pre><code>def __call__(\n    self,\n    completions: List[Any],\n    metadata: List[Dict[str, Any]],\n    debug: bool = False,\n    use_pbar: bool = False,\n) -&gt; BatchMolecularVerifierOutputModel:\n    \"\"\"Call the verifier to compute rewards.\n\n    This is a convenience method that calls get_score().\n\n    Args:\n        completions: List of model completions.\n        metadata: List of metadata dictionaries.\n        debug: If True, enables debug mode.\n        use_pbar: If True, displays a progress bar.\n\n    Returns:\n        BatchMolecularVerifierOutputModel with rewards and metadata.\n    \"\"\"\n    return self.get_score(\n        completions=completions, metadata=metadata, debug=debug, use_pbar=use_pbar\n    )\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.__init__","title":"<code>__init__(verifier_config)</code>","text":"<p>Initialize the MolecularVerifier.</p> <p>Parameters:</p> Name Type Description Default <code>verifier_config</code> <code>MolecularVerifierConfigModel</code> <p>Configuration model containing settings for all sub-verifiers (generation, property prediction, reaction).</p> required Source code in <code>mol_gen_docking/reward/molecular_verifier.py</code> <pre><code>def __init__(\n    self,\n    verifier_config: MolecularVerifierConfigModel,\n):\n    \"\"\"Initialize the MolecularVerifier.\n\n    Args:\n        verifier_config: Configuration model containing settings for all\n            sub-verifiers (generation, property prediction, reaction).\n    \"\"\"\n    self.verifier_config = verifier_config\n    self.__name__ = \"RewardScorer/MolecularVerifier\"\n    self.remote_tqdm = ray.remote(tqdm_ray.tqdm)\n\n    self._generation_verifier: None | GenerationVerifier = None\n    self._mol_prop_verifier: None | MolPropVerifier = None\n    self._reaction_verifier: None | ReactionVerifier = None\n    self.logger = logging.getLogger(\"RewardScorer\")\n\n    if not ray.is_initialized():\n        ray.init()\n</code></pre>"},{"location":"api/reward/molecular_verifier/#mol_gen_docking.reward.molecular_verifier.MolecularVerifier.get_score","title":"<code>get_score(completions, metadata, debug=False, use_pbar=False)</code>","text":"<p>Compute rewards for a batch of completions.</p> <p>This method automatically routes each completion to the appropriate verifier based on its metadata. It supports mixed batches containing generation, property prediction, and reaction tasks.</p> <p>Parameters:</p> Name Type Description Default <code>completions</code> <code>List[Any]</code> <p>List of model completions (strings or structured outputs).</p> required <code>metadata</code> <code>List[Dict[str, Any]]</code> <p>List of metadata dictionaries, one per completion. The metadata determines which verifier is used for each completion.</p> required <code>debug</code> <code>bool</code> <p>If True, enables debug mode with additional logging.</p> <code>False</code> <code>use_pbar</code> <code>bool</code> <p>If True, displays a progress bar during computation.</p> <code>False</code> <p>Returns:</p> Type Description <code>BatchMolecularVerifierOutputModel</code> <p>BatchMolecularVerifierOutputModel containing: - rewards: List of float rewards for each completion - verifier_metadatas: List of metadata from each verification</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If completions and metadata have different lengths.</p> Example <pre><code>completions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\"]\nmetadata = [{\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}]\nresults = verifier.get_score(completions, metadata)\n</code></pre> Source code in <code>mol_gen_docking/reward/molecular_verifier.py</code> <pre><code>def get_score(\n    self,\n    completions: List[Any],\n    metadata: List[Dict[str, Any]],\n    debug: bool = False,\n    use_pbar: bool = False,\n) -&gt; BatchMolecularVerifierOutputModel:\n    \"\"\"Compute rewards for a batch of completions.\n\n    This method automatically routes each completion to the appropriate verifier\n    based on its metadata. It supports mixed batches containing generation,\n    property prediction, and reaction tasks.\n\n    Args:\n        completions: List of model completions (strings or structured outputs).\n        metadata: List of metadata dictionaries, one per completion. The metadata\n            determines which verifier is used for each completion.\n        debug: If True, enables debug mode with additional logging.\n        use_pbar: If True, displays a progress bar during computation.\n\n    Returns:\n        BatchMolecularVerifierOutputModel containing:\n            - rewards: List of float rewards for each completion\n            - verifier_metadatas: List of metadata from each verification\n\n    Raises:\n        AssertionError: If completions and metadata have different lengths.\n\n    Example:\n        ```python\n        completions = [\"&lt;answer&gt;CCO&lt;/answer&gt;\"]\n        metadata = [{\"properties\": [\"QED\"], \"objectives\": [\"maximize\"], \"target\": [0.0]}]\n        results = verifier.get_score(completions, metadata)\n        ```\n    \"\"\"\n    assert len(completions) == len(metadata)\n    obj_to_fn: Dict[\n        str,\n        Callable[\n            [\n                BatchVerifiersInputModel,\n                bool,\n                bool,\n            ],\n            List[Any],\n        ],\n    ] = {\n        \"generation\": self._get_generation_score,\n        \"mol_prop\": self._get_prop_pred_score,\n        \"reaction\": self._get_reaction_score,\n    }\n    idxs: Dict[str, List[int]] = {\"generation\": [], \"mol_prop\": [], \"reaction\": []}\n    completions_per_obj: Dict[str, List[str]] = {\n        \"generation\": [],\n        \"mol_prop\": [],\n        \"reaction\": [],\n    }\n    metadata_per_obj: Dict[str, List[Dict[str, Any]]] = {\n        \"generation\": [],\n        \"mol_prop\": [],\n        \"reaction\": [],\n    }\n    for i, (completion, meta) in enumerate(zip(completions, metadata)):\n        assigned = assign_to_inputs(completion, meta)\n        idxs[assigned].append(i)\n        completions_per_obj[assigned].append(completion)\n        metadata_per_obj[assigned].append(meta)\n\n    rewards = [0.0 for _ in range(len(metadata))]\n    metadata_output = [\n        MolecularVerifierOutputMetadataModel() for _ in range(len(metadata))\n    ]\n    parsed_answers: List[str] = [\"\" for _ in range(len(metadata))]\n    for key, fn in obj_to_fn.items():\n        if len(completions_per_obj[key]) &gt; 0:\n            outputs_obj = fn(\n                BatchVerifiersInputModel(\n                    completions=completions_per_obj[key],\n                    metadatas=metadata_per_obj[key],\n                ),\n                debug,\n                use_pbar,\n            )\n            for i, output in zip(idxs[key], outputs_obj):\n                rewards[i] = output.reward\n                metadata_output[i] = (\n                    MolecularVerifierOutputMetadataModel.model_validate(\n                        {f\"{key}_verifier_metadata\": output.verifier_metadata}\n                    )\n                )\n                parsed_answers[i] = output.parsed_answer\n    return BatchMolecularVerifierOutputModel(\n        rewards=rewards,\n        parsed_answers=parsed_answers,\n        verifier_metadatas=metadata_output,\n    )\n</code></pre>"},{"location":"api/reward/molecular_verifier/#task-routing","title":"Task Routing","text":"<p>The verifier automatically determines the task type based on metadata structure:</p> Metadata Fields Task Type Verifier <code>properties</code>, <code>objectives</code> (maximize/minimize/above/below), <code>target</code> Generation <code>GenerationVerifier</code> <code>objectives</code> (regression/classification), <code>target</code>, <code>norm_var</code> Property Prediction <code>MolPropVerifier</code> <code>objectives</code> (full_path/smarts/...), <code>target</code>, <code>reactants</code> Reaction <code>ReactionVerifier</code>"},{"location":"api/reward/molecular_verifier/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Lazy Loading: Verifiers are instantiated on first use</li> <li>Ray Parallelization: Long-running computations (docking) are parallelized</li> <li>GPU Allocation: Docking uses fractional GPU allocation for concurrency</li> <li>Caching: Oracle results are cached to avoid redundant computations</li> </ul>"},{"location":"api/reward/molecular_verifier/#related","title":"Related","text":"<ul> <li>Generation Verifier</li> <li>Property Verifier</li> <li>Reaction Verifier</li> </ul>"},{"location":"api/reward/reaction_verifier/","title":"Reaction Verifier","text":"<p>The <code>ReactionVerifier</code> computes rewards for chemical reaction and retro-synthesis tasks, validating synthesis paths, SMARTS predictions, and reaction product verification.</p>"},{"location":"api/reward/reaction_verifier/#overview","title":"Overview","text":"<p>The Reaction Verifier supports various reaction-related tasks:</p> <ul> <li>Retro-synthesis Planning: Validate multi-step synthesis routes</li> <li>SMARTS Prediction: Evaluate predicted reaction SMARTS patterns</li> <li>Product/Reactant Prediction: Compare predicted molecules to ground truth</li> <li>Analog Generation: Generate molecular analogs via synthesis</li> </ul> Supported Task Types <p>The following task types are supported by the Reaction Verifier:</p> Task Type Description <code>final_product</code> Predict the final product of a reaction <code>reactant</code> Predict a single reactant <code>all_reactants</code> Predict all reactants for a reaction <code>smarts</code> Predict the SMARTS pattern for a reaction <code>full_path</code> Provide a complete retro-synthesis path <code>full_path_bb_ref</code> Synthesis path with building block constraints <code>full_path_smarts_ref</code> Synthesis path with SMARTS constraints <code>analog_gen</code> Generate molecular analogs <p>Reaction verifier for chemical reaction and retro-synthesis tasks.</p> <p>This module provides the ReactionVerifier class which computes rewards for chemical reaction tasks including retro-synthesis planning, SMARTS prediction, and reaction product verification.</p>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier_pydantic_model.ReactionVerifierConfigModel","title":"<code>ReactionVerifierConfigModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Pydantic model for molecular verifier configuration.</p> <p>This model defines the configuration parameters for the MolecularVerifier class, providing validation and documentation for all configuration options.</p> <p>Attributes:</p> Name Type Description <code>path_to_mappings</code> <p>Optional path to property mappings and docking targets configuration directory.</p> <code>rescale</code> <p>Whether to rescale the rewards to a normalized range.</p> <code>reaction_matrix_path</code> <code>str</code> <p>Path to the reaction matrix pickle file used for reaction verification.</p> <code>oracle_kwargs</code> <code>str</code> <p>Dictionary of keyword arguments to pass to the docking oracle. Can include:            - exhaustiveness: Docking exhaustiveness parameter            - n_cpu: Number of CPUs for docking            - docking_oracle: Type of docking oracle (\"pyscreener\" or \"autodock_gpu\")            - vina_mode: Command mode for AutoDock GPU</p> <code>docking_concurrency_per_gpu</code> <code>str</code> <p>Number of concurrent docking runs to allow per GPU.                          Default is 2 (uses ~1GB per run on 80GB GPU).</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier_pydantic_model.py</code> <pre><code>class ReactionVerifierConfigModel(BaseModel):\n    \"\"\"Pydantic model for molecular verifier configuration.\n\n    This model defines the configuration parameters for the MolecularVerifier class,\n    providing validation and documentation for all configuration options.\n\n    Attributes:\n        path_to_mappings: Optional path to property mappings and docking targets configuration directory.\n        rescale: Whether to rescale the rewards to a normalized range.\n        reaction_matrix_path: Path to the reaction matrix pickle file used for reaction verification.\n        oracle_kwargs: Dictionary of keyword arguments to pass to the docking oracle. Can include:\n                       - exhaustiveness: Docking exhaustiveness parameter\n                       - n_cpu: Number of CPUs for docking\n                       - docking_oracle: Type of docking oracle (\"pyscreener\" or \"autodock_gpu\")\n                       - vina_mode: Command mode for AutoDock GPU\n        docking_concurrency_per_gpu: Number of concurrent docking runs to allow per GPU.\n                                     Default is 2 (uses ~1GB per run on 80GB GPU).\n    \"\"\"\n\n    parsing_method: Literal[\"none\", \"answer_tags\", \"boxed\"] = Field(\n        default=\"answer_tags\",\n        description=\"Method to parse model completions for SMILES or property values.\",\n    )\n\n    reward: Literal[\"property\", \"valid_smiles\"] = Field(\n        default=\"property\",\n        description='Reward type: \"property\" for property-based or \"valid_smiles\" for validity-based rewards',\n    )\n\n    reaction_matrix_path: str = Field(\n        default=\"data/rxn_matrix.pkl\",\n        description=\"Path to the reaction matrix pickle file for reaction verification\",\n    )\n\n    reaction_reward_type: Literal[\"binary\", \"tanimoto\"] = Field(\n        default=\"tanimoto\",\n        description=\"For retro-synthesis, assign reward based on the exact match (binary) or Tanimoto similarity of the last product\",\n    )\n\n    class Config:\n        \"\"\"Pydantic configuration.\"\"\"\n\n        arbitrary_types_allowed = True\n        json_schema_extra = {\n            \"example\": {\n                \"reward\": \"property\",\n                \"reaction_matrix_path\": \"data/rxn_matrix.pkl\",\n                \"reaction_reward_type\": \"tanimoto\",\n            }\n        }\n\n    @model_validator(mode=\"after\")\n    def check_reaction_matrix_path(self) -&gt; \"ReactionVerifierConfigModel\":\n        \"\"\"Validate that the reaction matrix path exists.\"\"\"\n        if not os.path.exists(self.reaction_matrix_path):\n            raise ValueError(\n                f\"Reaction matrix path {self.reaction_matrix_path} does not exist.\"\n            )\n        return self\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier_pydantic_model.ReactionVerifierConfigModel.Config","title":"<code>Config</code>","text":"<p>Pydantic configuration.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier_pydantic_model.py</code> <pre><code>class Config:\n    \"\"\"Pydantic configuration.\"\"\"\n\n    arbitrary_types_allowed = True\n    json_schema_extra = {\n        \"example\": {\n            \"reward\": \"property\",\n            \"reaction_matrix_path\": \"data/rxn_matrix.pkl\",\n            \"reaction_reward_type\": \"tanimoto\",\n        }\n    }\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier_pydantic_model.ReactionVerifierConfigModel.check_reaction_matrix_path","title":"<code>check_reaction_matrix_path()</code>","text":"<p>Validate that the reaction matrix path exists.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier_pydantic_model.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_reaction_matrix_path(self) -&gt; \"ReactionVerifierConfigModel\":\n    \"\"\"Validate that the reaction matrix path exists.\"\"\"\n    if not os.path.exists(self.reaction_matrix_path):\n        raise ValueError(\n            f\"Reaction matrix path {self.reaction_matrix_path} does not exist.\"\n        )\n    return self\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.input_metadata.ReactionVerifierInputMetadataModel","title":"<code>ReactionVerifierInputMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input metadata model for reaction verifier.</p> <p>Defines the verification criteria for chemical reaction and retro-synthesis tasks, including objectives, target molecules, reactants, products, and validation constraints.</p> <p>Attributes:</p> Name Type Description <code>objectives</code> <code>List[ReactionObjT]</code> <p>List of objective types for the reaction verification. Valid values:</p> <ul> <li>\"final_product\": Verify only the final product matches the target</li> <li>\"reactant\": Verify a single reactant is valid</li> <li>\"all_reactants\": Verify all reactants are chemically valid</li> <li>\"all_reactants_bb_ref\": Verify all reactants are in the building blocks list</li> <li>\"smarts\": Verify reaction follows the given SMARTS pattern</li> <li>\"full_path\": Verify complete synthesis path to target molecule</li> <li>\"full_path_bb_ref\": Verify synthesis path with building block constraints</li> <li>\"full_path_smarts_ref\": Verify synthesis path matches SMARTS patterns</li> <li>\"full_path_smarts_bb_ref\": Verify synthesis path with both SMARTS and building block constraints</li> <li>\"analog_gen\": Verify analog generation task</li> </ul> <code>target</code> <code>List[str]</code> <p>List of target molecules (SMILES strings) for verification. For synthesis tasks: The desired final product molecule For SMARTS tasks: The expected product after reaction Empty list if not applicable to the objective type.</p> <code>reactants</code> <code>List[List[str]]</code> <p>List of reactant lists for each reaction step. Each inner list contains SMILES strings for reactants in one reaction. For ground truth verification in SMARTS prediction tasks. Empty list if not applicable to the objective type.</p> <code>products</code> <code>List[str]</code> <p>List of product molecules (SMILES strings) for each reaction step. For ground truth verification in multi-step synthesis. Empty list if not applicable to the objective type.</p> <code>building_blocks</code> <code>List[str] | None</code> <p>List of valid building block SMILES strings. Optional constraint for synthesis tasks requiring specific building blocks. None if no building block constraints apply.</p> <code>smarts</code> <code>List[str]</code> <p>Reference SMARTS strings for the reaction steps. Used to verify that reactions follow specific reaction templates. Empty list if not applicable to the objective type.</p> <code>or_smarts</code> <code>List[str]</code> <p>Original reference SMARTS strings for the reaction steps. Alternative SMARTS patterns that can also be valid. Empty list if not applicable to the objective type.</p> <code>n_steps_max</code> <code>int</code> <p>Maximum number of reaction steps allowed in the synthesis route. Default is 5. Only applies to full_path objectives.</p> <code>idx_chosen</code> <code>int</code> <p>Index of the chosen reaction for multi-reaction tasks. Default is 0. Used for tracking in batch processing.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/input_metadata.py</code> <pre><code>class ReactionVerifierInputMetadataModel(BaseModel):\n    \"\"\"Input metadata model for reaction verifier.\n\n    Defines the verification criteria for chemical reaction and retro-synthesis tasks,\n    including objectives, target molecules, reactants, products, and validation constraints.\n\n    Attributes:\n        objectives: List of objective types for the reaction verification.\n            Valid values:\n\n            - \"final_product\": Verify only the final product matches the target\n            - \"reactant\": Verify a single reactant is valid\n            - \"all_reactants\": Verify all reactants are chemically valid\n            - \"all_reactants_bb_ref\": Verify all reactants are in the building blocks list\n            - \"smarts\": Verify reaction follows the given SMARTS pattern\n            - \"full_path\": Verify complete synthesis path to target molecule\n            - \"full_path_bb_ref\": Verify synthesis path with building block constraints\n            - \"full_path_smarts_ref\": Verify synthesis path matches SMARTS patterns\n            - \"full_path_smarts_bb_ref\": Verify synthesis path with both SMARTS and building block constraints\n            - \"analog_gen\": Verify analog generation task\n\n        target: List of target molecules (SMILES strings) for verification.\n            For synthesis tasks: The desired final product molecule\n            For SMARTS tasks: The expected product after reaction\n            Empty list if not applicable to the objective type.\n\n        reactants: List of reactant lists for each reaction step.\n            Each inner list contains SMILES strings for reactants in one reaction.\n            For ground truth verification in SMARTS prediction tasks.\n            Empty list if not applicable to the objective type.\n\n        products: List of product molecules (SMILES strings) for each reaction step.\n            For ground truth verification in multi-step synthesis.\n            Empty list if not applicable to the objective type.\n\n        building_blocks: List of valid building block SMILES strings.\n            Optional constraint for synthesis tasks requiring specific building blocks.\n            None if no building block constraints apply.\n\n        smarts: Reference SMARTS strings for the reaction steps.\n            Used to verify that reactions follow specific reaction templates.\n            Empty list if not applicable to the objective type.\n\n        or_smarts: Original reference SMARTS strings for the reaction steps.\n            Alternative SMARTS patterns that can also be valid.\n            Empty list if not applicable to the objective type.\n\n        n_steps_max: Maximum number of reaction steps allowed in the synthesis route.\n            Default is 5. Only applies to full_path objectives.\n\n        idx_chosen: Index of the chosen reaction for multi-reaction tasks.\n            Default is 0. Used for tracking in batch processing.\n    \"\"\"\n\n    objectives: List[ReactionObjT] = Field(\n        ...,\n        description=\"The type of objective for the reaction verification.\",\n    )\n    target: List[str] = Field(\n        default_factory=list,\n        description=\"The target molecule or SMARTS string for verification.\",\n    )\n    reactants: List[List[str]] = Field(\n        default_factory=list,\n        description=\"List of reactants in a reaction.\",\n    )\n    intermediate_products: List[str] = Field(\n        default_factory=list,\n        description=\"The intermediate product molecules of the reaction steps.\",\n    )\n    products: List[str] = Field(\n        default_factory=list,\n        description=\"The product molecule of the reaction.\",\n    )\n    building_blocks: List[str] | None = Field(\n        None,\n        description=\"List of valid building blocks for the reaction.\",\n    )\n    smarts: List[str] = Field(\n        default_factory=list,\n        description=\"Reference SMARTS strings for the reaction steps.\",\n    )\n    or_smarts: List[str] = Field(\n        default_factory=list,\n        description=\"Original Reference SMARTS strings for the reaction steps.\",\n    )\n    n_steps_max: int = Field(\n        default=5,\n        gt=0,\n        description=\"Maximum number of reaction steps allowed in the synthesis route.\",\n    )\n    idx_chosen: int = Field(\n        0,\n        description=\"Index of the chosen reaction.\",\n    )\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier_pydantic_model.ReactionVerifierOutputModel","title":"<code>ReactionVerifierOutputModel</code>","text":"<p>               Bases: <code>VerifierOutputModel</code></p> <p>Output model for reaction verifier results.</p> <p>Attributes:</p> Name Type Description <code>reward</code> <code>float</code> <p>The computed reward for the reaction verification.</p> <code>parsed_answer</code> <code>str</code> <p>The parsed answer extracted from the model completion.</p> <code>verifier_metadata</code> <code>ReactionVerifierMetadataModel</code> <p>Metadata related to the reaction verification process.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier_pydantic_model.py</code> <pre><code>class ReactionVerifierOutputModel(VerifierOutputModel):\n    \"\"\"Output model for reaction verifier results.\n\n    Attributes:\n        reward: The computed reward for the reaction verification.\n        parsed_answer: The parsed answer extracted from the model completion.\n        verifier_metadata: Metadata related to the reaction verification process.\n    \"\"\"\n\n    reward: float = Field(\n        ...,\n        description=\"The computed reward for the reaction verification.\",\n    )\n    parsed_answer: str = Field(\n        ..., description=\"The parsed answer extracted from the model completion.\"\n    )\n    verifier_metadata: ReactionVerifierMetadataModel = Field(\n        ...,\n        description=\"Metadata related to the reaction verification process.\",\n    )\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier_pydantic_model.ReactionVerifierMetadataModel","title":"<code>ReactionVerifierMetadataModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata model for reaction verifier results.</p> <p>Contains detailed information about the reaction verification process, including validity, product correctness, and reactant validation.</p> <p>Attributes:</p> Name Type Description <code>valid</code> <code>float</code> <p>Proportion of valid reaction steps (0.0 to 1.0). For single reaction tasks: 1.0 if valid, 0.0 if invalid. For synthesis route tasks (full_path): proportion of reaction steps that are chemically valid.</p> <code>correct_product</code> <code>float</code> <p>Whether the product is correct or similarity to the target molecule. For SMARTS prediction tasks: 1.0 if reaction produces the correct product, 0.0 otherwise. For synthesis tasks with tanimoto similarity: Tanimoto similarity score (0.0 to 1.0) between the final product and the target molecule. For exact match tasks: 1.0 if exact match, 0.0 otherwise.</p> <code>correct_reactant</code> <code>bool</code> <p>Whether all reactants are correct. For building block constrained tasks: True if all reactants are in the allowed building blocks list. For unconstrained tasks: True if all reactants are chemically valid. False if any reactant is invalid or not allowed.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier_pydantic_model.py</code> <pre><code>class ReactionVerifierMetadataModel(BaseModel):\n    \"\"\"Metadata model for reaction verifier results.\n\n    Contains detailed information about the reaction verification process,\n    including validity, product correctness, and reactant validation.\n\n    Attributes:\n        valid: Proportion of valid reaction steps (0.0 to 1.0).\n            For single reaction tasks: 1.0 if valid, 0.0 if invalid.\n            For synthesis route tasks (full_path): proportion of reaction steps that are chemically valid.\n\n        correct_product: Whether the product is correct or similarity to the target molecule.\n            For SMARTS prediction tasks: 1.0 if reaction produces the correct product, 0.0 otherwise.\n            For synthesis tasks with tanimoto similarity: Tanimoto similarity score (0.0 to 1.0)\n            between the final product and the target molecule.\n            For exact match tasks: 1.0 if exact match, 0.0 otherwise.\n\n        correct_reactant: Whether all reactants are correct.\n            For building block constrained tasks: True if all reactants are in the allowed building blocks list.\n            For unconstrained tasks: True if all reactants are chemically valid.\n            False if any reactant is invalid or not allowed.\n    \"\"\"\n\n    valid: float = Field(\n        default=0.0,\n        description=\"Is the answer valid. If the task is to propose a synthesis route, this is the proportion of valid reaction steps (0.0 to 1.0).\",\n    )\n    correct_product: float = Field(\n        default=0.0,\n        description=\"Whether the product is correct. For synthesis tasks, if we use tanimoto similarity, similarity to the target molecule, for SMARTS prediction, do both of the chemical reactions lead to the correct product.\",\n    )\n    correct_reactant: bool = Field(\n        default=False,\n        description=\"Whether all reactants are correct.\",\n    )\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier","title":"<code>ReactionVerifier</code>","text":"<p>               Bases: <code>Verifier</code></p> <p>Verifier for chemical reaction and retro-synthesis tasks.</p> <p>This verifier computes rewards for various reaction-related tasks including: - Final product prediction - Reactant identification - SMARTS pattern prediction - Full retro-synthesis path validation</p> <p>The verifier uses a reaction matrix to validate synthesis steps and supports both binary and Tanimoto-based reward computation.</p> <p>Attributes:</p> Name Type Description <code>verifier_config</code> <code>ReactionVerifierConfigModel</code> <p>Configuration for the reaction verifier.</p> <code>rxn_matrix</code> <code>ReactantReactionMatrix</code> <p>Pre-loaded reaction matrix for validation.</p> <code>check_ground_truth_tasks</code> <p>List of task types requiring ground truth comparison.</p> <code>run_validation_tasks</code> <p>List of task types requiring path validation.</p> <code>logger</code> <p>Logger instance for the verifier.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>class ReactionVerifier(Verifier):\n    \"\"\"Verifier for chemical reaction and retro-synthesis tasks.\n\n    This verifier computes rewards for various reaction-related tasks including:\n    - Final product prediction\n    - Reactant identification\n    - SMARTS pattern prediction\n    - Full retro-synthesis path validation\n\n    The verifier uses a reaction matrix to validate synthesis steps and supports\n    both binary and Tanimoto-based reward computation.\n\n    Attributes:\n        verifier_config: Configuration for the reaction verifier.\n        rxn_matrix: Pre-loaded reaction matrix for validation.\n        check_ground_truth_tasks: List of task types requiring ground truth comparison.\n        run_validation_tasks: List of task types requiring path validation.\n        logger: Logger instance for the verifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        verifier_config: ReactionVerifierConfigModel,\n    ):\n        \"\"\"Initialize the ReactionVerifier.\n\n        Args:\n            verifier_config: Configuration containing reaction matrix path\n                and reward type settings.\n        \"\"\"\n        super().__init__(verifier_config)\n        self.verifier_config: ReactionVerifierConfigModel = verifier_config\n\n        self.rxn_matrix: ReactantReactionMatrix\n        with open(verifier_config.reaction_matrix_path, \"rb\") as f:\n            self.rxn_matrix = pickle.load(f)\n\n        self.reactants_csi: List[str] = [m.csmiles for m in self.rxn_matrix.reactants]\n        self.check_ground_truth_tasks = [\n            \"final_product\",\n            \"reactant\",\n            \"all_reactants\",\n            \"all_reactants_bb_ref\",\n        ]\n        self.run_validation_tasks = [\n            \"full_path\",\n            \"full_path_bb_ref\",\n            \"full_path_smarts_ref\",\n            \"full_path_smarts_bb_ref\",\n            \"full_path_intermediates_gt_reactants\",\n            \"full_path_intermediates\",  # We treat this task as a normal path validation\n        ]\n        self.logger = logging.getLogger(\"ReactionVerifier\")\n\n    def r_ground_truth_mols(\n        self,\n        mol_y: List[Molecule],\n        mol_label: List[Molecule],\n        reactants: List[str],\n        product: str,\n        smarts: str,\n        objective: str,\n    ) -&gt; float:\n        \"\"\"Compute reward for molecule prediction against ground truth.\"\"\"\n        self.logger.info(\n            f\"Computed molecules: {[mol.smiles for mol in mol_y]} vs labels: {[mol.smiles for mol in mol_label]}\"\n        )\n        smi_y = {mol.csmiles for mol in mol_y}\n        smi_y_true = {mol.csmiles for mol in mol_label}\n        if smi_y == smi_y_true:\n            return 1.0\n\n        # Check if the reaction still works\n        rxn = Reaction(smarts)\n        if objective == \"final_product\":\n            if len(mol_y) != 1:\n                return 0.0\n            react_mols = [Molecule(smi) for smi in reactants]\n            possible_products = []\n            for r_ in itertools.permutations(react_mols):\n                possible_products.extend(rxn(list(r_)))\n            # Change to csmiles to avoid unexpected behavior with python in\n            possible_products_csi = {m.csmiles for m in possible_products}\n            if mol_y[0].csmiles in possible_products_csi:  # Can be only one product\n                return 1.0\n            else:\n                return 0.0\n        elif objective in [\"reactant\", \"all_reactants\", \"all_reactants_bb_ref\"]:\n            # Start from all explicitly specified reactants, excludeing placeholders\n            # representing the label\n            react_mols = [Molecule(smi) for smi in reactants if not smi == \"???\"]\n            react_mols += mol_y\n        # Check that the number of reactants matches\n        if len(react_mols) != rxn.num_reactants:\n            return 0.0\n        # Check if all reactants are in the building blocks\n        if any(m.csmiles not in self.reactants_csi for m in react_mols):\n            return 0.0\n        possible_products = []\n        for r_ in itertools.permutations(react_mols):\n            possible_products.extend(rxn(list(r_)))\n        # Change to csmiles to avoid unexpected behavior with python in\n        possible_products_csi = {m.csmiles for m in possible_products}\n        prod_mol = Molecule(product)\n        if prod_mol.csmiles in possible_products_csi:\n            return 1.0\n        return 0.0\n\n    def ground_truth_reward_mol(\n        self,\n        answer: Dict[str, Any],\n        labels: List[str],\n        reactants: List[str],\n        product: str,\n        smarts: str,\n        objective: str,\n    ) -&gt; float:\n        \"\"\"Compute reward for molecule prediction tasks.\n\n        Notes\n        The answer must be contained in a JSON object with an \"answer\" key, which can be either a single SMILES string or a list of SMILES strings.\n\n        Args:\n            answer: Model completion containing the answer.\n            labels: List of ground truth SMILES strings.\n            reactants: List of reactant SMILES strings.\n            product: Expected product SMILES string.\n            objective: The type of objective for the reaction verification.\n\n        Returns:\n            Reward value between 0.0 and 1.0.\n        \"\"\"\n        if answer == {}:\n            return 0.0\n        mol_label = [Molecule(smi) for smi in labels]\n        if not all([m.is_valid for m in mol_label]):\n            self.logger.error(\"Invalid ground truth molecule\")\n            return 0.0\n        smiles = answer[\"answer\"]\n        if isinstance(smiles, str):\n            smiles_list = [smiles]\n        elif isinstance(smiles, list):\n            smiles_list = smiles\n        else:\n            return 0.0\n        mols = [Molecule(smi) for smi in smiles_list]\n\n        if any([not m.is_valid for m in mols]):\n            self.logger.info(\"Invalid molecule found in prediction\")\n            return 0.0\n\n        return self.r_ground_truth_mols(\n            mols,\n            mol_label,\n            reactants=reactants,\n            product=product,\n            smarts=smarts,\n            objective=objective,\n        )\n\n    def reward_smarts(\n        self,\n        answer: Dict[str, Any],\n        labels: List[str],\n        reactants: List[str],\n        product: str,\n    ) -&gt; Tuple[float, Dict[str, Any]]:\n        \"\"\"Compute reward for SMARTS prediction tasks.\n\n        Notes\n        The answer must be contained in a JSON object with an \"answer\" key,\n        which should be a SMARTS string representing the reaction. The reward is computed based\n        on whether the proposed SMARTS can produce the expected product from the given reactants.\n        A reward of 1.0 is given for an exact match with the ground truth SMARTS, 0.1 if the SMARTS\n        is valid and produces the correct product, and 0.0 otherwise.\n\n        Args:\n            answer: Model completion containing the SMARTS answer.\n            labels: List containing the ground truth SMARTS string.\n            reactants: List of reactant SMILES strings.\n            product: Expected product SMILES string.\n\n        Returns:\n            Tuple of (reward, metadata_dict) where metadata contains\n            'Reactants_contained' and 'Products_contained' flags.\n        \"\"\"\n        if answer == {}:\n            return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n        gt_smarts = labels[0]\n        smarts_pred = answer[\"answer\"]\n        if not isinstance(smarts_pred, str):\n            return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n\n        if smarts_pred.strip() == gt_smarts:\n            return 1.0, {\"Reactants_contained\": True, \"Products_contained\": True}\n        self.logger.info(\n            f\"Proposed SMARTS: {smarts_pred.strip()} | GT SMARTS: {gt_smarts}, checking reaction...\"\n        )\n        try:\n            rxnB = Reaction(smarts_pred.strip())\n            if rxnB.num_reactants != len(reactants):\n                return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n            p = rxnB([Molecule(r) for r in reactants])\n            reward = 0.0\n            if Molecule(product).csmiles in {prod.csmiles for prod in p}:\n                reward = 0.1\n            return reward, {\n                \"Reactants_contained\": True,\n                \"Products_contained\": reward == 0.1,\n            }\n        except Exception as e:\n            self.logger.info(\n                f\"Error in reaction SMARTS parsing: {e} (proposed: {smarts_pred} | gt: {gt_smarts})\"\n            )\n            return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n\n    def _find_reaction_smarts(\n        self,\n        reactants_step: List[Molecule],\n        products_step: List[Molecule],\n        allowed_smarts: ReactionContainer,\n    ) -&gt; List[Reaction]:\n        \"\"\"Find valid reaction SMARTS that can produce products from reactants.\n\n        Args:\n            reactants_step: List of reactant molecules for this step.\n            products_step: List of expected product molecules.\n            allowed_smarts: Container of allowed reaction SMARTS patterns.\n\n        Returns:\n            List of Reaction objects that successfully produce the expected products.\n        \"\"\"\n        found_reactions: List[Reaction] = []\n        id_poss_smarts: List[Dict[int, tuple[int, ...]]] = []\n        for r in reactants_step:\n            id_poss_smarts.append(allowed_smarts.match_reactions(r))\n\n        for id_reaction in id_poss_smarts[0]:\n            # Check if reaction can take the correct number of reactants\n            if allowed_smarts[id_reaction].num_reactants != len(reactants_step):\n                continue\n            if any(\n                id_reaction not in id_poss_smarts[i]\n                for i in range(1, len(reactants_step))\n            ):\n                continue\n\n            # Generate all permutations of reactants and test the reaction\n            possible_products = []\n            for reactants_ord in itertools.permutations(reactants_step):\n                possible_products.extend(\n                    allowed_smarts[id_reaction](list(reactants_ord))\n                )\n            possible_products_csi = {m.csmiles for m in possible_products}\n            all_found: bool = all(\n                p.csmiles in possible_products_csi for p in products_step\n            )\n            if all_found:\n                found_reactions.append(allowed_smarts[id_reaction])\n        return found_reactions\n\n    def _check_valid_step(\n        self,\n        reactants_step: List[Molecule],\n        products_step: List[Molecule],\n        possible_reactants: List[str],\n        allowed_smarts: ReactionContainer,\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"Check if a synthesis step is valid.\n        Used in the reward_run_path method to validate each step of the proposed synthesis path.\n\n        Validates that:\n        1. All reactants and products are valid molecules\n        2. All reactants are in building blocks or previous products\n        3. At least one reaction can produce the products from the reactants\n\n        Args:\n            reactants_step: List of reactant molecules for this step.\n            products_step: List of expected product molecules.\n            possible_reactants: List of valid starting materials (building blocks + previous products).\n            allowed_smarts: Container of allowed reaction SMARTS patterns.\n\n        Returns:\n            Tuple of (is_valid, fail_reason) where fail_reason is empty string if valid,\n            or one of \"reactants\", \"products\", \"reaction\" indicating what failed.\n        \"\"\"\n        # 1. Check that all reactants and products are valid molecules\n        if not all([r.is_valid for r in reactants_step]):\n            self.logger.info(\n                \"Reactants not valid in {}\".format([r.smiles for r in reactants_step])\n            )\n            return False, \"reactants\"\n        if not all([p.is_valid for p in products_step]):\n            self.logger.info(\n                \"Products not valid in {}\".format([p.smiles for p in products_step])\n            )\n            return False, \"products\"\n        # 2. Check that all reactants are in building blocks or previous products\n        for r in reactants_step:\n            if r.csmiles not in possible_reactants:\n                self.logger.info(\n                    \"Reactant {} not in building blocks or previous products\".format(\n                        r.smiles\n                    )\n                )\n                return False, \"reactants\"\n        if products_step == []:\n            self.logger.info(\"No products in step\")\n            return False, \"products\"\n\n        # 3. Check that there is at least one reaction that can produce the products from the reactants\n        found_reactions = self._find_reaction_smarts(\n            reactants_step, products_step, allowed_smarts\n        )\n        if len(found_reactions) == 0:\n            self.logger.info(\n                \"No reaction found for step: {} -&gt; {}\".format(\n                    [r.smiles for r in reactants_step],\n                    [p.smiles for p in products_step],\n                )\n            )\n            return False, \"reaction\"\n        # Log success\n        self.logger.info(\n            \"Found valid reaction for step: {} -&gt; {}\".format(\n                [r.smiles for r in reactants_step],\n                [p.smiles for p in products_step],\n            )\n        )\n        return True, \"\"\n\n    def reward_run_path(\n        self,\n        answer: Dict[str, Any],\n        label: str,\n        building_blocks: List[str],\n        smarts: List[str],\n        n_steps_max: int,\n        reward_type: Literal[\"binary\", \"tanimoto\"] = \"binary\",\n    ) -&gt; Tuple[float, Dict[str, Any]]:\n        \"\"\"Compute reward for retro-synthesis path validation.\n\n        Validates a multi-step synthesis path by checking:\n        1. All reactants are valid building blocks or previous products\n        2. Each reaction step has a valid SMARTS pattern\n        3. The final product matches the target (exactly or by Tanimoto similarity)\n\n        Notes\n        The answer must be contained in a JSON object with an \"answer\" key,\n        which should contain a list of steps: Dictionaries containing the keys:\n\n            - reactants: List of reactant SMILES strings for this step\n            - product: List of product SMILES strings for this step\n\n        Args:\n            answer: Model completion containing the synthesis path.\n            label: Target product SMILES string.\n            building_blocks: List of valid starting building block SMILES.\n            smarts: List of allowed SMARTS patterns (empty = use reaction matrix).\n            n_steps_max: Maximum allowed number of synthesis steps.\n            reward_type: \"binary\" for exact match or \"tanimoto\" for similarity-based.\n\n        Returns:\n            Tuple of (reward, metadata_dict) containing validation results.\n        \"\"\"\n        if answer == {}:\n            self.logger.info(\"No synthesis path found in completion\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        steps: List[Dict[str, Any]] = answer[\"answer\"]\n        if not isinstance(steps, list) or len(steps) == 0 or len(steps) &gt; n_steps_max:\n            self.logger.info(\"Synthesis path answer is not a list or is too long/short\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        # Validate each step structure\n        for step in steps:\n            if \"reactants\" not in step or \"product\" not in step:\n                self.logger.info(\"Synthesis step missing reactants or product\")\n                return 0.0, {\n                    \"valid\": 0.0,\n                    \"correct_product\": 0.0,\n                    \"correct_reactant\": False,\n                }\n            if step[\"reactants\"] == [] or step[\"product\"] == []:\n                self.logger.info(\"Synthesis step has empty reactants or product\")\n                return 0.0, {\n                    \"valid\": 0.0,\n                    \"correct_product\": 0.0,\n                    \"correct_reactant\": False,\n                }\n            if not isinstance(step[\"reactants\"], list) or not all(\n                isinstance(r, str) for r in step[\"reactants\"]\n            ):\n                self.logger.info(\"One or more reactants in step are not strings\")\n                return 0.0, {\n                    \"valid\": 0.0,\n                    \"correct_product\": 0.0,\n                    \"correct_reactant\": False,\n                }\n            if not isinstance(step[\"product\"], list) or not all(\n                isinstance(p, str) for p in step[\"product\"]\n            ):\n                self.logger.info(\"One or more products in step are not strings\")\n                return 0.0, {\n                    \"valid\": 0.0,\n                    \"correct_product\": 0.0,\n                    \"correct_reactant\": False,\n                }\n\n        reactants = [\n            [Molecule(smi.strip()) for smi in step[\"reactants\"]] for step in steps\n        ]\n        products = [\n            [Molecule(smi.strip()) for smi in step[\"product\"]] for step in steps\n        ]\n        n_steps = len(reactants)\n        label_mol = Molecule(label)\n        reward_mult: List[float] = [1.0 for _ in products]\n        if reward_type == \"binary\" and not any(\n            [label_mol == last_p for last_p in products[-1]]\n        ):\n            self.logger.info(\"Product not found\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_last_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        elif reward_type == \"tanimoto\":\n            # Compute the tanimoto similarity between the label and products at each step\n            for i, product in enumerate(products):\n                all_sims = label_mol.tanimoto_similarity(product)\n                reward_mult[i] = max(all_sims) ** 3\n\n        reactions: ReactionContainer\n        if smarts == []:\n            reactions = self.rxn_matrix.reactions\n        else:\n            reactions = ReactionContainer([Reaction(sma) for sma in smarts])\n\n        building_blocks_csi = (\n            self.reactants_csi\n        )  # For the moment, the building blocks passed in the metadata\n        # are only used to  help the model but not proper constraints.\n\n        n_valid = 0\n        fail_reason = \"\"\n        for i_reac, (reactant, product) in enumerate(zip(reactants, products)):\n            is_valid, fail_reason = self._check_valid_step(\n                reactant,\n                product,\n                building_blocks_csi\n                + [p.csmiles for step in products[:i_reac] for p in step],\n                reactions,\n            )\n            if not is_valid:\n                self.logger.info(f\"Invalid step at index {i_reac} due to {fail_reason}\")\n                break\n            else:\n                n_valid += 1\n        if n_valid &lt; n_steps:\n            return reward_mult[n_valid - 1] * (n_valid / n_steps) ** 2, {\n                \"valid\": n_valid / n_steps,\n                \"correct_product\": reward_mult[n_valid - 1],\n                \"correct_reactant\": fail_reason != \"reactants\",\n            }\n\n        return reward_mult[n_valid - 1], {\n            \"valid\": 1.0,\n            \"correct_product\": reward_mult[n_valid - 1],\n            \"correct_reactant\": True,\n        }\n\n    def parse_json_content(self, content: str) -&gt; Dict[str, Any]:\n        \"\"\"Parse JSON content from the model completion.\n\n        Args:\n            content: The extracted answer content from the model completion.\n        Returns:\n            Parsed JSON as a dictionary.\n        \"\"\"\n        # Find the first and last curly braces to extract JSON\n        # as a regex with { \"answer\": ... }\n        possible_json = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n        parsed: Dict[str, Any]\n        if possible_json is None:\n            return {}\n        try:\n            parsed = json.loads(possible_json.group(0))\n        except json.JSONDecodeError as e:\n            self.logger.info(f\"JSON decode error: {e}\")\n            return {}\n        if \"answer\" not in parsed:\n            return {}\n        return parsed\n\n    def get_score(\n        self, inputs: BatchVerifiersInputModel\n    ) -&gt; List[ReactionVerifierOutputModel]:\n        \"\"\"Compute reaction rewards for a batch of completions.\n\n        This method routes each completion to the appropriate reward function\n        based on the objective type specified in the metadata.\n\n        Args:\n            inputs: Batch of completions and metadata for verification.\n\n        Returns:\n            List of ReactionVerifierOutputModel containing rewards and metadata.\n\n        Notes:\n            - Ground truth tasks: final_product, reactant, all_reactants\n            - SMARTS tasks: smarts prediction with reaction validation\n            - Path tasks: full_path with step-by-step validation\n        \"\"\"\n        completions = inputs.completions\n        assert all(\n            isinstance(meta, ReactionVerifierInputMetadataModel)\n            for meta in inputs.metadatas\n        )\n        metadatas: List[ReactionVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n        output_models = []\n        for answer, meta in zip(completions, metadatas):\n            objective = meta.objectives[0]\n            reward = 0.0\n            reward_metadata = {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n            extracted_answer = self.parse_answer(answer)\n            json_answer = self.parse_json_content(extracted_answer)\n\n            if objective in self.check_ground_truth_tasks:\n                reward = self.ground_truth_reward_mol(\n                    json_answer,\n                    meta.target,\n                    reactants=meta.reactants[meta.idx_chosen],\n                    product=meta.products[meta.idx_chosen],\n                    smarts=meta.or_smarts[meta.idx_chosen],\n                    objective=objective,\n                )\n                reward_metadata = {\n                    \"valid\": float(reward &gt; 0.0),\n                    \"correct_product\": reward &gt; 0.0,\n                    \"correct_reactant\": reward &gt; 0.0,\n                }\n            elif objective == \"smarts\":\n                assert len(meta.reactants) &gt; 0, (\n                    \"Reactants must be provided for SMARTS objective\"\n                )\n                assert len(meta.products) &gt; 0, (\n                    \"Product must be provided for SMARTS objective\"\n                )\n                reward, raw_metadata = self.reward_smarts(\n                    json_answer,\n                    meta.target,\n                    meta.reactants[0],\n                    meta.products[0],\n                )\n                reward_metadata = {\n                    \"valid\": reward,\n                    \"correct_product\": raw_metadata.get(\"Products_contained\", False),\n                    \"correct_reactant\": raw_metadata.get(\"Reactants_contained\", False),\n                }\n            elif objective in self.run_validation_tasks:\n                assert len(meta.target) &gt; 0, (\n                    \"Target must be provided for run validation tasks\"\n                )\n                # If smarts constraint, add it\n                if \"smarts\" in objective:\n                    assert len(meta.smarts) &gt; 0, (\n                        \"SMARTS must be provided for smarts reference path validation\"\n                    )\n                    smarts = meta.smarts\n                else:\n                    smarts = []\n                reward, raw_metadata = self.reward_run_path(\n                    json_answer,\n                    meta.target[0],\n                    meta.building_blocks if meta.building_blocks else [],\n                    smarts=smarts,\n                    n_steps_max=meta.n_steps_max,\n                    reward_type=self.verifier_config.reaction_reward_type,\n                )\n                reward_metadata = raw_metadata\n            else:\n                raise ValueError(\n                    \"Unknown objective {} type for reaction verifier\".format(objective)\n                )\n\n            if self.verifier_config.reward == \"valid_smiles\":\n                reward = float(reward &gt; 0.0)\n\n            # Create the output model\n            output_model = ReactionVerifierOutputModel(\n                reward=reward,\n                parsed_answer=f\"{json_answer}\",\n                verifier_metadata=ReactionVerifierMetadataModel(\n                    valid=reward_metadata[\"valid\"],\n                    correct_product=reward_metadata[\"correct_product\"],\n                    correct_reactant=reward_metadata[\"correct_reactant\"],\n                ),\n            )\n            output_models.append(output_model)\n\n        return output_models\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.__init__","title":"<code>__init__(verifier_config)</code>","text":"<p>Initialize the ReactionVerifier.</p> <p>Parameters:</p> Name Type Description Default <code>verifier_config</code> <code>ReactionVerifierConfigModel</code> <p>Configuration containing reaction matrix path and reward type settings.</p> required Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def __init__(\n    self,\n    verifier_config: ReactionVerifierConfigModel,\n):\n    \"\"\"Initialize the ReactionVerifier.\n\n    Args:\n        verifier_config: Configuration containing reaction matrix path\n            and reward type settings.\n    \"\"\"\n    super().__init__(verifier_config)\n    self.verifier_config: ReactionVerifierConfigModel = verifier_config\n\n    self.rxn_matrix: ReactantReactionMatrix\n    with open(verifier_config.reaction_matrix_path, \"rb\") as f:\n        self.rxn_matrix = pickle.load(f)\n\n    self.reactants_csi: List[str] = [m.csmiles for m in self.rxn_matrix.reactants]\n    self.check_ground_truth_tasks = [\n        \"final_product\",\n        \"reactant\",\n        \"all_reactants\",\n        \"all_reactants_bb_ref\",\n    ]\n    self.run_validation_tasks = [\n        \"full_path\",\n        \"full_path_bb_ref\",\n        \"full_path_smarts_ref\",\n        \"full_path_smarts_bb_ref\",\n        \"full_path_intermediates_gt_reactants\",\n        \"full_path_intermediates\",  # We treat this task as a normal path validation\n    ]\n    self.logger = logging.getLogger(\"ReactionVerifier\")\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.get_score","title":"<code>get_score(inputs)</code>","text":"<p>Compute reaction rewards for a batch of completions.</p> <p>This method routes each completion to the appropriate reward function based on the objective type specified in the metadata.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>BatchVerifiersInputModel</code> <p>Batch of completions and metadata for verification.</p> required <p>Returns:</p> Type Description <code>List[ReactionVerifierOutputModel]</code> <p>List of ReactionVerifierOutputModel containing rewards and metadata.</p> Notes <ul> <li>Ground truth tasks: final_product, reactant, all_reactants</li> <li>SMARTS tasks: smarts prediction with reaction validation</li> <li>Path tasks: full_path with step-by-step validation</li> </ul> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def get_score(\n    self, inputs: BatchVerifiersInputModel\n) -&gt; List[ReactionVerifierOutputModel]:\n    \"\"\"Compute reaction rewards for a batch of completions.\n\n    This method routes each completion to the appropriate reward function\n    based on the objective type specified in the metadata.\n\n    Args:\n        inputs: Batch of completions and metadata for verification.\n\n    Returns:\n        List of ReactionVerifierOutputModel containing rewards and metadata.\n\n    Notes:\n        - Ground truth tasks: final_product, reactant, all_reactants\n        - SMARTS tasks: smarts prediction with reaction validation\n        - Path tasks: full_path with step-by-step validation\n    \"\"\"\n    completions = inputs.completions\n    assert all(\n        isinstance(meta, ReactionVerifierInputMetadataModel)\n        for meta in inputs.metadatas\n    )\n    metadatas: List[ReactionVerifierInputMetadataModel] = inputs.metadatas  # type: ignore\n\n    output_models = []\n    for answer, meta in zip(completions, metadatas):\n        objective = meta.objectives[0]\n        reward = 0.0\n        reward_metadata = {\n            \"valid\": 0.0,\n            \"correct_product\": 0.0,\n            \"correct_reactant\": False,\n        }\n        extracted_answer = self.parse_answer(answer)\n        json_answer = self.parse_json_content(extracted_answer)\n\n        if objective in self.check_ground_truth_tasks:\n            reward = self.ground_truth_reward_mol(\n                json_answer,\n                meta.target,\n                reactants=meta.reactants[meta.idx_chosen],\n                product=meta.products[meta.idx_chosen],\n                smarts=meta.or_smarts[meta.idx_chosen],\n                objective=objective,\n            )\n            reward_metadata = {\n                \"valid\": float(reward &gt; 0.0),\n                \"correct_product\": reward &gt; 0.0,\n                \"correct_reactant\": reward &gt; 0.0,\n            }\n        elif objective == \"smarts\":\n            assert len(meta.reactants) &gt; 0, (\n                \"Reactants must be provided for SMARTS objective\"\n            )\n            assert len(meta.products) &gt; 0, (\n                \"Product must be provided for SMARTS objective\"\n            )\n            reward, raw_metadata = self.reward_smarts(\n                json_answer,\n                meta.target,\n                meta.reactants[0],\n                meta.products[0],\n            )\n            reward_metadata = {\n                \"valid\": reward,\n                \"correct_product\": raw_metadata.get(\"Products_contained\", False),\n                \"correct_reactant\": raw_metadata.get(\"Reactants_contained\", False),\n            }\n        elif objective in self.run_validation_tasks:\n            assert len(meta.target) &gt; 0, (\n                \"Target must be provided for run validation tasks\"\n            )\n            # If smarts constraint, add it\n            if \"smarts\" in objective:\n                assert len(meta.smarts) &gt; 0, (\n                    \"SMARTS must be provided for smarts reference path validation\"\n                )\n                smarts = meta.smarts\n            else:\n                smarts = []\n            reward, raw_metadata = self.reward_run_path(\n                json_answer,\n                meta.target[0],\n                meta.building_blocks if meta.building_blocks else [],\n                smarts=smarts,\n                n_steps_max=meta.n_steps_max,\n                reward_type=self.verifier_config.reaction_reward_type,\n            )\n            reward_metadata = raw_metadata\n        else:\n            raise ValueError(\n                \"Unknown objective {} type for reaction verifier\".format(objective)\n            )\n\n        if self.verifier_config.reward == \"valid_smiles\":\n            reward = float(reward &gt; 0.0)\n\n        # Create the output model\n        output_model = ReactionVerifierOutputModel(\n            reward=reward,\n            parsed_answer=f\"{json_answer}\",\n            verifier_metadata=ReactionVerifierMetadataModel(\n                valid=reward_metadata[\"valid\"],\n                correct_product=reward_metadata[\"correct_product\"],\n                correct_reactant=reward_metadata[\"correct_reactant\"],\n            ),\n        )\n        output_models.append(output_model)\n\n    return output_models\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.ground_truth_reward_mol","title":"<code>ground_truth_reward_mol(answer, labels, reactants, product, smarts, objective)</code>","text":"<p>Compute reward for molecule prediction tasks.</p> <p>Notes The answer must be contained in a JSON object with an \"answer\" key, which can be either a single SMILES string or a list of SMILES strings.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>Dict[str, Any]</code> <p>Model completion containing the answer.</p> required <code>labels</code> <code>List[str]</code> <p>List of ground truth SMILES strings.</p> required <code>reactants</code> <code>List[str]</code> <p>List of reactant SMILES strings.</p> required <code>product</code> <code>str</code> <p>Expected product SMILES string.</p> required <code>objective</code> <code>str</code> <p>The type of objective for the reaction verification.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Reward value between 0.0 and 1.0.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def ground_truth_reward_mol(\n    self,\n    answer: Dict[str, Any],\n    labels: List[str],\n    reactants: List[str],\n    product: str,\n    smarts: str,\n    objective: str,\n) -&gt; float:\n    \"\"\"Compute reward for molecule prediction tasks.\n\n    Notes\n    The answer must be contained in a JSON object with an \"answer\" key, which can be either a single SMILES string or a list of SMILES strings.\n\n    Args:\n        answer: Model completion containing the answer.\n        labels: List of ground truth SMILES strings.\n        reactants: List of reactant SMILES strings.\n        product: Expected product SMILES string.\n        objective: The type of objective for the reaction verification.\n\n    Returns:\n        Reward value between 0.0 and 1.0.\n    \"\"\"\n    if answer == {}:\n        return 0.0\n    mol_label = [Molecule(smi) for smi in labels]\n    if not all([m.is_valid for m in mol_label]):\n        self.logger.error(\"Invalid ground truth molecule\")\n        return 0.0\n    smiles = answer[\"answer\"]\n    if isinstance(smiles, str):\n        smiles_list = [smiles]\n    elif isinstance(smiles, list):\n        smiles_list = smiles\n    else:\n        return 0.0\n    mols = [Molecule(smi) for smi in smiles_list]\n\n    if any([not m.is_valid for m in mols]):\n        self.logger.info(\"Invalid molecule found in prediction\")\n        return 0.0\n\n    return self.r_ground_truth_mols(\n        mols,\n        mol_label,\n        reactants=reactants,\n        product=product,\n        smarts=smarts,\n        objective=objective,\n    )\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.parse_json_content","title":"<code>parse_json_content(content)</code>","text":"<p>Parse JSON content from the model completion.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The extracted answer content from the model completion.</p> required <p>Returns:     Parsed JSON as a dictionary.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def parse_json_content(self, content: str) -&gt; Dict[str, Any]:\n    \"\"\"Parse JSON content from the model completion.\n\n    Args:\n        content: The extracted answer content from the model completion.\n    Returns:\n        Parsed JSON as a dictionary.\n    \"\"\"\n    # Find the first and last curly braces to extract JSON\n    # as a regex with { \"answer\": ... }\n    possible_json = re.search(r\"\\{.*\\}\", content, re.DOTALL)\n    parsed: Dict[str, Any]\n    if possible_json is None:\n        return {}\n    try:\n        parsed = json.loads(possible_json.group(0))\n    except json.JSONDecodeError as e:\n        self.logger.info(f\"JSON decode error: {e}\")\n        return {}\n    if \"answer\" not in parsed:\n        return {}\n    return parsed\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.r_ground_truth_mols","title":"<code>r_ground_truth_mols(mol_y, mol_label, reactants, product, smarts, objective)</code>","text":"<p>Compute reward for molecule prediction against ground truth.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def r_ground_truth_mols(\n    self,\n    mol_y: List[Molecule],\n    mol_label: List[Molecule],\n    reactants: List[str],\n    product: str,\n    smarts: str,\n    objective: str,\n) -&gt; float:\n    \"\"\"Compute reward for molecule prediction against ground truth.\"\"\"\n    self.logger.info(\n        f\"Computed molecules: {[mol.smiles for mol in mol_y]} vs labels: {[mol.smiles for mol in mol_label]}\"\n    )\n    smi_y = {mol.csmiles for mol in mol_y}\n    smi_y_true = {mol.csmiles for mol in mol_label}\n    if smi_y == smi_y_true:\n        return 1.0\n\n    # Check if the reaction still works\n    rxn = Reaction(smarts)\n    if objective == \"final_product\":\n        if len(mol_y) != 1:\n            return 0.0\n        react_mols = [Molecule(smi) for smi in reactants]\n        possible_products = []\n        for r_ in itertools.permutations(react_mols):\n            possible_products.extend(rxn(list(r_)))\n        # Change to csmiles to avoid unexpected behavior with python in\n        possible_products_csi = {m.csmiles for m in possible_products}\n        if mol_y[0].csmiles in possible_products_csi:  # Can be only one product\n            return 1.0\n        else:\n            return 0.0\n    elif objective in [\"reactant\", \"all_reactants\", \"all_reactants_bb_ref\"]:\n        # Start from all explicitly specified reactants, excludeing placeholders\n        # representing the label\n        react_mols = [Molecule(smi) for smi in reactants if not smi == \"???\"]\n        react_mols += mol_y\n    # Check that the number of reactants matches\n    if len(react_mols) != rxn.num_reactants:\n        return 0.0\n    # Check if all reactants are in the building blocks\n    if any(m.csmiles not in self.reactants_csi for m in react_mols):\n        return 0.0\n    possible_products = []\n    for r_ in itertools.permutations(react_mols):\n        possible_products.extend(rxn(list(r_)))\n    # Change to csmiles to avoid unexpected behavior with python in\n    possible_products_csi = {m.csmiles for m in possible_products}\n    prod_mol = Molecule(product)\n    if prod_mol.csmiles in possible_products_csi:\n        return 1.0\n    return 0.0\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.reward_run_path","title":"<code>reward_run_path(answer, label, building_blocks, smarts, n_steps_max, reward_type='binary')</code>","text":"<p>Compute reward for retro-synthesis path validation.</p> <p>Validates a multi-step synthesis path by checking: 1. All reactants are valid building blocks or previous products 2. Each reaction step has a valid SMARTS pattern 3. The final product matches the target (exactly or by Tanimoto similarity)</p> <p>Notes The answer must be contained in a JSON object with an \"answer\" key, which should contain a list of steps: Dictionaries containing the keys:</p> <pre><code>- reactants: List of reactant SMILES strings for this step\n- product: List of product SMILES strings for this step\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>Dict[str, Any]</code> <p>Model completion containing the synthesis path.</p> required <code>label</code> <code>str</code> <p>Target product SMILES string.</p> required <code>building_blocks</code> <code>List[str]</code> <p>List of valid starting building block SMILES.</p> required <code>smarts</code> <code>List[str]</code> <p>List of allowed SMARTS patterns (empty = use reaction matrix).</p> required <code>n_steps_max</code> <code>int</code> <p>Maximum allowed number of synthesis steps.</p> required <code>reward_type</code> <code>Literal['binary', 'tanimoto']</code> <p>\"binary\" for exact match or \"tanimoto\" for similarity-based.</p> <code>'binary'</code> <p>Returns:</p> Type Description <code>Tuple[float, Dict[str, Any]]</code> <p>Tuple of (reward, metadata_dict) containing validation results.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def reward_run_path(\n    self,\n    answer: Dict[str, Any],\n    label: str,\n    building_blocks: List[str],\n    smarts: List[str],\n    n_steps_max: int,\n    reward_type: Literal[\"binary\", \"tanimoto\"] = \"binary\",\n) -&gt; Tuple[float, Dict[str, Any]]:\n    \"\"\"Compute reward for retro-synthesis path validation.\n\n    Validates a multi-step synthesis path by checking:\n    1. All reactants are valid building blocks or previous products\n    2. Each reaction step has a valid SMARTS pattern\n    3. The final product matches the target (exactly or by Tanimoto similarity)\n\n    Notes\n    The answer must be contained in a JSON object with an \"answer\" key,\n    which should contain a list of steps: Dictionaries containing the keys:\n\n        - reactants: List of reactant SMILES strings for this step\n        - product: List of product SMILES strings for this step\n\n    Args:\n        answer: Model completion containing the synthesis path.\n        label: Target product SMILES string.\n        building_blocks: List of valid starting building block SMILES.\n        smarts: List of allowed SMARTS patterns (empty = use reaction matrix).\n        n_steps_max: Maximum allowed number of synthesis steps.\n        reward_type: \"binary\" for exact match or \"tanimoto\" for similarity-based.\n\n    Returns:\n        Tuple of (reward, metadata_dict) containing validation results.\n    \"\"\"\n    if answer == {}:\n        self.logger.info(\"No synthesis path found in completion\")\n        return 0.0, {\n            \"valid\": 0.0,\n            \"correct_product\": 0.0,\n            \"correct_reactant\": False,\n        }\n    steps: List[Dict[str, Any]] = answer[\"answer\"]\n    if not isinstance(steps, list) or len(steps) == 0 or len(steps) &gt; n_steps_max:\n        self.logger.info(\"Synthesis path answer is not a list or is too long/short\")\n        return 0.0, {\n            \"valid\": 0.0,\n            \"correct_product\": 0.0,\n            \"correct_reactant\": False,\n        }\n    # Validate each step structure\n    for step in steps:\n        if \"reactants\" not in step or \"product\" not in step:\n            self.logger.info(\"Synthesis step missing reactants or product\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        if step[\"reactants\"] == [] or step[\"product\"] == []:\n            self.logger.info(\"Synthesis step has empty reactants or product\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        if not isinstance(step[\"reactants\"], list) or not all(\n            isinstance(r, str) for r in step[\"reactants\"]\n        ):\n            self.logger.info(\"One or more reactants in step are not strings\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n        if not isinstance(step[\"product\"], list) or not all(\n            isinstance(p, str) for p in step[\"product\"]\n        ):\n            self.logger.info(\"One or more products in step are not strings\")\n            return 0.0, {\n                \"valid\": 0.0,\n                \"correct_product\": 0.0,\n                \"correct_reactant\": False,\n            }\n\n    reactants = [\n        [Molecule(smi.strip()) for smi in step[\"reactants\"]] for step in steps\n    ]\n    products = [\n        [Molecule(smi.strip()) for smi in step[\"product\"]] for step in steps\n    ]\n    n_steps = len(reactants)\n    label_mol = Molecule(label)\n    reward_mult: List[float] = [1.0 for _ in products]\n    if reward_type == \"binary\" and not any(\n        [label_mol == last_p for last_p in products[-1]]\n    ):\n        self.logger.info(\"Product not found\")\n        return 0.0, {\n            \"valid\": 0.0,\n            \"correct_last_product\": 0.0,\n            \"correct_reactant\": False,\n        }\n    elif reward_type == \"tanimoto\":\n        # Compute the tanimoto similarity between the label and products at each step\n        for i, product in enumerate(products):\n            all_sims = label_mol.tanimoto_similarity(product)\n            reward_mult[i] = max(all_sims) ** 3\n\n    reactions: ReactionContainer\n    if smarts == []:\n        reactions = self.rxn_matrix.reactions\n    else:\n        reactions = ReactionContainer([Reaction(sma) for sma in smarts])\n\n    building_blocks_csi = (\n        self.reactants_csi\n    )  # For the moment, the building blocks passed in the metadata\n    # are only used to  help the model but not proper constraints.\n\n    n_valid = 0\n    fail_reason = \"\"\n    for i_reac, (reactant, product) in enumerate(zip(reactants, products)):\n        is_valid, fail_reason = self._check_valid_step(\n            reactant,\n            product,\n            building_blocks_csi\n            + [p.csmiles for step in products[:i_reac] for p in step],\n            reactions,\n        )\n        if not is_valid:\n            self.logger.info(f\"Invalid step at index {i_reac} due to {fail_reason}\")\n            break\n        else:\n            n_valid += 1\n    if n_valid &lt; n_steps:\n        return reward_mult[n_valid - 1] * (n_valid / n_steps) ** 2, {\n            \"valid\": n_valid / n_steps,\n            \"correct_product\": reward_mult[n_valid - 1],\n            \"correct_reactant\": fail_reason != \"reactants\",\n        }\n\n    return reward_mult[n_valid - 1], {\n        \"valid\": 1.0,\n        \"correct_product\": reward_mult[n_valid - 1],\n        \"correct_reactant\": True,\n    }\n</code></pre>"},{"location":"api/reward/reaction_verifier/#mol_gen_docking.reward.verifiers.reaction_reward.reaction_verifier.ReactionVerifier.reward_smarts","title":"<code>reward_smarts(answer, labels, reactants, product)</code>","text":"<p>Compute reward for SMARTS prediction tasks.</p> <p>Notes The answer must be contained in a JSON object with an \"answer\" key, which should be a SMARTS string representing the reaction. The reward is computed based on whether the proposed SMARTS can produce the expected product from the given reactants. A reward of 1.0 is given for an exact match with the ground truth SMARTS, 0.1 if the SMARTS is valid and produces the correct product, and 0.0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>answer</code> <code>Dict[str, Any]</code> <p>Model completion containing the SMARTS answer.</p> required <code>labels</code> <code>List[str]</code> <p>List containing the ground truth SMARTS string.</p> required <code>reactants</code> <code>List[str]</code> <p>List of reactant SMILES strings.</p> required <code>product</code> <code>str</code> <p>Expected product SMILES string.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (reward, metadata_dict) where metadata contains</p> <code>Dict[str, Any]</code> <p>'Reactants_contained' and 'Products_contained' flags.</p> Source code in <code>mol_gen_docking/reward/verifiers/reaction_reward/reaction_verifier.py</code> <pre><code>def reward_smarts(\n    self,\n    answer: Dict[str, Any],\n    labels: List[str],\n    reactants: List[str],\n    product: str,\n) -&gt; Tuple[float, Dict[str, Any]]:\n    \"\"\"Compute reward for SMARTS prediction tasks.\n\n    Notes\n    The answer must be contained in a JSON object with an \"answer\" key,\n    which should be a SMARTS string representing the reaction. The reward is computed based\n    on whether the proposed SMARTS can produce the expected product from the given reactants.\n    A reward of 1.0 is given for an exact match with the ground truth SMARTS, 0.1 if the SMARTS\n    is valid and produces the correct product, and 0.0 otherwise.\n\n    Args:\n        answer: Model completion containing the SMARTS answer.\n        labels: List containing the ground truth SMARTS string.\n        reactants: List of reactant SMILES strings.\n        product: Expected product SMILES string.\n\n    Returns:\n        Tuple of (reward, metadata_dict) where metadata contains\n        'Reactants_contained' and 'Products_contained' flags.\n    \"\"\"\n    if answer == {}:\n        return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n    gt_smarts = labels[0]\n    smarts_pred = answer[\"answer\"]\n    if not isinstance(smarts_pred, str):\n        return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n\n    if smarts_pred.strip() == gt_smarts:\n        return 1.0, {\"Reactants_contained\": True, \"Products_contained\": True}\n    self.logger.info(\n        f\"Proposed SMARTS: {smarts_pred.strip()} | GT SMARTS: {gt_smarts}, checking reaction...\"\n    )\n    try:\n        rxnB = Reaction(smarts_pred.strip())\n        if rxnB.num_reactants != len(reactants):\n            return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n        p = rxnB([Molecule(r) for r in reactants])\n        reward = 0.0\n        if Molecule(product).csmiles in {prod.csmiles for prod in p}:\n            reward = 0.1\n        return reward, {\n            \"Reactants_contained\": True,\n            \"Products_contained\": reward == 0.1,\n        }\n    except Exception as e:\n        self.logger.info(\n            f\"Error in reaction SMARTS parsing: {e} (proposed: {smarts_pred} | gt: {gt_smarts})\"\n        )\n        return 0.0, {\"Reactants_contained\": False, \"Products_contained\": False}\n</code></pre>"},{"location":"api/reward/reaction_verifier/#related","title":"Related","text":"<ul> <li>Molecular Verifier - Main orchestrator</li> <li>Generation Verifier - De novo generation tasks</li> <li>Molecular Property Verifier - Molecular property prediction tasks</li> </ul>"},{"location":"datasets/generation/","title":"De Novo Molecular Generation Tasks","text":""},{"location":"datasets/generation/#overview","title":"Overview","text":"<p>The primary task in our benchmark is de novo molecular generation, which involves generating novel molecular compounds while optimizing a set of desirable properties. This task is fundamental to drug discovery, where the goal is to identify candidate molecules that satisfy multiple criteria simultaneously.</p> <p>In this setting, no high-scoring molecule is known a priori\u2014the generative model must identify drug candidates with no prior information on which molecules are promising. The task is inherently multi-objective: generated candidates must simultaneously satisfy multiple criteria, such as:</p> <ul> <li>Binding affinity: The ability to bind to a specific protein target</li> <li>Drug-likeness: Compliance with rules that predict good pharmacokinetic properties</li> <li>Synthetic accessibility: The feasibility of synthesizing the molecule in a laboratory</li> </ul>"},{"location":"datasets/generation/#dataset-statistics","title":"Dataset Statistics","text":"Split Number of Prompts Description Training 49,000 Multi-objective optimization prompts Evaluation 1,000 Held-out prompts for validation (binding targets are OOD, i.e not present in training set) Test 1,000 Final evaluation (binding targets are OOD, i.e not present in training set or evaluation set) <p>The dataset is well-balanced across optimization complexity:</p> <ul> <li>Single-property optimization: 29% of prompts</li> <li>Two-property optimization: 36% of prompts</li> <li>Three-property optimization: 35% of prompts</li> </ul> <p></p>"},{"location":"datasets/generation/#docking-simulations","title":"Docking Simulations","text":""},{"location":"datasets/generation/#what-is-molecular-docking","title":"What is Molecular Docking?","text":"<p>(Roy, Aritra (2021). Molecular Docking First Try.gif. figshare. https://doi.org/10.6084/m9.figshare.14676669.v1)</p> <p>Molecular docking is a computational technique that predicts the preferred orientation of a small molecule (ligand) when bound to a protein target. The docking score quantifies the strength of this interaction\u2014lower scores indicate better binding affinity.</p> <p>Docking is a standard computational tool in drug discovery to evaluate how well a compound fits into a protein's binding pocket, making it an ideal objective for molecular generation tasks.</p>"},{"location":"datasets/generation/#protein-structure-sources","title":"Protein Structure Sources","text":"<p>We extracted protein structures from two main sources:</p>"},{"location":"datasets/generation/#sair-dataset","title":"SAIR Dataset","text":"<p>The SAIR dataset consists of over 1 million protein-ligand pairs from approximately 5,000 unique protein structures. This dataset was built upon the structural predictions of Boltz1, a co-folding AI model, and notably contains predicted protein structures for which no known experimental structure exists.</p>"},{"location":"datasets/generation/#siu-dataset","title":"SIU Dataset","text":"<p>We complemented the SAIR structures with proteins from the SIU dataset, which provides labeled binding pockets.</p>"},{"location":"datasets/generation/#pocket-identification","title":"Pocket Identification","text":"<p>In the SAIR dataset, binding pockets are not pre-defined. To identify binding pockets for each protein structure, we performed a clustering of the protein's residues based on the predicted receptor-ligand structure. The pipeline:</p> <ol> <li>Filters structures to retain only high-quality ligand poses (top 50% by pIC50 potency and confidence score)</li> <li>Identifies pocket residues by finding the closest protein residues to each ligand atom</li> <li>Clusters pockets across multiple conformations using IoU-based similarity</li> <li>Selects representative conformations by minimizing pairwise RMSD across pocket residues</li> </ol> <p>After processing all structures with Meeko, we obtained 4,500 protein structures, each associated with a unique UniProt ID.</p>"},{"location":"datasets/generation/#description-of-molecular-properties-to-be-optimized","title":"Description of Molecular Properties to be Optimized","text":"<p>In addition to docking scores, our benchmark includes 12 classical molecular properties computed using RDKit. These properties are essential for evaluating drug-likeness and pharmacokinetic characteristics.</p>"},{"location":"datasets/generation/#drug-likeness-properties","title":"Drug-likeness Properties","text":"<ul> <li> <p> QED (Quantitative Estimate of Drug-likeness)</p> <p>A composite score combining multiple molecular properties to estimate how \"drug-like\" a molecule is. QED ranges from 0 to 1, with higher values indicating better drug-likeness.</p> <p>Typical objective: Maximize or keep above a threshold (e.g., &gt; 0.5)</p> </li> <li> <p> SA Score (Synthetic Accessibility)</p> <p>Estimates how difficult a molecule is to synthesize, ranging from 1 (easy) to 10 (difficult). The score considers fragment contributions and complexity penalties.</p> <p>Typical objective: Minimize or keep below a threshold (e.g., &lt; 4)</p> </li> </ul>"},{"location":"datasets/generation/#physicochemical-properties","title":"Physicochemical Properties","text":"<ul> <li> <p> LogP (Partition Coefficient)</p> <p>The logarithm of the octanol-water partition coefficient, measuring lipophilicity. Optimal values for oral drugs typically range from 1 to 5 (Lipinski's Rule of Five).</p> <p>Typical objective: Keep within a range or minimize (to improve solubility)</p> </li> <li> <p> Exact Molecular Weight</p> <p>The molecular mass in Daltons. Drug-like molecules typically have molecular weights below 500 Da.</p> <p>Typical objective: Keep below a threshold (e.g., &lt; 500 Da)</p> </li> <li> <p> TPSA (Topological Polar Surface Area)</p> <p>The surface area occupied by polar atoms (N, O, and their attached hydrogens). TPSA affects membrane permeability and oral absorption.</p> <p>Typical objective: Keep below a threshold (e.g., &lt; 140 \u00c5\u00b2 for oral bioavailability)</p> </li> </ul>"},{"location":"datasets/generation/#structural-properties","title":"Structural Properties","text":"<ul> <li> <p> Number of Hydrogen Bond Acceptors (NumHBA)</p> <p>Atoms capable of accepting hydrogen bonds. Lipinski's Rule suggests \u2264 10 acceptors for oral drugs.</p> </li> <li> <p> Number of Hydrogen Bond Donors (NumHBD)</p> <p>Atoms capable of donating hydrogen bonds. Lipinski's Rule suggests \u2264 5 donors for oral drugs.</p> </li> <li> <p> Number of Rotatable Bonds (NumRotatableBonds)</p> <p>Bonds that allow free rotation, affecting molecular flexibility. Fewer rotatable bonds (\u2264 10) generally improve oral bioavailability.</p> </li> <li> <p> Number of Aromatic Rings (NumAromaticRings)</p> <p>Count of aromatic ring systems. Too many aromatic rings can reduce solubility and increase toxicity risk.</p> </li> <li> <p> Fraction of sp\u00b3 Carbons (FractionCSP3)</p> <p>The ratio of sp\u00b3-hybridized carbons to total carbons. Higher values indicate more three-dimensionality, which is associated with better selectivity and fewer off-target effects.</p> </li> </ul>"},{"location":"datasets/generation/#shape-and-complexity-descriptors","title":"Shape and Complexity Descriptors","text":"<ul> <li> <p> Hall-Kier Alpha</p> <p>A molecular connectivity index describing molecular shape and branching.</p> </li> <li> <p> Phi (Flexibility Index)</p> <p>A descriptor related to molecular flexibility derived from the Kier shape indices.</p> </li> </ul>"},{"location":"datasets/generation/#property-distribution-in-dataset","title":"Property Distribution in Dataset","text":"<p>Properties are sampled with varying probabilities to encourage generation of feasible, drug-like molecules:</p> Property Relative Frequency Allowed Objectives QED High (7.0) Maximize, Above threshold SA Score High (3.0) Minimize, Below threshold LogP Medium (2) Above or Below threshold, Minimize, Maximize Phi Medium (0.5) Above or Below threshold, Minimize, Maximize Exact Molecular Weight Low (0.8) Above or Below threshold TPSA Low (0.6) Above or Below threshold Other properties Low (0.3-0.5) Various"},{"location":"datasets/generation/#reward-function","title":"Reward Function","text":"<p>The reward for molecular generation is computed as the geometric mean of per-property rewards:</p> \\[R(q, \\hat{y}) = \\left(\\prod_{i=1}^{n} r_i\\right)^{1/n}\\] <p>Where \\(r_i\\) is the reward for the \\(i\\)-th property objective and \\(n\\) is the total number of properties. Using the geometric mean ensures that only completions effectively optimizing all properties are rewarded\u2014a single per-property reward of 0 results in a total reward of 0.</p>"},{"location":"datasets/generation/#per-property-reward-functions","title":"Per-Property Reward Functions","text":"<p>The reward for each individual property depends on its optimization objective. We call \\(\\rho\\) the normalized property value, scaled to [0, 1] of a generated molecule.</p> <ul> <li> <p> Maximize Objective</p> \\[R = \\rho \\in [0, 1]\\] <p>The reward is simply the normalized property value. Higher property values receive higher rewards.</p> </li> <li> <p> Minimize Objective</p> \\[R = 1 - \\rho\\] <p>The reward is the inverse of the normalized property value. Lower property values receive higher rewards.</p> </li> <li> <p> Above Threshold</p> \\[R = \\begin{cases} 1 &amp; \\text{if } \\rho \\geq \\text{threshold} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Binary reward: full reward if the property meets or exceeds the threshold, zero otherwise.</p> </li> <li> <p> Below Threshold</p> \\[R = \\begin{cases} 1 &amp; \\text{if } \\rho \\leq \\text{threshold} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Binary reward: full reward if the property is at or below the threshold, zero otherwise.</p> </li> </ul> <p>Invalid Molecules</p> <p>If the generated SMILES string is invalid or cannot be parsed, the reward is automatically set to 0.</p>"},{"location":"datasets/main/","title":"Data Access","text":""},{"location":"datasets/main/#overview","title":"Overview","text":"<p>MolGenDocking addresses the challenge of de novo molecular generation, with a benchmark designed for Large Language Models (LLMs) and other generative architectures. Our dataset currently supports 3 downstream tasks:</p> Dataset Size Source Purpose De Novo Molecular Generation ~50k prompts SAIR and SIU Generate molecules optimizing a set of up to three properties Molecular Property prediction ~50k prompts Polaris Predict the properties of a molecule (regression + classification) RetroSynthesis Tasks ~50k reactions Enamine Retro-synthesis planning, reactants, products, SMARTS prediction"},{"location":"datasets/main/#downloading-datasets","title":"Downloading Datasets","text":"<p>Our dataset is available on huggingface. The three tasks are separated into three compressed folders: <code>molgendata.tar.gz</code>, <code>property_predictions.tar.gz</code>, and <code>synthesis_tasks.tar.gz</code>. HF datasets are also available, but do not include the metadatas necessary for the reward computation, and are thus only meant for inference.</p> <pre><code># Extract tarball datasets\ntar -xzf molgendata.tar.gz\ntar -xzf property_prediction.tar.gz\ntar -xzf synthesis_tasks.tar.gz\n</code></pre> <p>Warning</p> <p>To perform de novo molecular generation with docking constraints, the path to the <code>molgendata</code> folder should be provided to the reward server via the <code>DATA_PATH</code> environment variable. (See Reward Server Configuration for more details.)</p>"},{"location":"datasets/main/#data-organization","title":"Data Organization","text":"Molecular Generation DataProperty Prediction DataRetro-Synthesis Data <pre><code>molgendata/              # Docking targets\n\u251c\u2500\u2500 docking_targets.json\n\u251c\u2500\u2500 names_mapping.json\n\u251c\u2500\u2500 pockets_info.json\n\u251c\u2500\u2500 pdb_files/\n\u251c\u2500\u2500 train_data/\n\u2502   \u2514\u2500\u2500 train_prompts.jsonl\n\u2502   \u2514\u2500\u2500 train_prompts_boxed.jsonl\n\u251c\u2500\u2500 eval_data/\n\u2502   \u2514\u2500\u2500 eval_prompts_ood_boxed.jsonl\n\u2502   \u2514\u2500\u2500 eval_prompts_ood.jsonl\n\u2514\u2500\u2500 test_data/\n    \u2514\u2500\u2500 test_prompts_ood_boxed.jsonl\n    \u2514\u2500\u2500 test_prompts_ood.jsonl\n</code></pre> <pre><code>property_prediction/               # Reaction data\n\u251c\u2500\u2500 train_prompts_boxed.jsonl\n\u2514\u2500\u2500 eval_prompts_ood_boxed.jsonl\n</code></pre> <pre><code>synthesis_tasks/                 # Multi-task benchmarks\n\u251c\u2500\u2500 train_prompts.jsonl\n\u2514\u2500\u2500 eval_prompts\n    \u2514\u2500\u2500 eval_prompts_[chembl/enamine]_[#building blocks].jsonl\n</code></pre>"},{"location":"datasets/main/#data-format","title":"Data Format","text":"<p>Our data are stored in JSONL format, represented by a pydantic base model (source):</p> <pre><code>class Message(BaseModel):\n    \"\"\"Represents a single message in a conversation\"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n    meta: Dict[str, Any] = Field(default_factory=dict)\n    identifier: Optional[str] = None\n    multimodal_document: Optional[Dict[str, Any]] = None\n\n\nclass Conversation(BaseModel):\n    \"\"\"Complete conversation structure\"\"\"\n\n    meta: Dict[\n        str, Any\n    ]\n    messages: List[Message]\n    system_prompt: Optional[str] = None\n    available_tools: Optional[List[str]] = None\n    truncate_at_max_tokens: Optional[int] = None\n    truncate_at_max_image_tokens: Optional[int] = None\n    output_modalities: Optional[List[str]] = None\n    identifier: str\n    references: List[Any] = Field(default_factory=list)\n    rating: Optional[float] = None\n    source: Optional[str] = None\n    training_masks_strategy: str\n    custom_training_masks: Optional[Dict[str, Any]] = None\n\n\nclass Sample(BaseModel):\n    \"\"\"Root model containing all conversations\"\"\"\n\n    identifier: str\n    conversations: List[Conversation]\n    trajectories: List[Any] = Field(default_factory=list)\n    meta: Dict[str, Any] = Field(default_factory=dict)\n    source: Optional[str] = None\n</code></pre> <p>JSONL Format</p> <p>Each line in a JSONL file represents a <code>Sample</code> object with the following structure:</p> Molecular Generation Sample ExampleProperty Prediction Sample ExampleRetro-Synthesis Sample Example <pre><code>{\n  \"identifier\": \"sample_001\",\n  \"conversations\": [\n    {\n      \"meta\": {\n        \"properties\": [\"QED\", ...],\n        \"objectives\": [\"above\", ...],\n        \"target\": [0.5, ...],\n        ... # Additional metadata such as pocket box, target information, etc.\n      },\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a molecular generation assistant...\",\n          \"meta\": {}\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Generate a molecule that binds to GSK3B with high affinity\",\n          \"meta\": {},\n          \"identifier\": \"msg_001\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"identifier\": \"id0\",\n  \"conversations\": [\n    {\n      \"meta\": {\n        \"properties\": [\"biogen/adme-fang-hclint-reg-v1\"],\n        \"objectives\": [\"regression\"],\n        \"target\": [0.67],\n        \"smiles\": [\"Brc1ccc(-c2nnc(Cn3cnc4ccccc43)o2)o1\"],\n        \"norm_var\": 0.62,\n        ... # Additional metadata\n      },\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a property prediction assistant...\",\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Predict the clearance value for the molecule with SMILES: Brc1ccc(-c2nnc(Cn3cnc4ccccc43)o2)o1\",\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <pre><code>{\n  \"identifier\": \"id0\",\n  \"conversations\": [\n    {\n      \"meta\": {\n        \"properties\": [\"full_path_bb_ref\"],\n        \"objectives\": [\"full_path_bb_ref\"],\n        \"target\": [\"CCCC[C@H](NC(=O)[C@H](CCCCN)NC(=O)[C@H](CCCNC(=N)N)NC(=O)c1ccc(/C=C2\\\\SC(=O)N(c3ccc(C)cc3)C2=O)cc1)C(N)=O\"],\n        ... # Additional metadata\n      },\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a retrosynthesis planning assistant...\",\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Given the product molecule with SMILES: CC(=O)OC1=CC=CC=C1C(=O)O, provide the reactants used in its synthesis.\",\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"datasets/prop_prediction/","title":"Property Prediction Datasets","text":"<p>This page describes the molecular property prediction datasets used in our benchmark, extracted from the Polaris Hub. These datasets cover a wide range of ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties and target-specific bioactivity predictions.</p>"},{"location":"datasets/prop_prediction/#overview","title":"Overview","text":"Dataset Task Type Property ASAP Discovery - MERS-CoV Mpro Regression Antiviral Potency AstraZeneca LogD Regression Lipophilicity AstraZeneca PPB Clearance Regression Plasma Protein Binding Novartis CYP3A4 Regression CYP Inactivation PKIS2 Kinase Inhibition Classification Kinase Inhibition Biogen ADME Regression Various ADME Properties TDCommons Mixed Various ADMET Properties"},{"location":"datasets/prop_prediction/#assay-description","title":"Assay description","text":"<ul> <li> <p> ASAP Discovery - MERS-CoV Mpro</p> <p>Source: <code>asap-discovery/antiviral-potency-2025-unblinded</code></p> <p>Task Type: Regression</p> <p>Target Property: pIC50 against MERS-CoV main protease</p> <p>This dataset contains compounds tested for their inhibitory activity against the MERS-CoV main protease (Mpro), essential for viral replication. Main proteases are highly conserved across coronaviruses, making them attractive targets for broad-spectrum antiviral development.</p> </li> <li> <p> AstraZeneca LogD</p> <p>Source: <code>polaris/az-logd-74-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Octan-1-ol/water (pH 7.4) distribution coefficient (LogD)</p> <p>LogD at pH 7.4 measures lipophilicity under physiological conditions, accounting for ionization state. Optimal values (1-3) are associated with good oral bioavailability and CNS penetration.</p> </li> <li> <p> AstraZeneca PPB Clearance</p> <p>Source: <code>polaris/az-ppb-clearance-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log percent of compound unbound to whole human plasma</p> <p>Plasma protein binding measures how much drug binds to plasma proteins (mainly albumin). Only the unbound fraction is pharmacologically active. High binding affects distribution, clearance, and drug-drug interactions.</p> </li> <li> <p> Novartis CYP3A4</p> <p>Source: <code>novartis/novartis-cyp3a4-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-inactivation rate constant (log kobs) of CYP enzymes</p> <p>Measures time-dependent inhibition of CYP3A4, the most abundant liver enzyme metabolizing ~50% of marketed drugs. CYP3A4 inhibition can cause serious drug-drug interactions.</p> </li> <li> <p> PKIS2 - EGFR</p> <p>Source: <code>polaris/drewry2017-pkis2-subset-v2</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of the EGFR kinase</p> <p>EGFR is a receptor tyrosine kinase involved in cell proliferation. EGFR inhibitors are used in cancer therapy for non-small cell lung cancer and colorectal cancer.</p> </li> <li> <p> PKIS2 - KIT</p> <p>Source: <code>polaris/drewry2017-pkis2-subset-v2</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of the KIT kinase</p> <p>KIT plays a role in cell survival and proliferation. KIT inhibitors treat gastrointestinal stromal tumors (GIST) and certain leukemias.</p> </li> <li> <p> PKIS2 - RET</p> <p>Source: <code>polaris/drewry2017-pkis2-subset-v2</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of the RET kinase</p> <p>RET is involved in cell growth and differentiation. RET inhibitors are approved for RET-fusion positive cancers and medullary thyroid carcinoma.</p> </li> <li> <p> PKIS2 - LOK</p> <p>Source: <code>polaris/drewry2017-pkis2-subset-v2</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of the LOK kinase</p> <p>LOK (STK10) is a serine/threonine kinase involved in lymphocyte migration and immune cell function.</p> </li> <li> <p> PKIS2 - SLK</p> <p>Source: <code>polaris/drewry2017-pkis2-subset-v2</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of the SLK kinase</p> <p>SLK is involved in cell cycle regulation, apoptosis, and cytoskeletal organization.</p> </li> <li> <p> Solubility</p> <p>Source: <code>biogen/adme-fang-solu-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-solubility</p> <p>Aqueous solubility affects drug absorption and formulation. Poor solubility is a major cause of drug development failures.</p> </li> <li> <p> Rat Plasma Protein Binding</p> <p>Source: <code>biogen/adme-fang-rppb-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-rat plasma protein binding rate</p> <p>Fraction of compound bound to rat plasma proteins, useful for preclinical pharmacokinetic studies.</p> </li> <li> <p> Human Plasma Protein Binding</p> <p>Source: <code>biogen/adme-fang-hppb-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-human plasma protein binding rate</p> <p>Fraction bound to human plasma proteins, critical for clinical pharmacokinetic predictions.</p> </li> <li> <p> Permeability (MDR1-MDCK)</p> <p>Source: <code>biogen/adme-fang-perm-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-MDR1 MDCK efflux ratio</p> <p>Measures P-glycoprotein mediated efflux. P-gp substrates may have limited brain penetration and variable oral bioavailability.</p> </li> <li> <p> Human Liver Microsomal Stability</p> <p>Source: <code>biogen/adme-fang-hclint-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-human liver microsomal stability (CLint)</p> <p>Intrinsic clearance predicts hepatic metabolic stability. Higher values indicate faster metabolism affecting exposure and dosing.</p> </li> <li> <p> Rat Liver Microsomal Stability</p> <p>Source: <code>biogen/adme-fang-rclint-reg-v1</code></p> <p>Task Type: Regression</p> <p>Target Property: Log-rat liver microsomal stability (CLint)</p> <p>Intrinsic clearance in rat liver microsomes for preclinical species selection and PK prediction.</p> </li> <li> <p> P-glycoprotein Inhibition</p> <p>Source: <code>tdcommons/pgp-broccatelli</code></p> <p>Task Type: Classification</p> <p>Target Property: Inhibitor of P-glycoprotein (P-gp)</p> <p>P-gp is an efflux transporter. P-gp inhibitors can enhance brain penetration but may cause drug-drug interactions.</p> </li> <li> <p> Blood-Brain Barrier Penetration</p> <p>Source: <code>tdcommons/bbb-martins</code></p> <p>Task Type: Classification</p> <p>Target Property: Ability to penetrate the blood-brain barrier (BBB)</p> <p>Critical for CNS drug development. BBB penetration is necessary for brain-targeting drugs but should be avoided for peripheral drugs.</p> </li> <li> <p> Caco-2 Permeability</p> <p>Source: <code>tdcommons/caco2-wang</code></p> <p>Task Type: Regression</p> <p>Target Property: Rate of compounds passing through Caco-2 cells</p> <p>Caco-2 cells model intestinal absorption. Low permeability often correlates with poor oral bioavailability.</p> </li> <li> <p> Volume of Distribution</p> <p>Source: <code>tdcommons/vdss-lombardo</code></p> <p>Task Type: Regression</p> <p>Target Property: Volume of distribution at steady state (Vdss)</p> <p>High Vdss indicates extensive tissue distribution; low Vdss suggests the drug remains in plasma. Affects dosing and accumulation.</p> </li> <li> <p> Half-Life</p> <p>Source: <code>tdcommons/half-life-obach</code></p> <p>Task Type: Regression</p> <p>Target Property: Duration for drug concentration to be reduced by half</p> <p>Short half-life requires frequent dosing; long half-life may lead to accumulation.</p> </li> <li> <p> Hepatocyte Clearance</p> <p>Source: <code>tdcommons/clearance-hepatocyte-az</code></p> <p>Task Type: Regression</p> <p>Target Property: Drug clearance measured in hepatocytes</p> <p>More physiologically relevant than microsomes as it includes Phase I and Phase II metabolism.</p> </li> <li> <p> Microsome Clearance</p> <p>Source: <code>tdcommons/clearance-microsome-az</code></p> <p>Task Type: Regression</p> <p>Target Property: Drug clearance measured in microsomes</p> <p>Primarily reflects CYP-mediated Phase I metabolism.</p> </li> <li> <p> Lipophilicity</p> <p>Source: <code>tdcommons/lipophilicity-astrazeneca</code></p> <p>Task Type: Regression</p> <p>Target Property: Lipophilicity (LogD)</p> <p>Affects membrane permeability, protein binding, metabolism, and overall pharmacokinetics.</p> </li> <li> <p> Drug-Induced Liver Injury (DILI)</p> <p>Source: <code>tdcommons/dili</code></p> <p>Task Type: Classification</p> <p>Target Property: Potential to induce liver injuries</p> <p>DILI is a major cause of drug withdrawal. Hepatotoxicity is a leading cause of clinical trial failures.</p> </li> <li> <p> hERG Inhibition</p> <p>Source: <code>tdcommons/herg</code></p> <p>Task Type: Classification</p> <p>Target Property: Blocker of hERG channel</p> <p>hERG inhibition can cause QT prolongation and fatal cardiac arrhythmias. Screening is mandatory in drug development.</p> </li> <li> <p> Ames Mutagenicity</p> <p>Source: <code>tdcommons/ames</code></p> <p>Task Type: Classification</p> <p>Target Property: Mutagenic potential (Ames test positive/negative)</p> <p>Mutagenic compounds are potential carcinogens. Positive Ames test often disqualifies compounds from development.</p> </li> <li> <p> Acute Toxicity (LD50)</p> <p>Source: <code>tdcommons/ld50-zhu</code></p> <p>Task Type: Regression</p> <p>Target Property: Acute toxicity LD50 (lethal dose for 50% of test animals)</p> <p>Provides initial safety assessment and helps establish safe starting doses.</p> </li> <li> <p> CYP2C9 Substrate</p> <p>Source: <code>tdcommons/cyp2c9-substrate-carbonmangels</code></p> <p>Task Type: Classification</p> <p>Target Property: Substrate of CYP2C9</p> <p>CYP2C9 metabolizes ~15% of drugs including warfarin and NSAIDs.</p> </li> <li> <p> CYP2D6 Substrate</p> <p>Source: <code>tdcommons/cyp2d6-substrate-carbonmangels</code></p> <p>Task Type: Classification</p> <p>Target Property: Substrate of CYP2D6</p> <p>CYP2D6 is highly polymorphic, affecting ~25% of drugs. Genetic variations lead to poor, intermediate, extensive, and ultra-rapid metabolizer phenotypes.</p> </li> <li> <p> CYP3A4 Substrate</p> <p>Source: <code>tdcommons/cyp3a4-substrate-carbonmangels</code></p> <p>Task Type: Classification</p> <p>Target Property: Substrate of CYP3A4</p> <p>CYP3A4 is the most important drug-metabolizing enzyme, processing ~50% of drugs.</p> </li> <li> <p> Solubility (AqSolDB)</p> <p>Source: <code>tdcommons/solubility-aqsoldb</code></p> <p>Task Type: Regression</p> <p>Target Property: Aqueous solubility</p> <p>Data from AqSolDB, one of the largest curated aqueous solubility datasets. Essential for drug absorption and formulation.</p> </li> </ul>"},{"location":"datasets/prop_prediction/#reward-functions","title":"Reward Functions","text":"<p>Property prediction tasks use different reward functions depending on whether the task is regression or classification:</p> <ul> <li> <p> Regression Reward</p> \\[R = 1 - \\frac{(\\hat{y} - y)^2}{\\sigma^2}\\] <p>Where \\(\\hat{y}\\) is the predicted value, \\(y\\) is the ground truth, and \\(\\sigma\\) is the standard deviation of training labels. This normalizes prediction errors and ensures rewards are in [0, 1].</p> <p>Training samples: ~44,000</p> </li> <li> <p> Classification Reward</p> \\[R = \\mathbb{1}_{pred = label}\\] <p>Binary reward: 1 if the prediction exactly matches the ground truth label, 0 otherwise.</p> <p>Training samples: ~11,000</p> </li> </ul> <p>Invalid Predictions</p> <p>If the model generates an invalid or unparseable prediction, the reward is automatically set to 0.</p>"},{"location":"datasets/prop_prediction/#references","title":"References","text":"<ol> <li>ASAP Discovery Consortium. Antiviral Potency Dataset (2025).</li> <li>Drewry, D.H., et al. \"Progress towards a public chemogenomic set for protein kinases and a call for contributions.\" PLOS ONE (2017).</li> <li>Lombardo, F., et al. \"Trend Analysis of a Database of Intravenous Pharmacokinetic Parameters in Humans for 670 Drug Compounds.\" Drug Metabolism and Disposition (2008).</li> <li>Martins, I.F., et al. \"A Bayesian Approach to in Silico Blood-Brain Barrier Penetration Modeling.\" Journal of Chemical Information and Modeling (2012).</li> <li>Therapeutics Data Commons: https://tdcommons.ai/</li> <li>Polaris Hub: https://polarishub.io/</li> </ol>"},{"location":"datasets/retro_synthesis/","title":"Retro-Synthesis Tasks","text":"<p>This page describes the chemical reaction and retro-synthesis datasets used in our benchmark. These tasks investigate the influence of synthesis knowledge on molecular generation, helping models learn to generate compounds that are both optimized and synthetically accessible.</p>"},{"location":"datasets/retro_synthesis/#overview","title":"Overview","text":"Split Size Description Training 50,000 reactions Multi-step synthesis routes with various task types Test (ChEMBL) 1,000 molecules Real-world synthesis prediction Test (Enamine) 1,000 molecules Real-world synthesis prediction"},{"location":"datasets/retro_synthesis/#task-distribution","title":"Task Distribution","text":"<p>The training dataset includes four main task types:</p> Task Type Proportion Description Retro-synthesis Planning 59% Predict complete multi-step synthesis pathways Reactant Prediction 20% Identify missing reactants for a reaction step SMARTS Prediction 9% Predict the reaction template (SMARTS notation) Product Prediction 12% Predict the final product of a multi-step synthesis"},{"location":"datasets/retro_synthesis/#synthesis-complexity","title":"Synthesis Complexity","text":"<p>The dataset contains reactions of varying complexity:</p> <ul> <li>Single-step reactions: ~24% of dataset</li> <li>Two-step reactions: ~34% of dataset</li> <li>Multi-step reactions (3-5 steps): ~42% of dataset</li> </ul>"},{"location":"datasets/retro_synthesis/#data-generation-pipeline","title":"Data Generation Pipeline","text":"<p>We follow a methodology that employs building blocks from the Enamine catalog and 115 chemical reaction templates described in SMARTS notation to generate multi-step reactions.</p> <p>References</p> <p>This data generation approach is derived by:</p> <pre><code>1. Lee et al., \"Rethinking Molecule Synthesizability with Chain-of-Reaction.\" (2025)\n2. Gao et al., \"Generative Artificial Intelligence for Navigating Synthesizable Chemical Space.\" (2024)\n</code></pre> <p>by using their proposed Reactant-Reaction Matrix.</p>"},{"location":"datasets/retro_synthesis/#multi-step-synthesis-generation","title":"Multi-Step Synthesis Generation","text":"<p>We generate synthetic pathways through an iterative stochastic process:</p> <ul> <li> <p> 1. Initialization</p> <p>Select a number of steps to sample for the synthesis pathway (1 to 5), and a random number of initialization steps.</p> <p>Select a random seed reaction and identify available reactants via the compatibility matrix. Sample up to 10 valid reactant combinations and apply the reaction using RDKit. Filter products based on physicochemical properties and atom count.</p> </li> <li> <p> 1. Initialization</p> <p>Select a random seed reaction and identify available reactants via the compatibility matrix. Sample up to 10 valid reactant combinations and apply the reaction using RDKit. Filter products based on physicochemical properties and atom count.</p> </li> <li> <p> 2. Probabilistic Product Selection</p> <p>For each valid product, compute a probability score based on a target distribution over molecular properties (QED, molecular weight, TPSA, H-bond donors/acceptors, rotatable bonds, aromatic rings). Products are selected proportionally to these scores.</p> </li> <li> <p> 3. Chain Extension</p> <p>With up to 5 reaction steps, iteratively select a new reaction compatible with the last product, identify available reactant partners via the matrix, apply the reaction with property-based filtering, and add the product to the synthesis chain.</p> </li> <li> <p> 4. Termination</p> <p>Synthesis continues until the maximum number of steps is reached or no valid reactions can be applied. This ensures all pathways are chemically feasible.</p> </li> </ul>"},{"location":"datasets/retro_synthesis/#molecular-property-filtering","title":"Molecular Property Filtering","text":"<p>Products must satisfy strict physicochemical constraints to remain in the dataset, ensuring drug-like molecules:</p> Property Min Max QED (Drug-likeness) 0.30 1.00 Molecular Weight (Da) 0 600 TPSA (\u0172) 0 160 H-Bond Acceptors 0 10 H-Bond Donors 0 10 Rotatable Bonds 1 10 Aromatic Rings 0 6 Atom Count - 60"},{"location":"datasets/retro_synthesis/#target-distribution-modeling","title":"Target Distribution Modeling","text":"<p>Rather than using hard constraints alone, we compute log-probabilities for products via Beta distributions over normalized property ranges. This biases the stochastic selection toward drug-like molecules without rejecting valid synthetic products. The distribution parameters are tuned on the ZINC-250K dataset.</p>"},{"location":"datasets/retro_synthesis/#task-types","title":"Task Types","text":"<p>We created eleven distinct objective templates to train models on complementary synthesis reasoning tasks. These tasks are designed to showcase different levels of complexity hopefully leading the model to effectively acquire the necessary skills to generate a full synthesis pathway.</p>"},{"location":"datasets/retro_synthesis/#single-step-tasks","title":"Single-Step Tasks","text":"<ul> <li> <p> Final Product Prediction</p> <p>Predict the final product of a multi-step synthesis given the reaction sequence, and the last step's SMARTS template.</p> <p>Training samples: ~6k</p> </li> <li> <p> Reactant Prediction</p> <p>Identify a missing reactant for a single synthesis step given the product and another reactant.</p> <p>Training samples: ~2.7k</p> </li> <li> <p> All Reactants Prediction</p> <p>Given a reaction SMARTS and target product, predict all required reactants (always first step).</p> <p>Training samples:</p> <ul> <li>~2.6k with no additional information</li> <li>~4.5k with a set of building blocks provided</li> </ul> </li> <li> <p> SMARTS Identification</p> <p>Predict the SMARTS representation for a reaction step, given reactants and product.</p> <p>Training samples: ~3.6k</p> </li> </ul>"},{"location":"datasets/retro_synthesis/#multi-step-path-tasks","title":"Multi-Step / Path Tasks","text":"<ul> <li> <p> Full Synthesis Path</p> <p>Generate a complete multi-step synthesis pathway to a target molecule.</p> <p>Training samples:</p> <ul> <li>~5.7k with not additional information</li> <li>~5.8k with a set of SMARTS templates provided</li> <li>~5.8k with the 4, 8 or 16 most similar building blocks to the target molecule provided</li> <li>~2.9k with both SMARTS templates and most similar building blocks provided</li> </ul> </li> <li> <p> Full Path With Interm. Products</p> <p>Generate a complete multi-step synthesis pathway to a target molecule, given possible intermediate products to help guide the model.</p> <p>Training samples:</p> <ul> <li>~4.7k with not additional information</li> <li>~4.7k with a building blocks available (including the ones used in the synthesis)</li> </ul> </li> </ul>"},{"location":"datasets/retro_synthesis/#reward-functions","title":"Reward Functions","text":"<p>The reward functions for chemical reaction tasks are designed to progressively guide the model toward correct predictions:</p> <ul> <li> <p> Reactant/Product Prediction</p> \\[R = \\begin{cases} 1 &amp; \\text{if prediction is correct} \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] <p>Evaluates correctness by verifying if using the predicted reactants/products in the reaction yields the expected product/reactants.</p> </li> <li> <p> SMARTS Prediction</p> \\[R = \\frac{9 \\times \\mathbb{1}_{SMARTS_{pred} = SMARTS_{ref}} + \\mathbb{1}_{product\\_match}}{10}\\] <p>High reward for exact SMARTS match, small reward if applying the predicted SMARTS produces the correct product.</p> </li> <li> <p> Retro-Synthesis Planning</p> \\[R = \\left(\\frac{n_{valid}}{n}\\right)^2 \\times \\text{sim}(target, \\hat{y})^3\\] <p>Where \\(n_{valid}\\) is the number of valid steps, \\(n\\) is total steps, and \\(\\hat{y}\\) is the last valid product. Rewards increase with valid step proportion and Tanimoto similarity to target.</p> </li> </ul> <p>Invalid Predictions</p> <p>If the extracted answer is invalid (unparseable SMILES, invalid reaction), the reward is automatically set to 0.</p>"},{"location":"datasets/retro_synthesis/#evaluation","title":"Evaluation","text":""},{"location":"datasets/retro_synthesis/#test-sets","title":"Test Sets","text":"<p>Following established methodology, we evaluate on real-world synthesis prediction rather than synthetic data:</p> Test Set Size Description ChEMBL 1,000 molecules Drug-like molecules from the ChEMBL database Enamine 1,000 molecules Molecules from the Enamine catalog <p>For each molecule, we either:</p> <ol> <li>Directly prompt the model to predict the synthesis route</li> <li>Prompt the model to predict the synthesis route given a set of building blocks (4, 8, or 16 most similar to the target).</li> </ol>"},{"location":"datasets/retro_synthesis/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Model performance is evaluated based on:</p> <ul> <li>Success rate: Proportion of molecules successfully synthesized using predicted routes</li> <li>Tanimoto similarity: Similarity between target molecule and synthesized product (when synthesis fails)</li> <li>Valid step ratio: Proportion of chemically valid steps in predicted routes</li> </ul>"},{"location":"datasets/retro_synthesis/#references","title":"References","text":"<ol> <li>Lee, S., et al. \"Rethinking Molecule Synthesizability with Chain-of-Reaction.\" (2025)</li> <li>Gao, W., et al. \"Generative Artificial Intelligence for Navigating Synthesizable Chemical Space.\" (2024)</li> <li>Enamine Building Blocks Catalog: https://enamine.net/building-blocks/building-blocks-catalog</li> </ol>"},{"location":"reward_server/Server_Asynchronous/","title":"Async Reference","text":""},{"location":"reward_server/Server_Asynchronous/#mol_gen_docking.server_utils.buffer.RewardBuffer","title":"<code>RewardBuffer</code>","text":"Source code in <code>mol_gen_docking/server_utils/buffer.py</code> <pre><code>class RewardBuffer:\n    def __init__(\n        self,\n        app: Any,\n        buffer_time: float = 1.0,\n        max_batch_size: int = 1024,\n        server_mode: Literal[\"singleton\", \"batch\"] = \"singleton\",\n    ) -&gt; None:\n        \"\"\"Asynchronous buffer for batching and processing reward scoring requests.\n\n        This class implements a request buffering system that collects incoming molecular\n        verifier queries and processes them in optimized batches to maximize throughput\n        and reduce latency. It uses asyncio for non-blocking I/O and Ray for distributed\n        computation of reward scores.\n\n        The buffering strategy works as follows:\n\n        1. Requests are added to an asyncio queue without blocking the caller\n        2. A background task periodically flushes the queue or when it reaches max size\n        3. Multiple queries are merged into a single batch with aligned completions/metadata\n        4. Results are computed in parallel using Ray actors\n        5. Results are grouped back to individual queries and returned asynchronously\n\n        This approach provides several benefits:\n\n        - Amortizes overhead across multiple requests\n        - Allows Ray to optimize kernel launches on GPU\n        - Reduces context switching and memory fragmentation\n\n        Attributes:\n            app (Any): FastAPI/Starlette application instance containing Ray actors in\n                app.state.reward_model for distributed reward computation.\n\n            buffer_time (float): Maximum time in seconds to buffer requests before\n                processing. Trades off latency for batch efficiency. Setting this too high\n                increases latency; too low reduces batch efficiency.\n                Default: 1.0\n\n            max_batch_size (int): Maximum number of queries to process in a single batch.\n                Larger batches improve GPU utilization but increase per-request latency.\n                Should be tuned based on GPU memory and target latency.\n                Default: 1024\n\n            server_mode (Literal[\"singleton\", \"batch\"]): Server operation mode.\n\n                - \"singleton\": Each query contains a single completion to score.\n                - \"batch\": Each query can contain multiple completions to score.\n                Default: \"singleton\"\n\n\n        Note:\n            This class is thread-safe for asyncio but not for multi-threaded access.\n            All methods must be called from the same asyncio event loop.\n        \"\"\"\n        self.app = app\n        self.buffer_time = buffer_time\n        self.server_mode = server_mode\n        self.max_batch_size = max_batch_size\n        self.queue: deque[Tuple[MolecularVerifierServerQuery, asyncio.Future]] = deque()\n        self.lock = asyncio.Lock()\n        self.processing_task = asyncio.create_task(self._batch_loop())\n\n    async def add_query(\n        self, query: MolecularVerifierServerQuery\n    ) -&gt; MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse:\n        \"\"\"Add a query to the buffer and wait for its result asynchronously.\n\n        This method is non-blocking: it immediately queues the request and returns\n        an awaitable future. The actual computation happens asynchronously in the\n        background batch processing task. Multiple callers can call this method\n        concurrently without blocking each other.\n\n        The method uses an asyncio.Lock to ensure thread-safe queue access. If an\n        error occurs during queueing, the exception is logged and re-raised to the\n        caller.\n\n        Args:\n            query (MolecularVerifierServerQuery): A molecular verifier query containing:\n\n                - metadata: List of metadata dictionaries (one per completion)\n                - query: List of completion strings to score\n                - prompts: Optional list of original prompts for tracking\n\n        Returns:\n            MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse:\n                Response type depends on server_mode:\n\n                - In \"singleton\" mode: MolecularVerifierServerResponse containing:\n                    - reward: Single reward score\n                    - meta: Single metadata object with detailed scoring information\n                    - error: Error message if scoring failed\n                    - next_turn_feedback: Optional feedback for multi-turn conversations\n\n                - In \"batch\" mode: BatchMolecularVerifierServerResponse containing:\n                    - rewards: List of individual reward scores (one per completion)\n                    - metas: List of metadata objects (one per completion)\n                    - error: Error message if scoring failed\n                    - next_turn_feedback: Optional feedback for multi-turn conversations\n\n        Raises:\n            Exception: Any exception raised during queueing or during result\n                computation in the background task. The exception is logged\n                before being raised to the caller.\n        \"\"\"\n        try:\n            future: asyncio.Future = asyncio.get_event_loop().create_future()\n            async with self.lock:\n                self.queue.append((query, future))\n            return await future  # type:ignore\n        except Exception as e:\n            logger.error(f\"Error in add_query: {e}\")\n            raise e\n\n    async def _batch_loop(self) -&gt; None:\n        \"\"\"Background task that continuously processes buffered queries in batches.\n\n        This coroutine runs indefinitely in the background (started in __init__),\n        periodically waking up to process accumulated requests. It implements a\n        simple time-based flushing strategy: every buffer_time seconds, any pending\n        queries are processed as a batch.\n\n        The loop continues even if individual batch processing raises exceptions\n        (which are caught in _process_pending_queries). This ensures the buffer\n        remains responsive and recovers from transient errors.\n\n        Processing frequency:\n\n        - Minimum: Every buffer_time seconds\n        - Maximum: Immediate if batch reaches max_batch_size (handled in\n          _process_pending_queries)\n\n        Note:\n            This task should only be created once during __init__ and continues\n            until the application shuts down or the event loop exits.\n        \"\"\"\n        while True:\n            await asyncio.sleep(self.buffer_time)\n            await self._process_pending_queries()\n\n    async def _process_pending_queries(self) -&gt; None:\n        \"\"\"Process all pending queries in the buffer as one or more batches.\n\n        This method is called periodically by _batch_loop. It safely removes queries\n        from the queue (up to max_batch_size at a time) and calls _process_batch\n        to compute rewards in parallel. Results are then delivered to each query's\n        future, unblocking the corresponding add_query caller.\n\n        If the queue is empty, this method returns immediately without doing work.\n        If the queue has items, they are popped in chunks of max_batch_size and\n        processed separately.\n\n        Error handling:\n\n        - Exceptions during _process_batch are caught and set on all futures in\n          that batch, preventing one failure from affecting other batches\n        - The loop continues processing other batches even if one fails\n        - All errors are logged for debugging\n\n        Concurrency:\n\n        - Uses asyncio.Lock to safely access the queue without race conditions\n        - Futures are unblocked sequentially after batch completion\n\n        Returns:\n            None. Results are delivered via asyncio.Future.set_result() and\n            asyncio.Future.set_exception().\n\n        Raises:\n            Exceptions are caught and logged but not re-raised. They are instead\n            passed to futures via set_exception().\n        \"\"\"\n        try:\n            async with self.lock:\n                if not self.queue:\n                    return\n\n                batch = [\n                    self.queue.popleft()\n                    for _ in range(min(len(self.queue), self.max_batch_size))\n                ]\n\n            queries, futures = zip(*batch)\n            logger.info(f\"Processing batch of size {len(queries)}\")\n            try:\n                responses = await self._process_batch(list(queries))\n                for fut, res in zip(futures, responses):\n                    if not fut.done():\n                        fut.set_result(res)\n            except Exception as e:\n                for fut in futures:\n                    if not fut.done():\n                        fut.set_exception(e)\n        except Exception as e:\n            logger.error(f\"Error in _process_pending_queries: {e}\")\n            raise e\n\n    async def _process_batch(\n        self, queries: List[MolecularVerifierServerQuery]\n    ) -&gt; List[MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse]:\n        \"\"\"Process a batch of queries by merging, scoring, and grouping results.\n\n        This is the core batch processing pipeline. It combines multiple queries\n        into a single large batch, sends it to the Ray actor for parallel scoring,\n        and then groups results back to individual queries.\n\n        Pipeline steps:\n\n        1. **Merge inputs**: Concatenate all completions and metadata from all\n           queries. Track which query each item came from using query_indices.\n\n        2. **Handle empty batch**: Return error responses if no valid completions.\n\n        3. **Compute scores**: Send merged batch to Ray actor via remote call.\n           The actor returns BatchMolecularVerifierOutputModel with rewards and\n           metadata for each completion.\n\n        4. **Group by query**: Redistribute results back to original queries using\n           query_indices. Each query gets back only its own results.\n\n        5. **Aggregate metrics**: For each query, compute aggregate reward as the\n           average of its individual rewards. Pack metadata into structured objects.\n\n        Args:\n            queries (List[MolecularVerifierServerQuery]): A list of queries to\n                process together. Each query can contain multiple completions.\n                Typically 1-16 queries per batch.\n\n        Returns:\n            List[MolecularVerifierServerResponse]: One response per input query,\n\n                in the same order. Each response contains:\n                - reward: Average reward across all completions in that query\n                - reward_list: Individual rewards for each completion\n                - meta: Detailed metadata from the verifier for each completion\n                - error: Error message if batch processing failed\n\n        Raises:\n            Any exception raised by the Ray actor's get_score method is\n            propagated to the caller and will be set on all futures in\n            _process_pending_queries.\n        \"\"\"\n        app = self.app\n\n        # --- Step 1. Merge all batched inputs ---\n        all_completions: List[str] = []\n        all_metadata: List[dict[str, Any]] = []\n        query_indices: List[int] = []\n\n        for i, q in enumerate(queries):\n            all_completions.extend(q.query)\n            all_metadata.extend(q.metadata)\n            query_indices.extend([i] * len(q.query))\n\n        if len(all_completions) == 0:\n            # All failed early\n            return [\n                q\n                if isinstance(q, MolecularVerifierServerResponse)\n                else MolecularVerifierServerResponse(\n                    reward=0.0, reward_list=[], error=\"Empty batch\"\n                )\n                for q in queries\n            ]\n\n        # --- Step 2. Compute batched reward ---\n        reward_actor = app.state.reward_model\n\n        # Run in parallel\n        rewards_job = reward_actor.get_score.remote(\n            completions=all_completions, metadata=all_metadata\n        )\n\n        out = ray.get(rewards_job)\n        rewards = out.rewards\n        parsed_answers = out.parsed_answers\n        metadatas = out.verifier_metadatas\n        # --- Step 3. Group results by original query ---\n        grouped_results: List[List[float]] = [[] for _ in range(len(queries))]\n        grouped_meta: List[List[MolecularVerifierOutputMetadataModel]] = [\n            [] for _ in range(len(queries))\n        ]\n        grouped_parsed_answers: List[List[str]] = [[] for _ in range(len(queries))]\n\n        for r, m, p, idx in zip(rewards, metadatas, parsed_answers, query_indices):\n            grouped_results[idx].append(r)\n            grouped_meta[idx].append(m)\n            grouped_parsed_answers[idx].append(p)\n\n        # --- Step 4. Compute per-query metrics ---\n        responses: List[\n            MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse\n        ] = []\n        for i, q in enumerate(queries):\n            if isinstance(q, MolecularVerifierServerResponse):\n                # prefilled error\n                responses.append(q)\n                continue\n            rewards_i = grouped_results[i]\n            metadata_i = grouped_meta[i]\n            parsed_answers_i = grouped_parsed_answers[i]\n            # Transform metadata to pydantic models\n            server_metadata_i: List[MolecularVerifierServerMetadata] = [\n                MolecularVerifierServerMetadata.model_validate(m.model_dump())\n                for m in metadata_i\n            ]\n            for serv_m, p_answer in zip(server_metadata_i, parsed_answers_i):\n                serv_m.parsed_answer = p_answer\n\n            response: (\n                MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse\n            )\n            if self.server_mode == \"singleton\":\n                assert len(rewards_i) == 1, (\n                    \"Expected singleton mode to have one reward per query\"\n                )\n                response = MolecularVerifierServerResponse(\n                    reward=rewards_i[0],\n                    meta=server_metadata_i[0],\n                    error=None,\n                )\n            elif self.server_mode == \"batch\":\n                response = BatchMolecularVerifierServerResponse(\n                    rewards=rewards_i,\n                    metas=server_metadata_i,\n                    error=None,\n                )\n            responses.append(response)\n\n        return responses\n</code></pre>"},{"location":"reward_server/Server_Asynchronous/#mol_gen_docking.server_utils.buffer.RewardBuffer.__init__","title":"<code>__init__(app, buffer_time=1.0, max_batch_size=1024, server_mode='singleton')</code>","text":"<p>Asynchronous buffer for batching and processing reward scoring requests.</p> <p>This class implements a request buffering system that collects incoming molecular verifier queries and processes them in optimized batches to maximize throughput and reduce latency. It uses asyncio for non-blocking I/O and Ray for distributed computation of reward scores.</p> <p>The buffering strategy works as follows:</p> <ol> <li>Requests are added to an asyncio queue without blocking the caller</li> <li>A background task periodically flushes the queue or when it reaches max size</li> <li>Multiple queries are merged into a single batch with aligned completions/metadata</li> <li>Results are computed in parallel using Ray actors</li> <li>Results are grouped back to individual queries and returned asynchronously</li> </ol> <p>This approach provides several benefits:</p> <ul> <li>Amortizes overhead across multiple requests</li> <li>Allows Ray to optimize kernel launches on GPU</li> <li>Reduces context switching and memory fragmentation</li> </ul> <p>Attributes:</p> Name Type Description <code>app</code> <code>Any</code> <p>FastAPI/Starlette application instance containing Ray actors in app.state.reward_model for distributed reward computation.</p> <code>buffer_time</code> <code>float</code> <p>Maximum time in seconds to buffer requests before processing. Trades off latency for batch efficiency. Setting this too high increases latency; too low reduces batch efficiency. Default: 1.0</p> <code>max_batch_size</code> <code>int</code> <p>Maximum number of queries to process in a single batch. Larger batches improve GPU utilization but increase per-request latency. Should be tuned based on GPU memory and target latency. Default: 1024</p> <code>server_mode</code> <code>Literal['singleton', 'batch']</code> <p>Server operation mode.</p> <ul> <li>\"singleton\": Each query contains a single completion to score.</li> <li>\"batch\": Each query can contain multiple completions to score. Default: \"singleton\"</li> </ul> Note <p>This class is thread-safe for asyncio but not for multi-threaded access. All methods must be called from the same asyncio event loop.</p> Source code in <code>mol_gen_docking/server_utils/buffer.py</code> <pre><code>def __init__(\n    self,\n    app: Any,\n    buffer_time: float = 1.0,\n    max_batch_size: int = 1024,\n    server_mode: Literal[\"singleton\", \"batch\"] = \"singleton\",\n) -&gt; None:\n    \"\"\"Asynchronous buffer for batching and processing reward scoring requests.\n\n    This class implements a request buffering system that collects incoming molecular\n    verifier queries and processes them in optimized batches to maximize throughput\n    and reduce latency. It uses asyncio for non-blocking I/O and Ray for distributed\n    computation of reward scores.\n\n    The buffering strategy works as follows:\n\n    1. Requests are added to an asyncio queue without blocking the caller\n    2. A background task periodically flushes the queue or when it reaches max size\n    3. Multiple queries are merged into a single batch with aligned completions/metadata\n    4. Results are computed in parallel using Ray actors\n    5. Results are grouped back to individual queries and returned asynchronously\n\n    This approach provides several benefits:\n\n    - Amortizes overhead across multiple requests\n    - Allows Ray to optimize kernel launches on GPU\n    - Reduces context switching and memory fragmentation\n\n    Attributes:\n        app (Any): FastAPI/Starlette application instance containing Ray actors in\n            app.state.reward_model for distributed reward computation.\n\n        buffer_time (float): Maximum time in seconds to buffer requests before\n            processing. Trades off latency for batch efficiency. Setting this too high\n            increases latency; too low reduces batch efficiency.\n            Default: 1.0\n\n        max_batch_size (int): Maximum number of queries to process in a single batch.\n            Larger batches improve GPU utilization but increase per-request latency.\n            Should be tuned based on GPU memory and target latency.\n            Default: 1024\n\n        server_mode (Literal[\"singleton\", \"batch\"]): Server operation mode.\n\n            - \"singleton\": Each query contains a single completion to score.\n            - \"batch\": Each query can contain multiple completions to score.\n            Default: \"singleton\"\n\n\n    Note:\n        This class is thread-safe for asyncio but not for multi-threaded access.\n        All methods must be called from the same asyncio event loop.\n    \"\"\"\n    self.app = app\n    self.buffer_time = buffer_time\n    self.server_mode = server_mode\n    self.max_batch_size = max_batch_size\n    self.queue: deque[Tuple[MolecularVerifierServerQuery, asyncio.Future]] = deque()\n    self.lock = asyncio.Lock()\n    self.processing_task = asyncio.create_task(self._batch_loop())\n</code></pre>"},{"location":"reward_server/Server_Asynchronous/#mol_gen_docking.server_utils.buffer.RewardBuffer.add_query","title":"<code>add_query(query)</code>  <code>async</code>","text":"<p>Add a query to the buffer and wait for its result asynchronously.</p> <p>This method is non-blocking: it immediately queues the request and returns an awaitable future. The actual computation happens asynchronously in the background batch processing task. Multiple callers can call this method concurrently without blocking each other.</p> <p>The method uses an asyncio.Lock to ensure thread-safe queue access. If an error occurs during queueing, the exception is logged and re-raised to the caller.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>MolecularVerifierServerQuery</code> <p>A molecular verifier query containing:</p> <ul> <li>metadata: List of metadata dictionaries (one per completion)</li> <li>query: List of completion strings to score</li> <li>prompts: Optional list of original prompts for tracking</li> </ul> required <p>Returns:</p> Type Description <code>MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse</code> <p>MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse: Response type depends on server_mode:</p> <ul> <li> <p>In \"singleton\" mode: MolecularVerifierServerResponse containing:</p> <ul> <li>reward: Single reward score</li> <li>meta: Single metadata object with detailed scoring information</li> <li>error: Error message if scoring failed</li> <li>next_turn_feedback: Optional feedback for multi-turn conversations</li> </ul> </li> <li> <p>In \"batch\" mode: BatchMolecularVerifierServerResponse containing:</p> <ul> <li>rewards: List of individual reward scores (one per completion)</li> <li>metas: List of metadata objects (one per completion)</li> <li>error: Error message if scoring failed</li> <li>next_turn_feedback: Optional feedback for multi-turn conversations</li> </ul> </li> </ul> <p>Raises:</p> Type Description <code>Exception</code> <p>Any exception raised during queueing or during result computation in the background task. The exception is logged before being raised to the caller.</p> Source code in <code>mol_gen_docking/server_utils/buffer.py</code> <pre><code>async def add_query(\n    self, query: MolecularVerifierServerQuery\n) -&gt; MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse:\n    \"\"\"Add a query to the buffer and wait for its result asynchronously.\n\n    This method is non-blocking: it immediately queues the request and returns\n    an awaitable future. The actual computation happens asynchronously in the\n    background batch processing task. Multiple callers can call this method\n    concurrently without blocking each other.\n\n    The method uses an asyncio.Lock to ensure thread-safe queue access. If an\n    error occurs during queueing, the exception is logged and re-raised to the\n    caller.\n\n    Args:\n        query (MolecularVerifierServerQuery): A molecular verifier query containing:\n\n            - metadata: List of metadata dictionaries (one per completion)\n            - query: List of completion strings to score\n            - prompts: Optional list of original prompts for tracking\n\n    Returns:\n        MolecularVerifierServerResponse | BatchMolecularVerifierServerResponse:\n            Response type depends on server_mode:\n\n            - In \"singleton\" mode: MolecularVerifierServerResponse containing:\n                - reward: Single reward score\n                - meta: Single metadata object with detailed scoring information\n                - error: Error message if scoring failed\n                - next_turn_feedback: Optional feedback for multi-turn conversations\n\n            - In \"batch\" mode: BatchMolecularVerifierServerResponse containing:\n                - rewards: List of individual reward scores (one per completion)\n                - metas: List of metadata objects (one per completion)\n                - error: Error message if scoring failed\n                - next_turn_feedback: Optional feedback for multi-turn conversations\n\n    Raises:\n        Exception: Any exception raised during queueing or during result\n            computation in the background task. The exception is logged\n            before being raised to the caller.\n    \"\"\"\n    try:\n        future: asyncio.Future = asyncio.get_event_loop().create_future()\n        async with self.lock:\n            self.queue.append((query, future))\n        return await future  # type:ignore\n    except Exception as e:\n        logger.error(f\"Error in add_query: {e}\")\n        raise e\n</code></pre>"},{"location":"reward_server/endpoints/","title":"Reward Server API Documentation","text":"<p>The Reward Server provides REST endpoints for molecular scoring and docking calculations. It uses FastAPI to handle asynchronous requests and Ray for distributed computation.</p>"},{"location":"reward_server/endpoints/#endpoints","title":"Endpoints","text":""},{"location":"reward_server/endpoints/#1-liveness-check","title":"1. Liveness Check","text":"<p>Endpoint: <code>GET /liveness</code></p> <p>Description: Health check endpoint to verify the server is running.</p>"},{"location":"reward_server/endpoints/#2-get-reward","title":"2. Get Reward","text":"<p>Endpoint: <code>POST /get_reward</code></p> <p>Description: Calculate molecular rewards (docking scores, validity, etc.) for generated molecules. This endpoint handles batched requests efficiently through a buffering system that groups concurrent requests before computation.</p>"},{"location":"reward_server/endpoints/#3-prepare-receptor","title":"3. Prepare Receptor","text":"<p>Endpoint: <code>POST /prepare_receptor</code></p> <p>Description: Prepare protein receptors for docking calculations. This is an internal endpoint that preprocesses receptor files needed for AutoDock GPU computations.</p> <p>Request Body:</p> <pre><code>{\n  \"metadata\": [\n    {\n      \"properties\": [\"1a28\", \"2x5y\"],\n      \"objectives\": [],\n      \"target\": []\n    }\n  ],\n  \"query\": [\"dummy\"],\n  \"prompts\": null\n}\n</code></pre> <p>Response (Success):</p> <pre><code>{\n  \"status\": \"Success\"\n}\n</code></pre> <p>Response (No Preparation Needed):</p> <pre><code>{\n  \"status\": \"No need to prepare receptors for the selected docking oracle.\"\n}\n</code></pre> <p>Response (Error):</p> <pre><code>{\n  \"status\": \"Error\",\n  \"info\": \"Receptors [1a28, 2x5y] could not be processed.\"\n}\n</code></pre> <p>Note</p> <p>This endpoint is automatically called before reward calculation when using the AutoDock GPU oracle. It validates and preprocesses PDB receptor files. This results in larger time overhead during the first docking request for new receptors, but subsequent requests will be faster.</p>"},{"location":"reward_server/endpoints/#usage-example","title":"Usage Example","text":"<pre><code>import requests\n\n# Initialize the server\n# uvicorn mol_gen_docking.server:app --reload\n\n# Check liveness\nresponse = requests.get(\"http://localhost:8000/liveness\")\nprint(response.json())  # {\"status\": \"ok\"}\n\n# Get molecular rewards\nquery_data = {\n    \"metadata\": [\n        {\n            \"properties\": [\"QED\", \"SA\"],\n            \"objectives\": [\"maximize\", \"minimize\"],\n            \"target\": [0.0, 0.0]\n        }\n    ],\n    \"query\": [\n        \"&lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n    ]\n}\n\nresponse = requests.post(\"http://localhost:8000/get_reward\", json=query_data)\nresult = response.json()\nprint(f\"Overall reward: {result['reward']}\")\nprint(f\"Individual scores: {result['reward_list']}\")\nprint(f\"Feedback: {result['next_turn_feedback']}\")\n</code></pre>"},{"location":"reward_server/endpoints/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Batching: The server uses a buffering system (default 20 seconds) to batch multiple requests together for efficient GPU utilization. This can increase the time to perform a request. If you are working in a synchronous setting, consider decreasing the buffer time.</li> <li>Concurrency: Up to 2 docking runs per GPU by default.</li> </ul>"},{"location":"reward_server/getting_started/","title":"Getting Started with the Reward Server","text":"<p>The Reward Server is a FastAPI application that evaluates molecular structures based on various scoring functions including docking, drug-likeness, and bioactivity predictions.</p>"},{"location":"reward_server/getting_started/#starting-the-server","title":"Starting the Server","text":"<p>Note</p> <p>For GPU-accelerated docking, ensure AutoDock-GPU is installed (follow https://github.com/ccsb-scripps/AutoDock-GPU for installation instructions).</p> <p>Set required environment variables (see here for all options):</p> <pre><code>export DOCKING_ORACLE=autodock_gpu\nexport DATA_PATH=data\n</code></pre> <p>Start the server:</p> <pre><code>uvicorn --host 0.0.0.0 --port 8000 mol_gen_docking.server:app\n</code></pre> <p>You should see: <pre><code>INFO:     Uvicorn running on http://0.0.0.0:8000\nINFO:     Application startup complete\n</code></pre></p>"},{"location":"reward_server/getting_started/#basic-usage","title":"Basic Usage","text":""},{"location":"reward_server/getting_started/#python-client","title":"Python Client","text":"<pre><code>import requests\n\nresponse = requests.post(\n    \"http://localhost:8000/get_reward\",\n    json={\n        \"query\": \"&lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n        \"prompt\": \"[Textual prompt used to generate the molecule]\",\n        \"metadata\": [\n             {\n                 \"properties\": [\"sample_654138_model_0\", \"CalcExactMolWt\"],\n                 \"objectives\": [\"below\", \"below\"],\n                 \"target\": [-10.86, 197.27]\n             }\n        ]\n    }\n)\n</code></pre>"},{"location":"reward_server/getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore how to configure the server in Server Configuration</li> <li>Understand the query and response formats in Query and Answer Format</li> <li>Learn about the full API in API Documentation</li> </ul>"},{"location":"reward_server/query_answer_format/","title":"Format of the Queries and Answers for the Molecular Verifier Server","text":"<p>All queries to the Molecular Verifier server and its responses follow specific formats defined using Pydantic models. Below we outline the expected structure for both requests and responses.</p>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerQuery","title":"<code>MolecularVerifierServerQuery</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input query model for the Molecular Verifier server.</p> <p>Represents a complete request to the molecular verifier service, containing metadata for scoring and completions to evaluate.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>List[dict[str, Any]]</code> <p>List of metadata dictionaries, one per query item. Each dictionary will be converted to a MolecularVerifierMetadata object for scoring.</p> <code>query</code> <code>List[str]</code> <p>List of completion strings from the language model.</p> <p>The parser extracts content between these tags based on 'parsing_method'</p> <code>prompts</code> <code>Optional[List[str]]</code> <p>Optional. Original prompts used to generate the completions. Useful for tracking and debugging. If provided, should have same length as query list.</p> <p>Notes: This model also supports single-item requests not in list form. For example: <pre><code>{\n  \"query\": \"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n  \"prompt\": \"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\",\n  \"metadata\": {\n    \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n    \"objectives\": [\"above\", \"minimize\"],\n    \"target\": [3.0, 0.0]\n  }\n}\n</code></pre> is also accepted and will be internally converted to: <pre><code>{      \"query\": [\"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\"],\n  \"prompt\": [\"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\"],\n  \"metadata\": [\n    {\n      \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n      \"objectives\": [\"above\", \"minimize\"],\n      \"target\": [3.0, 0.0]\n    }\n  ]\n}\n</code></pre></p> <p>Example: <pre><code>{\n  \"query\": \"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n  \"prompt\": \"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\",\n  \"metadata\": [\n    {\n      \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n      \"objectives\": [\"above\", \"minimize\"],\n      \"target\": [3.0, 0.0]\n    }\n  ]\n}\n</code></pre></p> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>class MolecularVerifierServerQuery(BaseModel):\n    \"\"\"Input query model for the Molecular Verifier server.\n\n    Represents a complete request to the molecular verifier service,\n    containing metadata for scoring and completions to evaluate.\n\n    Attributes:\n        metadata: List of metadata dictionaries, one per query item.\n            Each dictionary will be converted to a MolecularVerifierMetadata\n            object for scoring.\n\n        query: List of completion strings from the language model.\n\n            The parser extracts content between these tags based on\n            'parsing_method'\n\n        prompts: Optional. Original prompts used to generate the completions.\n            Useful for tracking and debugging. If provided, should have\n            same length as query list.\n\n    Notes:\n    This model also supports single-item requests not in list form.\n    For example:\n    ```json\n    {\n      \"query\": \"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n      \"prompt\": \"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\",\n      \"metadata\": {\n        \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n        \"objectives\": [\"above\", \"minimize\"],\n        \"target\": [3.0, 0.0]\n      }\n    }\n    ```\n    is also accepted and will be internally converted to:\n    ```json\n    {      \"query\": [\"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\"],\n      \"prompt\": [\"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\"],\n      \"metadata\": [\n        {\n          \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n          \"objectives\": [\"above\", \"minimize\"],\n          \"target\": [3.0, 0.0]\n        }\n      ]\n    }\n    ```\n\n    Example:\n    ```json\n    {\n      \"query\": \"Here is a molecules: &lt;answer&gt;CC(C)Cc1ccc(cc1)C(C)C(=O)O&lt;/answer&gt;\",\n      \"prompt\": \"Generate a molecule that binds to my target protein with high affinity and has more than 3 rotatable bonds.\",\n      \"metadata\": [\n        {\n          \"properties\": [\"CalcNumRotatableBonds\", \"sample_228234_model_0\"],\n          \"objectives\": [\"above\", \"minimize\"],\n          \"target\": [3.0, 0.0]\n        }\n      ]\n    }\n    ```\n    \"\"\"\n\n    metadata: List[dict[str, Any]] = Field(\n        ..., description=\"List of metadata dictionaries, one per query item.\"\n    )\n    query: List[str] = Field(\n        ..., description=\"List of completion strings from the language model.\"\n    )\n    prompts: Optional[List[str]] = Field(\n        None, description=\"Original prompts used to generate the completions.\"\n    )\n\n    @field_validator(\"metadata\", \"query\", \"prompts\", mode=\"before\")\n    @classmethod\n    def convert_to_lists(cls, v: Any) -&gt; Any:\n        \"\"\"Automatically convert fields to lists if they are not already.\n\n        Ensures that metadata, query, and prompts fields are lists.\n        If a field is not a list, it will be wrapped in a list.\n        None values are left as-is for optional fields.\n\n        Args:\n            v: The field value to potentially convert\n\n        Returns:\n            The value as a list, or None if the input is None\n        \"\"\"\n        if v is None:\n            return None\n        if not isinstance(v, list):\n            return [v]\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_equal_lengths(self) -&gt; \"MolecularVerifierServerQuery\":\n        \"\"\"Validate that all fields have equal length.\n\n        Ensures that metadata, query, and prompts lists all have the same length.\n        This validator runs after all fields have been set and converted.\n\n        Returns:\n            The validated instance\n\n        Raises:\n            ValueError: If field lengths are not equal\n        \"\"\"\n        # Extract lengths of non-None fields\n        lengths = {}\n        lengths[\"metadata\"] = len(self.metadata)\n        lengths[\"query\"] = len(self.query)\n        if self.prompts is not None:\n            lengths[\"prompts\"] = len(self.prompts)\n        unique_lengths = set(lengths.values())\n        if len(unique_lengths) &gt; 1:\n            raise ValueError(f\"All query fields must have equal length. Got: {lengths}\")\n        return self\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerQuery.convert_to_lists","title":"<code>convert_to_lists(v)</code>  <code>classmethod</code>","text":"<p>Automatically convert fields to lists if they are not already.</p> <p>Ensures that metadata, query, and prompts fields are lists. If a field is not a list, it will be wrapped in a list. None values are left as-is for optional fields.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The field value to potentially convert</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The value as a list, or None if the input is None</p> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>@field_validator(\"metadata\", \"query\", \"prompts\", mode=\"before\")\n@classmethod\ndef convert_to_lists(cls, v: Any) -&gt; Any:\n    \"\"\"Automatically convert fields to lists if they are not already.\n\n    Ensures that metadata, query, and prompts fields are lists.\n    If a field is not a list, it will be wrapped in a list.\n    None values are left as-is for optional fields.\n\n    Args:\n        v: The field value to potentially convert\n\n    Returns:\n        The value as a list, or None if the input is None\n    \"\"\"\n    if v is None:\n        return None\n    if not isinstance(v, list):\n        return [v]\n    return v\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerQuery.validate_equal_lengths","title":"<code>validate_equal_lengths()</code>","text":"<p>Validate that all fields have equal length.</p> <p>Ensures that metadata, query, and prompts lists all have the same length. This validator runs after all fields have been set and converted.</p> <p>Returns:</p> Type Description <code>MolecularVerifierServerQuery</code> <p>The validated instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If field lengths are not equal</p> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_equal_lengths(self) -&gt; \"MolecularVerifierServerQuery\":\n    \"\"\"Validate that all fields have equal length.\n\n    Ensures that metadata, query, and prompts lists all have the same length.\n    This validator runs after all fields have been set and converted.\n\n    Returns:\n        The validated instance\n\n    Raises:\n        ValueError: If field lengths are not equal\n    \"\"\"\n    # Extract lengths of non-None fields\n    lengths = {}\n    lengths[\"metadata\"] = len(self.metadata)\n    lengths[\"query\"] = len(self.query)\n    if self.prompts is not None:\n        lengths[\"prompts\"] = len(self.prompts)\n    unique_lengths = set(lengths.values())\n    if len(unique_lengths) &gt; 1:\n        raise ValueError(f\"All query fields must have equal length. Got: {lengths}\")\n    return self\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerMetadata","title":"<code>MolecularVerifierServerMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata returned with each scored molecule.</p> <p>Aggregates detailed scoring information from all verifier types (Generation, Molecular Property, and Reaction). Each field may be populated or empty depending on which verifier was used.</p> <p>Attributes:</p> Name Type Description <code>parsed_answer</code> <code>str</code> <p>The parsed answer extracted from the model completion.</p> <code>generation_verifier_metadata</code> <code>Optional[GenerationVerifierMetadataModel]</code> <p>Metadata from the generation verifier containing:</p> <ul> <li>properties: List of evaluated property names</li> <li>individual_rewards: Individual reward for each property</li> <li>all_smi_rewards: Rewards for all SMILES found in completion</li> <li>all_smi: List of all extracted SMILES strings</li> <li>smiles_extraction_failure: Error message if SMILES extraction failed</li> </ul> <code>mol_prop_verifier_metadata</code> <code>Optional[MolPropVerifierMetadataModel]</code> <p>Metadata from the molecular property verifier containing:</p> <ul> <li>extracted_answer: The numerical answer extracted from the completion</li> <li>extraction_success: Whether the answer extraction was successful</li> </ul> <code>reaction_verifier_metadata</code> <code>Optional[ReactionVerifierMetadataModel]</code> <p>Metadata from the reaction verifier containing:</p> <ul> <li>valid: Proportion of valid reaction steps (0.0 to 1.0)</li> <li>correct_product: Product correctness or similarity to target molecule</li> <li>correct_reactant: Whether all building blocks used are valid</li> </ul> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>class MolecularVerifierServerMetadata(BaseModel):\n    \"\"\"Metadata returned with each scored molecule.\n\n    Aggregates detailed scoring information from all verifier types (Generation,\n    Molecular Property, and Reaction). Each field may be populated or empty\n    depending on which verifier was used.\n\n    Attributes:\n        parsed_answer: The parsed answer extracted from the model completion.\n        generation_verifier_metadata: Metadata from the generation verifier containing:\n\n            - properties: List of evaluated property names\n            - individual_rewards: Individual reward for each property\n            - all_smi_rewards: Rewards for all SMILES found in completion\n            - all_smi: List of all extracted SMILES strings\n            - smiles_extraction_failure: Error message if SMILES extraction failed\n\n        mol_prop_verifier_metadata: Metadata from the molecular property verifier containing:\n\n            - extracted_answer: The numerical answer extracted from the completion\n            - extraction_success: Whether the answer extraction was successful\n\n        reaction_verifier_metadata: Metadata from the reaction verifier containing:\n\n            - valid: Proportion of valid reaction steps (0.0 to 1.0)\n            - correct_product: Product correctness or similarity to target molecule\n            - correct_reactant: Whether all building blocks used are valid\n    \"\"\"\n\n    parsed_answer: str = Field(\n        \"\",\n        description=\"The parsed answer extracted from the model completion.\",\n    )\n    generation_verifier_metadata: Optional[GenerationVerifierMetadataModel] = Field(\n        None,\n        description=\"Metadata from the generation verifier, if applicable.\",\n    )\n    mol_prop_verifier_metadata: Optional[MolPropVerifierMetadataModel] = Field(\n        None,\n        description=\"Metadata from the molecular property verifier, if applicable.\",\n    )\n    reaction_verifier_metadata: Optional[ReactionVerifierMetadataModel] = Field(\n        None,\n        description=\"Metadata from the reaction verifier, if applicable.\",\n    )\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerResponse","title":"<code>MolecularVerifierServerResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response from the Molecular Verifier server when the sever is in single-item mode (default).</p> <p>Contains the computed reward score and detailed metadata for a single evaluated completion.</p> <p>Attributes:</p> Name Type Description <code>reward</code> <code>float</code> <p>Overall reward score combining all evaluated properties. Typically normalized to [0.0, 1.0] range when rescaling is enabled.</p> <code>error</code> <code>Optional[str]</code> <p>Error message if scoring failed. None if successful.</p> <code>meta</code> <code>MolecularVerifierServerMetadata</code> <p>Metadata dictionary with detailed scoring information. Contains extraction failures, property rewards, and verification results for the completion.</p> <code>next_turn_feedback</code> <code>Optional[str]</code> <p>Optional feedback for multi-turn conversations. Can be used to guide subsequent model generations.</p> Example <pre><code>{\n  \"reward\": 0.75,\n  \"error\": null,\n  \"meta\": {\n      \"smiles_extraction_failure\": \"\",\n      \"all_smi\": [\"CC(C)Cc1ccc(cc1)\"],\n      \"all_smi_rewards\": [0.75],\n      \"properties\": [\"GSK3B\", \"CalcLogP\"],\n      \"individual_rewards\": [1.0, 0.5]\n    },\n  \"next_turn_feedback\": null\n}\n</code></pre> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>class MolecularVerifierServerResponse(BaseModel):\n    \"\"\"Response from the Molecular Verifier server when the sever is in single-item mode (default).\n\n    Contains the computed reward score and detailed metadata for a single\n    evaluated completion.\n\n    Attributes:\n        reward: Overall reward score combining all evaluated properties.\n            Typically normalized to [0.0, 1.0] range when rescaling is enabled.\n\n        error: Error message if scoring failed. None if successful.\n\n        meta: Metadata dictionary with detailed scoring information.\n            Contains extraction failures, property rewards, and verification\n            results for the completion.\n\n        next_turn_feedback: Optional feedback for multi-turn conversations.\n            Can be used to guide subsequent model generations.\n\n    Example:\n        ```json\n        {\n          \"reward\": 0.75,\n          \"error\": null,\n          \"meta\": {\n              \"smiles_extraction_failure\": \"\",\n              \"all_smi\": [\"CC(C)Cc1ccc(cc1)\"],\n              \"all_smi_rewards\": [0.75],\n              \"properties\": [\"GSK3B\", \"CalcLogP\"],\n              \"individual_rewards\": [1.0, 0.5]\n            },\n          \"next_turn_feedback\": null\n        }\n        ```\n    \"\"\"\n\n    reward: float = Field(\n        ..., description=\"Overall reward score combining all evaluated properties.\"\n    )\n    error: Optional[str] = Field(None, description=\"Error message if scoring failed.\")\n    meta: MolecularVerifierServerMetadata = Field(\n        default_factory=MolecularVerifierServerMetadata,\n        description=\"Metadata dictionary with detailed scoring information.\",\n    )\n    next_turn_feedback: Optional[str] = Field(\n        None, description=\"Optional feedback for multi-turn conversations.\"\n    )\n\n    @field_validator(\"reward\")\n    @classmethod\n    def validate_reward(cls, v: Any) -&gt; float:\n        \"\"\"Validate and normalize reward value.\n\n        If reward is None or NaN, sets it to 0.0.\n        Ensures reward is a valid float.\n\n        Args:\n            v: The reward value to validate\n\n        Returns:\n            The validated reward value (float), or 0.0 if None/NaN\n\n        Raises:\n            ValueError: If reward is not a valid numeric type\n        \"\"\"\n        # Handle None case\n        if v is None:\n            return 0.0\n        # Handle NaN case\n        if math.isnan(float(v)):\n            return 0.0\n        # Check if it's a numeric type\n        if not isinstance(v, (int, float)):\n            raise ValueError(f\"reward must be a float, got {type(v).__name__}\")\n        return float(v)\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.MolecularVerifierServerResponse.validate_reward","title":"<code>validate_reward(v)</code>  <code>classmethod</code>","text":"<p>Validate and normalize reward value.</p> <p>If reward is None or NaN, sets it to 0.0. Ensures reward is a valid float.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The reward value to validate</p> required <p>Returns:</p> Type Description <code>float</code> <p>The validated reward value (float), or 0.0 if None/NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reward is not a valid numeric type</p> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>@field_validator(\"reward\")\n@classmethod\ndef validate_reward(cls, v: Any) -&gt; float:\n    \"\"\"Validate and normalize reward value.\n\n    If reward is None or NaN, sets it to 0.0.\n    Ensures reward is a valid float.\n\n    Args:\n        v: The reward value to validate\n\n    Returns:\n        The validated reward value (float), or 0.0 if None/NaN\n\n    Raises:\n        ValueError: If reward is not a valid numeric type\n    \"\"\"\n    # Handle None case\n    if v is None:\n        return 0.0\n    # Handle NaN case\n    if math.isnan(float(v)):\n        return 0.0\n    # Check if it's a numeric type\n    if not isinstance(v, (int, float)):\n        raise ValueError(f\"reward must be a float, got {type(v).__name__}\")\n    return float(v)\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.BatchMolecularVerifierServerResponse","title":"<code>BatchMolecularVerifierServerResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response from the Molecular Verifier server when in batch mode.</p> <p>Contains the computed reward scores and detailed metadata for each evaluated completion.</p> <p>Attributes:</p> Name Type Description <code>rewards</code> <code>List[float]</code> <p>List of individual reward scores, one per evaluated completion in the request.</p> <code>error</code> <code>Optional[str]</code> <p>Error message if scoring failed. None if successful.</p> <code>metas</code> <code>List[MolecularVerifierServerMetadata]</code> <p>List of metadata dictionaries with detailed scoring information. Contains extraction failures, property rewards, and verification results for each completion. One metadata object per query item.</p> <code>next_turn_feedback</code> <code>Optional[str]</code> <p>Optional feedback for multi-turn conversations. Can be used to guide subsequent model generations.</p> Example <pre><code>{\n  \"rewards\": [0.75],\n  \"error\": null,\n  \"metas\": [\n     \"generation_verifier_metadata\": {\n            \"smiles_extraction_failure\": \"\",\n            \"all_smi\": [\"CC(C)Cc1ccc(cc1)\"],\n            \"all_smi_rewards\": [0.75],\n            \"properties\": [\"GSK3B\", \"CalcLogP\"],\n            \"individual_rewards\": [1.0, 0.5]\n        }\n    }\n  ],\n  \"next_turn_feedback\": null\n}\n</code></pre> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>class BatchMolecularVerifierServerResponse(BaseModel):\n    \"\"\"Response from the Molecular Verifier server when in batch mode.\n\n    Contains the computed reward scores and detailed metadata for each\n    evaluated completion.\n\n    Attributes:\n        rewards: List of individual reward scores, one per evaluated\n            completion in the request.\n\n        error: Error message if scoring failed. None if successful.\n\n        metas: List of metadata dictionaries with detailed scoring information.\n            Contains extraction failures, property rewards, and verification\n            results for each completion. One metadata object per query item.\n\n        next_turn_feedback: Optional feedback for multi-turn conversations.\n            Can be used to guide subsequent model generations.\n\n    Example:\n        ```json\n        {\n          \"rewards\": [0.75],\n          \"error\": null,\n          \"metas\": [\n             \"generation_verifier_metadata\": {\n                    \"smiles_extraction_failure\": \"\",\n                    \"all_smi\": [\"CC(C)Cc1ccc(cc1)\"],\n                    \"all_smi_rewards\": [0.75],\n                    \"properties\": [\"GSK3B\", \"CalcLogP\"],\n                    \"individual_rewards\": [1.0, 0.5]\n                }\n            }\n          ],\n          \"next_turn_feedback\": null\n        }\n        ```\n    \"\"\"\n\n    rewards: List[float] = Field(\n        ...,\n        description=\"List of individual reward scores, one per evaluated completion.\",\n    )\n    error: Optional[str] = Field(None, description=\"Error message if scoring failed.\")\n    metas: List[MolecularVerifierServerMetadata] = Field(\n        default_factory=list,\n        description=\"List of metadata with detailed scoring information.\",\n    )\n    next_turn_feedback: Optional[str] = Field(\n        None, description=\"Optional feedback for multi-turn conversations.\"\n    )\n\n    @field_validator(\"rewards\")\n    @classmethod\n    def validate_rewards(cls, v: Any) -&gt; List[float]:\n        \"\"\"Validate and normalize reward value.\n\n        If any reward is None or NaN, sets it to 0.0.\n        Ensures reward is a valid float.\n\n        Args:\n            v: The reward value to validate\n\n        Returns:\n            The validated reward value (float), or 0.0 if None/NaN\n\n        Raises:\n            ValueError: If reward is not a valid numeric type\n        \"\"\"\n        assert isinstance(v, list)\n        # Handle None case\n        for i, val in enumerate(v):\n            if val is None:\n                val = 0.0\n            # Handle NaN case\n            if math.isnan(float(val)):\n                val = 0.0\n            # Check if it's a numeric type\n            if not isinstance(val, (int, float)):\n                raise ValueError(f\"reward must be a float, got {type(val).__name__}\")\n            v[i] = float(val)\n        return v\n</code></pre>"},{"location":"reward_server/query_answer_format/#mol_gen_docking.server_utils.utils.BatchMolecularVerifierServerResponse.validate_rewards","title":"<code>validate_rewards(v)</code>  <code>classmethod</code>","text":"<p>Validate and normalize reward value.</p> <p>If any reward is None or NaN, sets it to 0.0. Ensures reward is a valid float.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Any</code> <p>The reward value to validate</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>The validated reward value (float), or 0.0 if None/NaN</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If reward is not a valid numeric type</p> Source code in <code>mol_gen_docking/server_utils/utils.py</code> <pre><code>@field_validator(\"rewards\")\n@classmethod\ndef validate_rewards(cls, v: Any) -&gt; List[float]:\n    \"\"\"Validate and normalize reward value.\n\n    If any reward is None or NaN, sets it to 0.0.\n    Ensures reward is a valid float.\n\n    Args:\n        v: The reward value to validate\n\n    Returns:\n        The validated reward value (float), or 0.0 if None/NaN\n\n    Raises:\n        ValueError: If reward is not a valid numeric type\n    \"\"\"\n    assert isinstance(v, list)\n    # Handle None case\n    for i, val in enumerate(v):\n        if val is None:\n            val = 0.0\n        # Handle NaN case\n        if math.isnan(float(val)):\n            val = 0.0\n        # Check if it's a numeric type\n        if not isinstance(val, (int, float)):\n            raise ValueError(f\"reward must be a float, got {type(val).__name__}\")\n        v[i] = float(val)\n    return v\n</code></pre>"},{"location":"reward_server/server_configuration/","title":"Configuration of the Molecular Verifier Server","text":""},{"location":"reward_server/server_configuration/#using-a-pydantic-basesettings","title":"Using a Pydantic BaseSettings","text":"<p>We use Pydantic's <code>BaseSettings</code> to manage the configuration of our application through environment variables. It automatically reads and validates environment variables, converting them to Python types defined in your settings class.</p> <p>How it works: When you define a class that inherits from <code>BaseSettings</code>, Pydantic:</p> <ol> <li>Looks for environment variables matching the field names (case-insensitive)</li> <li>Converts them to the appropriate Python types</li> <li>Validates them against the field constraints</li> <li>Falls back to default values if environment variables are not provided</li> </ol> <p>Setting configuration: Simply export environment variables before starting the server: <pre><code>export DOCKING_ORACLE=\"autodock_gpu\"\nexport DATA_PATH=\"/path/to/data\"\nuvicorn ...\n</code></pre></p> <p>Or set them inline: <pre><code>DOCKING_ORACLE=autodock_gpu DATA_PATH=./data uvicorn ...\n</code></pre></p>"},{"location":"reward_server/server_configuration/#server-configuration","title":"Server Configuration","text":""},{"location":"reward_server/server_configuration/#mol_gen_docking.server_utils.server_setting.MolecularVerifierServerSettings","title":"<code>MolecularVerifierServerSettings</code>","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Configuration settings for the Molecular Verifier server.</p> <p>This class manages all configuration parameters for running the Molecular Verifier server, including docking settings, oracle configuration, and verifier behavior. Settings can be loaded from environment variables, .env files, or passed directly.</p> <p>The settings are used to initialize verifier configurations for molecular property prediction, reaction verification, and molecular generation tasks. It provides validation of settings and conversion to verifier config models.</p> <p>Attributes:</p> Name Type Description <code>server_mode</code> <code>Literal['singleton', 'batch']</code> <p>Server operation mode.</p> <ul> <li>\"singleton\": Handles one request at a time.</li> <li>\"batch\": Accepts multiple concurrent requests for batch processing. Default: \"singleton\"</li> </ul> <code>scorer_exhaustiveness</code> <code>int</code> <p>Exhaustiveness parameter for docking scoring. Controls the thoroughness of the docking search. Higher values increase accuracy but require more computation time. Default: 8</p> <code>scorer_ncpus</code> <code>int</code> <p>Number of CPU cores to allocate for scoring operations. Must be compatible with exhaustiveness. Default: 8</p> <code>docking_concurrency_per_gpu</code> <code>int</code> <p>Number of concurrent docking jobs per GPU. Controls GPU utilization. Default: 2</p> <code>reaction_matrix_path</code> <code>str</code> <p>Path to the pickled reaction matrix file. Must exist and be accessible for reaction verification tasks. Default: \"data/rxn_matrix.pkl\"</p> <code>docking_oracle</code> <code>Literal['pyscreener', 'autodock_gpu']</code> <p>The docking software to use for molecular docking. \"pyscreener\" for PyScreener docking or \"autodock_gpu\" for GPU-accelerated AutoDock. Default: \"autodock_gpu\"</p> <code>vina_mode</code> <code>str</code> <p>Command mode for AutoDock GPU execution. Example: \"autodock_gpu_256wi\" for 256 work items mode. Only used when docking_oracle is \"autodock_gpu\". Default: \"autodock_gpu_256wi\"</p> <code>data_path</code> <code>str</code> <p>Path to the molecular data directory containing: - names_mapping.json: Property name mappings - docking_targets.json: Target definitions for docking Used by generation tasks, and must be accessible. Default: \"data/molgendata\"</p> <code>buffer_time</code> <code>int</code> <p>Time in seconds to buffer requests before processing. Allows batch processing of concurrent requests to improve efficiency. Default: 20</p> <code>parsing_method</code> <code>Literal['none', 'answer_tags', 'boxed']</code> <p>Method to parse model completions:</p> <ul> <li>\"none\": No parsing, use full completion (not recommended, risks of ambiguity in the answer extraction)</li> <li>\"answer_tags\": Extract content within special tags</li> <li>\"boxed\": Extract content within answer tags and boxed in the '\\boxed{...}' LaTeX command</li> </ul> <code>debug_logging</code> <code>bool</code> <p>Enable detailed debug logging for server operations. Useful for troubleshooting and development. Default: False</p> Example <pre><code>from mol_gen_docking.server_utils.server_setting import MolecularVerifierServerSettings\nfrom mol_gen_docking.reward import MolecularVerifier\n\n# Load settings from environment variables or .env file\nsettings = MolecularVerifierServerSettings()\n\n# Convert to verifier configuration\nconfig = settings.to_molecular_verifier_config(reward=\"property\")\n\n# Create verifier instance\nverifier = MolecularVerifier(verifier_config=config)\n</code></pre> Environment Variables <p>Settings can be configured via environment variables (non-case sensitive) names:</p> <ul> <li>SERVER_MODE</li> <li>SCORER_EXHAUSTIVENESS</li> <li>SCORER_NCPUS</li> <li>DOCKING_CONCURRENCY_PER_GPU</li> <li>REACTION_MATRIX_PATH</li> <li>DOCKING_ORACLE</li> <li>VINA_MODE</li> <li>DATA_PATH</li> <li>BUFFER_TIME</li> <li>PARSING_METHODS</li> <li>DEBUG_LOGGING</li> </ul> Source code in <code>mol_gen_docking/server_utils/server_setting.py</code> <pre><code>class MolecularVerifierServerSettings(BaseSettings):\n    \"\"\"Configuration settings for the Molecular Verifier server.\n\n    This class manages all configuration parameters for running the Molecular Verifier\n    server, including docking settings, oracle configuration, and verifier behavior.\n    Settings can be loaded from environment variables, .env files, or passed directly.\n\n    The settings are used to initialize verifier configurations for molecular property\n    prediction, reaction verification, and molecular generation tasks. It provides\n    validation of settings and conversion to verifier config models.\n\n    Attributes:\n        server_mode (Literal[\"singleton\", \"batch\"]): Server operation mode.\n\n            - \"singleton\": Handles one request at a time.\n            - \"batch\": Accepts multiple concurrent requests for batch processing.\n            Default: \"singleton\"\n        scorer_exhaustiveness (int): Exhaustiveness parameter for docking scoring.\n            Controls the thoroughness of the docking search. Higher values increase\n            accuracy but require more computation time.\n            Default: 8\n\n        scorer_ncpus (int): Number of CPU cores to allocate for scoring operations.\n            Must be compatible with exhaustiveness.\n            Default: 8\n\n        docking_concurrency_per_gpu (int): Number of concurrent docking jobs per GPU.\n            Controls GPU utilization.\n            Default: 2\n\n        reaction_matrix_path (str): Path to the pickled reaction matrix file.\n            Must exist and be accessible for reaction verification tasks.\n            Default: \"data/rxn_matrix.pkl\"\n\n        docking_oracle (Literal[\"pyscreener\", \"autodock_gpu\"]): The docking software\n            to use for molecular docking. \"pyscreener\" for PyScreener docking or\n            \"autodock_gpu\" for GPU-accelerated AutoDock.\n            Default: \"autodock_gpu\"\n\n        vina_mode (str): Command mode for AutoDock GPU execution.\n            Example: \"autodock_gpu_256wi\" for 256 work items mode.\n            Only used when docking_oracle is \"autodock_gpu\".\n            Default: \"autodock_gpu_256wi\"\n\n        data_path (str): Path to the molecular data directory containing:\n            - names_mapping.json: Property name mappings\n            - docking_targets.json: Target definitions for docking\n            Used by generation tasks, and must be accessible.\n            Default: \"data/molgendata\"\n\n        buffer_time (int): Time in seconds to buffer requests before processing.\n            Allows batch processing of concurrent requests to improve efficiency.\n            Default: 20\n\n        parsing_method: Method to parse model completions:\n\n            - \"none\": No parsing, use full completion (not recommended, risks of ambiguity in the answer extraction)\n            - \"answer_tags\": Extract content within special tags\n            - \"boxed\": Extract content within answer tags and boxed in the '\\\\boxed{...}' LaTeX command\n\n        debug_logging (bool): Enable detailed debug logging for server operations.\n            Useful for troubleshooting and development.\n            Default: False\n\n    Example:\n        ```python\n        from mol_gen_docking.server_utils.server_setting import MolecularVerifierServerSettings\n        from mol_gen_docking.reward import MolecularVerifier\n\n        # Load settings from environment variables or .env file\n        settings = MolecularVerifierServerSettings()\n\n        # Convert to verifier configuration\n        config = settings.to_molecular_verifier_config(reward=\"property\")\n\n        # Create verifier instance\n        verifier = MolecularVerifier(verifier_config=config)\n        ```\n\n    Environment Variables:\n        Settings can be configured via environment variables (non-case sensitive) names:\n\n        - SERVER_MODE\n        - SCORER_EXHAUSTIVENESS\n        - SCORER_NCPUS\n        - DOCKING_CONCURRENCY_PER_GPU\n        - REACTION_MATRIX_PATH\n        - DOCKING_ORACLE\n        - VINA_MODE\n        - DATA_PATH\n        - BUFFER_TIME\n        - PARSING_METHODS\n        - DEBUG_LOGGING\n    \"\"\"\n\n    server_mode: Literal[\"singleton\", \"batch\"] = \"singleton\"\n    scorer_exhaustiveness: int = 8\n    scorer_ncpus: int = 8\n    docking_concurrency_per_gpu: int = 2\n    reaction_matrix_path: str = \"data/rxn_matrix.pkl\"\n    docking_oracle: Literal[\"pyscreener\", \"autodock_gpu\"] = \"autodock_gpu\"\n    vina_mode: str = \"autodock_gpu_256wi\"\n    data_path: str = \"data/molgendata\"\n    buffer_time: int = 20\n    parsing_method: Literal[\"none\", \"answer_tags\", \"boxed\"] = \"answer_tags\"\n    debug_logging: bool = False\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate all settings after initialization.\n\n        Performs critical validation checks to ensure settings are consistent\n        and valid before the server starts. This method is called automatically\n        after the pydantic BaseSettings initializes the instance.\n\n        Raises:\n            AssertionError: If any of the following conditions are not met:\n\n                - scorer_exhaustiveness &gt; 0\n                - scorer_ncpus &gt; 0\n                - docking_concurrency_per_gpu &gt; 0\n                - reaction_matrix_path file exists\n        \"\"\"\n        assert self.scorer_exhaustiveness &gt; 0, \"Exhaustiveness must be greater than 0\"\n        assert self.scorer_ncpus &gt; 0, \"Number of CPUs must be greater than 0\"\n\n        assert Path(self.reaction_matrix_path).exists(), (\n            f\"Reaction matrix file {self.reaction_matrix_path} does not exist\"\n        )\n\n    def to_molecular_verifier_config(\n        self, reward: Literal[\"property\", \"valid_smiles\"] = \"property\"\n    ) -&gt; MolecularVerifierConfigModel:\n        \"\"\"Convert server settings to a complete verifier configuration model.\n\n        Creates a MolecularVerifierConfigModel with all sub-verifier configurations\n        (generation, reaction, and property) initialized from the server settings.\n        This method bridges the gap between server-level settings and verifier-level\n        configurations, ensuring consistent parameter propagation.\n\n        Args:\n            reward (Literal[\"property\", \"valid_smiles\"]): Type of reward to compute.\n\n                - \"property\": Use property-based rewards for molecular optimization.\n                  Evaluates predicted property values against target objectives.\n                - \"valid_smiles\": Use validity-based rewards. Returns 1.0 for valid\n                  molecules and 0.0 for invalid ones.\n                Default: \"property\"\n\n        Returns:\n            MolecularVerifierConfigModel: A fully configured verifier model containing:\n\n                - GenerationVerifierConfigModel: For molecular generation tasks with:\n                  - path_to_mappings set to data_path\n                  - reward type synchronized\n                  - rescale enabled\n                  - oracle_kwargs with docking settings\n                  - docking_concurrency_per_gpu configured\n\n                - ReactionVerifierConfigModel: For reaction verification with:\n                  - reaction_matrix_path configured\n                  - reward type synchronized\n                  - reaction_reward_type set to \"tanimoto\" (default)\n\n                - MolPropVerifierConfigModel: For property prediction with:\n                  - reward type synchronized\n\n                All sub-configs are automatically synced to the specified reward type.\n\n        Raises:\n            ValueError: If any of the referenced configuration paths don't exist.\n                This can happen if data_path or reaction_matrix_path are invalid.\n        \"\"\"\n        # Create oracle kwargs from server settings\n        oracle_kwargs = {\n            \"exhaustiveness\": self.scorer_exhaustiveness,\n            \"n_cpu\": self.scorer_ncpus,\n            \"docking_oracle\": self.docking_oracle,\n            \"vina_mode\": self.vina_mode,\n        }\n\n        # Create GenerationVerifierConfigModel\n        generation_config = GenerationVerifierConfigModel(\n            path_to_mappings=self.data_path,\n            reward=reward,\n            rescale=True,\n            oracle_kwargs=oracle_kwargs,\n            docking_concurrency_per_gpu=self.docking_concurrency_per_gpu,\n        )\n\n        # Create ReactionVerifierConfigModel\n        reaction_config = ReactionVerifierConfigModel(\n            reaction_matrix_path=self.reaction_matrix_path,\n            reward=reward,\n        )\n\n        # Create MolPropVerifierConfigModel\n        molprop_config = MolPropVerifierConfigModel(reward=reward)\n\n        # Create and return MolecularVerifierConfigModel\n        return MolecularVerifierConfigModel(\n            parsing_method=self.parsing_method,\n            reward=reward,\n            generation_verifier_config=generation_config,\n            reaction_verifier_config=reaction_config,\n            mol_prop_verifier_config=molprop_config,\n        )\n</code></pre>"},{"location":"reward_server/server_configuration/#mol_gen_docking.server_utils.server_setting.MolecularVerifierServerSettings.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate all settings after initialization.</p> <p>Performs critical validation checks to ensure settings are consistent and valid before the server starts. This method is called automatically after the pydantic BaseSettings initializes the instance.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If any of the following conditions are not met:</p> <ul> <li>scorer_exhaustiveness &gt; 0</li> <li>scorer_ncpus &gt; 0</li> <li>docking_concurrency_per_gpu &gt; 0</li> <li>reaction_matrix_path file exists</li> </ul> Source code in <code>mol_gen_docking/server_utils/server_setting.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate all settings after initialization.\n\n    Performs critical validation checks to ensure settings are consistent\n    and valid before the server starts. This method is called automatically\n    after the pydantic BaseSettings initializes the instance.\n\n    Raises:\n        AssertionError: If any of the following conditions are not met:\n\n            - scorer_exhaustiveness &gt; 0\n            - scorer_ncpus &gt; 0\n            - docking_concurrency_per_gpu &gt; 0\n            - reaction_matrix_path file exists\n    \"\"\"\n    assert self.scorer_exhaustiveness &gt; 0, \"Exhaustiveness must be greater than 0\"\n    assert self.scorer_ncpus &gt; 0, \"Number of CPUs must be greater than 0\"\n\n    assert Path(self.reaction_matrix_path).exists(), (\n        f\"Reaction matrix file {self.reaction_matrix_path} does not exist\"\n    )\n</code></pre>"},{"location":"reward_server/server_configuration/#mol_gen_docking.server_utils.server_setting.MolecularVerifierServerSettings.to_molecular_verifier_config","title":"<code>to_molecular_verifier_config(reward='property')</code>","text":"<p>Convert server settings to a complete verifier configuration model.</p> <p>Creates a MolecularVerifierConfigModel with all sub-verifier configurations (generation, reaction, and property) initialized from the server settings. This method bridges the gap between server-level settings and verifier-level configurations, ensuring consistent parameter propagation.</p> <p>Parameters:</p> Name Type Description Default <code>reward</code> <code>Literal['property', 'valid_smiles']</code> <p>Type of reward to compute.</p> <ul> <li>\"property\": Use property-based rewards for molecular optimization.   Evaluates predicted property values against target objectives.</li> <li>\"valid_smiles\": Use validity-based rewards. Returns 1.0 for valid   molecules and 0.0 for invalid ones. Default: \"property\"</li> </ul> <code>'property'</code> <p>Returns:</p> Name Type Description <code>MolecularVerifierConfigModel</code> <code>MolecularVerifierConfigModel</code> <p>A fully configured verifier model containing:</p> <ul> <li>GenerationVerifierConfigModel: For molecular generation tasks with:</li> <li>path_to_mappings set to data_path</li> <li>reward type synchronized</li> <li>rescale enabled</li> <li>oracle_kwargs with docking settings</li> <li> <p>docking_concurrency_per_gpu configured</p> </li> <li> <p>ReactionVerifierConfigModel: For reaction verification with:</p> </li> <li>reaction_matrix_path configured</li> <li>reward type synchronized</li> <li> <p>reaction_reward_type set to \"tanimoto\" (default)</p> </li> <li> <p>MolPropVerifierConfigModel: For property prediction with:</p> </li> <li>reward type synchronized</li> </ul> <p>All sub-configs are automatically synced to the specified reward type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the referenced configuration paths don't exist. This can happen if data_path or reaction_matrix_path are invalid.</p> Source code in <code>mol_gen_docking/server_utils/server_setting.py</code> <pre><code>def to_molecular_verifier_config(\n    self, reward: Literal[\"property\", \"valid_smiles\"] = \"property\"\n) -&gt; MolecularVerifierConfigModel:\n    \"\"\"Convert server settings to a complete verifier configuration model.\n\n    Creates a MolecularVerifierConfigModel with all sub-verifier configurations\n    (generation, reaction, and property) initialized from the server settings.\n    This method bridges the gap between server-level settings and verifier-level\n    configurations, ensuring consistent parameter propagation.\n\n    Args:\n        reward (Literal[\"property\", \"valid_smiles\"]): Type of reward to compute.\n\n            - \"property\": Use property-based rewards for molecular optimization.\n              Evaluates predicted property values against target objectives.\n            - \"valid_smiles\": Use validity-based rewards. Returns 1.0 for valid\n              molecules and 0.0 for invalid ones.\n            Default: \"property\"\n\n    Returns:\n        MolecularVerifierConfigModel: A fully configured verifier model containing:\n\n            - GenerationVerifierConfigModel: For molecular generation tasks with:\n              - path_to_mappings set to data_path\n              - reward type synchronized\n              - rescale enabled\n              - oracle_kwargs with docking settings\n              - docking_concurrency_per_gpu configured\n\n            - ReactionVerifierConfigModel: For reaction verification with:\n              - reaction_matrix_path configured\n              - reward type synchronized\n              - reaction_reward_type set to \"tanimoto\" (default)\n\n            - MolPropVerifierConfigModel: For property prediction with:\n              - reward type synchronized\n\n            All sub-configs are automatically synced to the specified reward type.\n\n    Raises:\n        ValueError: If any of the referenced configuration paths don't exist.\n            This can happen if data_path or reaction_matrix_path are invalid.\n    \"\"\"\n    # Create oracle kwargs from server settings\n    oracle_kwargs = {\n        \"exhaustiveness\": self.scorer_exhaustiveness,\n        \"n_cpu\": self.scorer_ncpus,\n        \"docking_oracle\": self.docking_oracle,\n        \"vina_mode\": self.vina_mode,\n    }\n\n    # Create GenerationVerifierConfigModel\n    generation_config = GenerationVerifierConfigModel(\n        path_to_mappings=self.data_path,\n        reward=reward,\n        rescale=True,\n        oracle_kwargs=oracle_kwargs,\n        docking_concurrency_per_gpu=self.docking_concurrency_per_gpu,\n    )\n\n    # Create ReactionVerifierConfigModel\n    reaction_config = ReactionVerifierConfigModel(\n        reaction_matrix_path=self.reaction_matrix_path,\n        reward=reward,\n    )\n\n    # Create MolPropVerifierConfigModel\n    molprop_config = MolPropVerifierConfigModel(reward=reward)\n\n    # Create and return MolecularVerifierConfigModel\n    return MolecularVerifierConfigModel(\n        parsing_method=self.parsing_method,\n        reward=reward,\n        generation_verifier_config=generation_config,\n        reaction_verifier_config=reaction_config,\n        mol_prop_verifier_config=molprop_config,\n    )\n</code></pre>"}]}